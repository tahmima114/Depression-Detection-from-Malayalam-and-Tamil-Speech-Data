{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14985459,"datasetId":9592660,"databundleVersionId":15859169},{"sourceType":"datasetVersion","sourceId":14986512,"datasetId":9593253,"databundleVersionId":15860320}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nBASE = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det\"\n\nprint(\"Malayalam dep train:\", len(os.listdir(BASE + \"/Malayalam/Malayalam/Depressed/Train_set\")))\nprint(\"Malayalam nondep train:\", len(os.listdir(BASE + \"/Malayalam/Malayalam/Non_depressed/Train_set\")))\nprint(\"Tamil dep train:\", len(os.listdir(BASE + \"/Tamil/Tamil/Depressed/Train_set\")))\nprint(\"Tamil nondep train:\", len(os.listdir(BASE + \"/Tamil/Tamil/Non-depressed/Train_set\")))\nprint(\"Malayalam test:\", len(os.listdir(BASE + \"/Test_set_mal/Test_set_mal\")))\nprint(\"Tamil test:\", len(os.listdir(BASE + \"/Test-set-tamil/Test-set-tamil\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:16:53.823456Z","iopub.execute_input":"2026-02-27T19:16:53.823787Z","iopub.status.idle":"2026-02-27T19:16:53.845350Z","shell.execute_reply.started":"2026-02-27T19:16:53.823758Z","shell.execute_reply":"2026-02-27T19:16:53.844695Z"}},"outputs":[{"name":"stdout","text":"Malayalam dep train: 788\nMalayalam nondep train: 900\nTamil dep train: 454\nTamil nondep train: 920\nMalayalam test: 200\nTamil test: 160\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Reproducibility\n\nRandom seeds are fixed for Python, NumPy, and PyTorch to ensure deterministic and reproducible results. CUDA determinism is enabled, and a worker seed function is used for DataLoader consistency","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport torch\n\nSEED = 42\n\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Optional: stronger determinism (may error if some ops are nondeterministic)\n# torch.use_deterministic_algorithms(True)\n\ndef seed_worker(worker_id):\n    worker_seed = SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:20:18.302957Z","iopub.execute_input":"2026-02-27T19:20:18.303370Z","iopub.status.idle":"2026-02-27T19:20:22.371444Z","shell.execute_reply.started":"2026-02-27T19:20:18.303330Z","shell.execute_reply":"2026-02-27T19:20:22.370338Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Resampling\n\nResamples Malayalam depression dataset audio to 16 kHz mono while preserving folder structure and logging errors.","metadata":{}},{"cell_type":"code","source":"import os, glob, logging\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom tqdm import tqdm\n\n# -----------------------------\n# Paths (Kaggle)\n# -----------------------------\nBASE = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det\"\n\nRAW_ROOT = os.path.join(BASE, \"Malayalam\", \"Malayalam\")      # contains Depressed/ and Non_depressed/\nOUT_ROOT = \"/kaggle/working/data/Malayalam_16k\"              # writable output\nos.makedirs(OUT_ROOT, exist_ok=True)\n\n# -----------------------------\n# Logging\n# -----------------------------\nlogging.basicConfig(\n    filename=\"resample_errors.log\",\n    level=logging.ERROR,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\n\ndef resample_audio(in_path, out_path, target_sr=16000):\n    \"\"\"\n    - Load mono\n    - Resample to 16k if needed\n    - Avoid peak normalization (prevents amplifying noise / removing energy cues)\n    - Basic guards for empty / silent audio\n    \"\"\"\n    try:\n        y, sr = librosa.load(in_path, sr=None, mono=True)\n\n        if y is None or len(y) == 0:\n            raise ValueError(\"Empty audio\")\n\n        if float(np.sqrt(np.mean(y**2))) < 1e-6:\n            raise ValueError(\"Near-silent audio\")\n\n        if sr != target_sr:\n            try:\n                y = librosa.resample(y, orig_sr=sr, target_sr=target_sr, res_type=\"soxr_hq\")\n            except Exception:\n                y = librosa.resample(y, orig_sr=sr, target_sr=target_sr, res_type=\"kaiser_best\")\n\n        y = np.clip(y, -1.0, 1.0)\n\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n        sf.write(out_path, y, target_sr)\n\n    except Exception as e:\n        logging.error(f\"Error processing {in_path}: {e}\")\n        print(f\"[Resample error] {in_path} -> {e}\")\n\ndef process_folder(src_folder, dst_folder):\n    wav_files = glob.glob(os.path.join(src_folder, \"**/*.wav\"), recursive=True)\n    if len(wav_files) == 0:\n        print(f\"[Skip] No wav files found in: {src_folder}\")\n        return\n\n    for in_path in tqdm(wav_files, desc=f\"Resampling {os.path.basename(src_folder)}\"):\n        # Preserve any nested structure (even if currently flat)\n        rel = os.path.relpath(in_path, src_folder)\n        out_path = os.path.join(dst_folder, rel)\n\n        if os.path.exists(out_path):\n            continue\n\n        resample_audio(in_path, out_path, target_sr=16000)\n\n# -----------------------------\n# Run\n# -----------------------------\npairs = [\n    (\"Depressed\", \"Depressed\"),\n    (\"Non_depressed\", \"Non_depressed\"),\n]\n\nfor src_sub, dst_sub in pairs:\n    src_path = os.path.join(RAW_ROOT, src_sub)\n    dst_path = os.path.join(OUT_ROOT, dst_sub)\n    process_folder(src_path, dst_path)\n\nprint(\"\\n✅ Resampling complete! Output root:\", OUT_ROOT)\nprint(\"Check errors in: resample_errors.log\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:24:06.348042Z","iopub.execute_input":"2026-02-27T19:24:06.348839Z","iopub.status.idle":"2026-02-27T19:25:02.138528Z","shell.execute_reply.started":"2026-02-27T19:24:06.348789Z","shell.execute_reply":"2026-02-27T19:25:02.137742Z"}},"outputs":[{"name":"stderr","text":"Resampling Depressed: 100%|██████████| 788/788 [00:30<00:00, 26.18it/s]\nResampling Non_depressed: 100%|██████████| 900/900 [00:23<00:00, 37.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n✅ Resampling complete! Output root: /kaggle/working/data/Malayalam_16k\nCheck errors in: resample_errors.log\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os, glob\nimport pandas as pd\n\nDATA_ROOT = \"/kaggle/working/data/Malayalam_16k\"\n\nrows = []\n\ndef collect(folder, label):\n    files = glob.glob(os.path.join(folder, \"**/*.wav\"), recursive=True)\n    for f in files:\n        rows.append({\n            \"file_path\": f,\n            \"label\": label,\n            \"fname\": os.path.splitext(os.path.basename(f))[0]\n        })\n\ncollect(os.path.join(DATA_ROOT, \"Depressed\"), 1)\ncollect(os.path.join(DATA_ROOT, \"Non_depressed\"), 0)\n\ndf = pd.DataFrame(rows)\nassert len(df) > 0, \"No wav files found. Check DATA_ROOT and your resampling output.\"\n\n# Deterministic shuffle\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nmetadata_path = os.path.join(DATA_ROOT, \"malayalam_metadata.csv\")\ndf.to_csv(metadata_path, index=False)\n\nprint(\"✅ CSV saved at:\", metadata_path)\nprint(df.head())\nprint(\"\\nLabel counts:\")\nprint(df[\"label\"].value_counts())\nprint(\"\\nTotal files:\", len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:25:40.579517Z","iopub.execute_input":"2026-02-27T19:25:40.580373Z","iopub.status.idle":"2026-02-27T19:25:40.933579Z","shell.execute_reply.started":"2026-02-27T19:25:40.580340Z","shell.execute_reply":"2026-02-27T19:25:40.932735Z"}},"outputs":[{"name":"stdout","text":"✅ CSV saved at: /kaggle/working/data/Malayalam_16k/malayalam_metadata.csv\n                                           file_path  label        fname\n0  /kaggle/working/data/Malayalam_16k/Non_depress...      0     ND4_0039\n1  /kaggle/working/data/Malayalam_16k/Depressed/T...      1    D_S0020_b\n2  /kaggle/working/data/Malayalam_16k/Depressed/T...      1  D_F001_48_4\n3  /kaggle/working/data/Malayalam_16k/Depressed/T...      1  D_F001_45_1\n4  /kaggle/working/data/Malayalam_16k/Non_depress...      0     ND2_0129\n\nLabel counts:\nlabel\n0    900\n1    788\nName: count, dtype: int64\n\nTotal files: 1688\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Sanity Check of Data\n","metadata":{}},{"cell_type":"code","source":"import os, re\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nfrom tqdm import tqdm\nfrom sklearn.model_selection import GroupShuffleSplit\n\n# -----------------------------\n# Load metadata\n# -----------------------------\nDATA_ROOT = \"/kaggle/working/data/Malayalam_16k\"\ncsv_path = os.path.join(DATA_ROOT, \"malayalam_metadata.csv\")\n\ndf = pd.read_csv(csv_path)\nprint(\"Loaded CSV:\", len(df))\n\n# -----------------------------\n# Speaker & utterance parsing (TAILORED)\n# -----------------------------\ndef extract_speaker(fname):\n    \"\"\"\n    Examples:\n    D_F001_25_1  -> D_F001\n    D_A032_4    -> D_A032\n    ND3_0133    -> ND3\n    ND1_0009    -> ND1\n    \"\"\"\n    parts = fname.split(\"_\")\n    if fname.startswith(\"ND\"):\n        return parts[0]            # ND1, ND3, ...\n    if fname.startswith(\"D_\"):\n        return \"_\".join(parts[:2]) # D_F001, D_A032, ...\n    return parts[0]                # fallback\n\ndef extract_utt_base(fname):\n    \"\"\"\n    Removes repeat suffix:\n    D_F001_25_1  -> D_F001_25\n    D_F001_64_3  -> D_F001_64\n    ND3_0133    -> ND3_0133\n    \"\"\"\n    return re.sub(r\"_\\d+$\", \"\", fname)\n\ndf[\"speaker\"] = df[\"fname\"].apply(extract_speaker)\ndf[\"utt_base\"] = df[\"fname\"].apply(extract_utt_base)\n\n# -----------------------------\n# Basic dataset stats\n# -----------------------------\nprint(\"\\n[Label distribution]\")\nprint(df[\"label\"].value_counts())\n\nprint(\"\\n[Unique speakers per class]\")\nprint(df.groupby(\"label\")[\"speaker\"].nunique().rename({0:\"Non-depressed\", 1:\"Depressed\"}))\n\nprint(\"\\n[Top speakers by sample count]\")\nprint(df[\"speaker\"].value_counts().head(10))\n\n# -----------------------------\n# Repeat-utterance analysis\n# -----------------------------\nrepeat_counts = (\n    df.groupby([\"speaker\", \"utt_base\"])\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values(\"count\", ascending=False)\n)\n\nrepeats = repeat_counts[repeat_counts[\"count\"] > 1]\n\nprint(\"\\n[Repeated utterances by same speaker]\")\nprint(\"Number of repeated (speaker, utt_base) pairs:\", len(repeats))\nif len(repeats) > 0:\n    print(repeats.head(10))\n\n# -----------------------------\n# Audio health checks\n# -----------------------------\ndef audio_stats(path):\n    try:\n        info = sf.info(path)\n        sr = info.samplerate\n        dur = info.frames / float(sr)\n\n        y, _ = sf.read(path, dtype=\"float32\", always_2d=False)\n        if y.ndim > 1:\n            y = np.mean(y, axis=1)\n\n        sil_ratio = float(np.mean(np.abs(y) < 1e-4))\n        rms = float(np.sqrt(np.mean(y**2)) + 1e-12)\n\n        return sr, dur, sil_ratio, rms, None\n    except Exception as e:\n        return None, None, None, None, str(e)\n\nrows = []\nfor p in tqdm(df[\"file_path\"], desc=\"Scanning audio\"):\n    rows.append(audio_stats(p))\n\ndf[[\"sr\",\"dur_sec\",\"sil_ratio\",\"rms\",\"err\"]] = pd.DataFrame(rows, index=df.index)\n\nprint(\"\\n[Read errors]:\", df[\"err\"].notna().sum())\n\nprint(\"\\n[Sample rate distribution]\")\nprint(df[\"sr\"].value_counts())\n\nprint(\"\\n[Duration stats (sec)]\")\nprint(df[\"dur_sec\"].describe(percentiles=[0.01,0.05,0.5,0.95,0.99]))\n\nprint(\"\\n[Silence ratio stats]\")\nprint(df[\"sil_ratio\"].describe(percentiles=[0.01,0.05,0.5,0.95,0.99]))\n\nprint(\"\\n[Potential issues]\")\nprint(\"Too short (<0.3s):\", (df[\"dur_sec\"] < 0.3).sum())\nprint(\"Very silent (sil_ratio > 0.98):\", (df[\"sil_ratio\"] > 0.98).sum())\n\n# -----------------------------\n# Speaker-safe split simulation\n# -----------------------------\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, val_idx = next(gss.split(df, y=df[\"label\"], groups=df[\"speaker\"]))\n\ntrain_df = df.iloc[train_idx]\nval_df   = df.iloc[val_idx]\n\nspeaker_overlap = set(train_df[\"speaker\"]).intersection(set(val_df[\"speaker\"]))\nutt_overlap = set(zip(train_df[\"speaker\"], train_df[\"utt_base\"])) & \\\n              set(zip(val_df[\"speaker\"], val_df[\"utt_base\"]))\n\nprint(\"\\n[Speaker-safe split check]\")\nprint(\"Train speakers:\", train_df[\"speaker\"].nunique())\nprint(\"Val speakers:\", val_df[\"speaker\"].nunique())\nprint(\"Speaker overlap:\", len(speaker_overlap))\n\nprint(\"Overlap of (speaker, utt_base) across splits:\", len(utt_overlap))\nif len(utt_overlap) > 0:\n    print(\"Example overlaps:\", list(utt_overlap)[:10])\n\nprint(\"\\n✅ Sanity check done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:27:52.427104Z","iopub.execute_input":"2026-02-27T19:27:52.427905Z","iopub.status.idle":"2026-02-27T19:27:54.133263Z","shell.execute_reply.started":"2026-02-27T19:27:52.427870Z","shell.execute_reply":"2026-02-27T19:27:54.132183Z"}},"outputs":[{"name":"stdout","text":"Loaded CSV: 1688\n\n[Label distribution]\nlabel\n0    900\n1    788\nName: count, dtype: int64\n\n[Unique speakers per class]\nlabel\nNon-depressed      5\nDepressed        127\nName: speaker, dtype: int64\n\n[Top speakers by sample count]\nspeaker\nD_F001     284\nND4        180\nND2        180\nND5        180\nND1        180\nND3        180\nD_S0020      4\nD_A018       4\nD_S0052      4\nD_S0049      4\nName: count, dtype: int64\n\n[Repeated utterances by same speaker]\nNumber of repeated (speaker, utt_base) pairs: 138\n    speaker utt_base  count\n389     ND1      ND1    180\n392     ND4      ND4    180\n390     ND2      ND2    180\n391     ND3      ND3    180\n393     ND5      ND5    180\n28   D_A029   D_A029      4\n26   D_A027   D_A027      4\n27   D_A028   D_A028      4\n24   D_A025   D_A025      4\n25   D_A026   D_A026      4\n","output_type":"stream"},{"name":"stderr","text":"Scanning audio: 100%|██████████| 1688/1688 [00:01<00:00, 1150.78it/s]","output_type":"stream"},{"name":"stdout","text":"\n[Read errors]: 0\n\n[Sample rate distribution]\nsr\n16000    1688\nName: count, dtype: int64\n\n[Duration stats (sec)]\ncount    1688.000000\nmean        4.714041\nstd         1.417860\nmin         1.522625\n1%          1.979884\n5%          2.449813\n50%         4.644813\n95%         6.943516\n99%         8.261785\nmax        10.360875\nName: dur_sec, dtype: float64\n\n[Silence ratio stats]\ncount    1688.000000\nmean        0.052202\nstd         0.100364\nmin         0.000746\n1%          0.000975\n5%          0.001190\n50%         0.005252\n95%         0.294750\n99%         0.388547\nmax         0.579842\nName: sil_ratio, dtype: float64\n\n[Potential issues]\nToo short (<0.3s): 0\nVery silent (sil_ratio > 0.98): 0\n\n[Speaker-safe split check]\nTrain speakers: 105\nVal speakers: 27\nSpeaker overlap: 0\nOverlap of (speaker, utt_base) across splits: 0\n\n✅ Sanity check done.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/data/Malayalam_16k/malayalam_metadata.csv\")\n\ndef extract_speaker(fname):\n    # Non-depressed speakers: ND1, ND2, ...\n    m = re.match(r\"^(ND\\d+)_\", fname, flags=re.IGNORECASE)\n    if m:\n        return m.group(1).upper()\n\n    # Depressed: we want REAL speaker IDs like D_F001, D_M002, D_S005 etc\n    # (exclude D_A032 which looks like utterance ID)\n    m = re.match(r\"^(D)_([FMS]\\d{3})_\", fname, flags=re.IGNORECASE)\n    if m:\n        return f\"{m.group(1).upper()}_{m.group(2).upper()}\"\n\n    # If it's depressed but doesn't match known speaker pattern, bucket it\n    if fname.upper().startswith(\"D_\"):\n        return \"D_OTHER\"\n\n    return \"UNKNOWN\"\n\ndef extract_utt_base(fname):\n    # remove trailing repeat suffix _1/_2/_3 etc (ONLY at end)\n    return re.sub(r\"_\\d+$\", \"\", fname.upper())\n\ndf[\"speaker_fixed\"] = df[\"fname\"].apply(extract_speaker)\ndf[\"utt_base_fixed\"] = df[\"fname\"].apply(extract_utt_base)\n\nprint(\"Unique speakers per class (fixed):\")\nprint(df.groupby(\"label\")[\"speaker_fixed\"].nunique().rename({0:\"Non-depressed\", 1:\"Depressed\"}))\n\nprint(\"\\nTop speakers (fixed):\")\nprint(df[\"speaker_fixed\"].value_counts().head(20))\n\nprint(\"\\nHow many D_OTHER?\")\nprint((df[\"speaker_fixed\"]==\"D_OTHER\").sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:30:15.196081Z","iopub.execute_input":"2026-02-27T19:30:15.196537Z","iopub.status.idle":"2026-02-27T19:30:15.223667Z","shell.execute_reply.started":"2026-02-27T19:30:15.196504Z","shell.execute_reply":"2026-02-27T19:30:15.222665Z"}},"outputs":[{"name":"stdout","text":"Unique speakers per class (fixed):\nlabel\nNon-depressed     5\nDepressed        11\nName: speaker_fixed, dtype: int64\n\nTop speakers (fixed):\nspeaker_fixed\nD_OTHER    477\nD_F001     284\nND4        180\nND2        180\nND5        180\nND3        180\nND1        180\nD_S004       3\nD_S006       3\nD_S009       3\nD_S001       3\nD_S007       3\nD_S003       3\nD_S002       3\nD_S005       3\nD_S008       3\nName: count, dtype: int64\n\nHow many D_OTHER?\n477\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Speaker-Aware Leakage-Free Train–Validation Split\nCreates speaker-aware, leakage-free stratified train–validation splits for the Malayalam dataset using grouped cross-validation.","metadata":{}},{"cell_type":"code","source":"\nimport os, re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedGroupKFold\n\n# ✅ Kaggle working paths\nIN_PATH  = \"/kaggle/working/data/Malayalam_16k/malayalam_metadata.csv\"\nOUT_PATH = \"/kaggle/working/data/Malayalam_16k/malayalam_metadata_with_split.csv\"\n\ndf = pd.read_csv(IN_PATH)\n\n# ---------- same parsing as before ----------\ndef extract_speaker(fname):\n    m = re.match(r\"^(ND\\d+)_\", fname, flags=re.IGNORECASE)\n    if m: \n        return m.group(1).upper()\n\n    m = re.match(r\"^(D)_([FMS]\\d{3})_\", fname, flags=re.IGNORECASE)\n    if m:\n        return f\"{m.group(1).upper()}_{m.group(2).upper()}\"\n\n    if fname.upper().startswith(\"D_\"):\n        return \"D_OTHER\"\n\n    return \"UNKNOWN\"\n\ndef utt_base(fname):\n    return re.sub(r\"_\\d+$\", \"\", fname.upper())\n\ndf[\"speaker_fixed\"] = df[\"fname\"].apply(extract_speaker)\ndf[\"utt_base_fixed\"] = df[\"fname\"].apply(utt_base)\n\ndef make_group(row):\n    if row[\"label\"] == 0:  # non-depressed\n        return row[\"speaker_fixed\"]          # ND1..ND5\n    if row[\"speaker_fixed\"] != \"D_OTHER\":\n        return row[\"speaker_fixed\"]          # D_F001, D_S###\n    return \"DUTT_\" + row[\"utt_base_fixed\"]   # prevent repeat leakage\n\ndf[\"group_id\"] = df.apply(make_group, axis=1)\n\noverall_ratio = df[\"label\"].mean()\nprint(\"Overall depressed ratio:\", overall_ratio)\n\n# ---------- try folds and score them ----------\nsgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n\ncandidates = []\nX = np.zeros((len(df), 1))  # dummy X for sklearn\n\nfor fold, (train_idx, val_idx) in enumerate(sgkf.split(X, df[\"label\"], groups=df[\"group_id\"])):\n    val = df.iloc[val_idx]\n    train = df.iloc[train_idx]\n\n    val_counts = val[\"label\"].value_counts().to_dict()\n    train_counts = train[\"label\"].value_counts().to_dict()\n\n    val_size = len(val)\n    has_both = (0 in val_counts) and (1 in val_counts)\n    val_ratio = val[\"label\"].mean() if val_size > 0 else 0.0\n\n    score = abs(val_ratio - overall_ratio)\n    if not has_both:\n        score += 10.0\n    if val_size < 200:\n        score += (200 - val_size) / 50.0\n\n    candidates.append((score, fold, val_size, val_counts, val_ratio, train_counts))\n\nprint(\"\\nFold candidates:\")\nfor score, fold, val_size, val_counts, val_ratio, train_counts in sorted(candidates, key=lambda x: x[0]):\n    print(f\"fold={fold}  score={score:.3f}  val_size={val_size}  val_counts={val_counts}  val_ratio={val_ratio:.3f}\")\n\nbest = sorted(candidates, key=lambda x: x[0])[0]\n_, best_fold, *_ = best\nprint(\"\\n✅ Choosing best fold:\", best_fold)\n\n# rerun to get indices for chosen fold\nfor fold, (train_idx, val_idx) in enumerate(sgkf.split(X, df[\"label\"], groups=df[\"group_id\"])):\n    if fold == best_fold:\n        break\n\ndf[\"split\"] = \"train\"\ndf.loc[val_idx, \"split\"] = \"val\"\n\n# verify group overlap\ntrain_df = df[df[\"split\"]==\"train\"]\nval_df = df[df[\"split\"]==\"val\"]\noverlap = set(train_df[\"group_id\"]) & set(val_df[\"group_id\"])\nprint(\"Group overlap:\", len(overlap))\n\nprint(\"\\nTrain label counts:\")\nprint(train_df[\"label\"].value_counts(), \" Train ratio:\", train_df[\"label\"].mean())\n\nprint(\"\\nVal label counts:\")\nprint(val_df[\"label\"].value_counts(), \" Val ratio:\", val_df[\"label\"].mean())\n\ndf.to_csv(OUT_PATH, index=False)\nprint(\"\\n✅ Saved split CSV:\", OUT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:34:22.044701Z","iopub.execute_input":"2026-02-27T19:34:22.045353Z","iopub.status.idle":"2026-02-27T19:34:22.297009Z","shell.execute_reply.started":"2026-02-27T19:34:22.045289Z","shell.execute_reply":"2026-02-27T19:34:22.296250Z"}},"outputs":[{"name":"stdout","text":"Overall depressed ratio: 0.466824644549763\n\nFold candidates:\nfold=1  score=0.051  val_size=308  val_counts={0: 180, 1: 128}  val_ratio=0.416\nfold=2  score=0.059  val_size=304  val_counts={0: 180, 1: 124}  val_ratio=0.408\nfold=3  score=0.209  val_size=485  val_counts={0: 360, 1: 125}  val_ratio=0.258\nfold=4  score=0.225  val_size=584  val_counts={1: 404, 0: 180}  val_ratio=0.692\nfold=0  score=14.393  val_size=7  val_counts={1: 7}  val_ratio=1.000\n\n✅ Choosing best fold: 1\nGroup overlap: 0\n\nTrain label counts:\nlabel\n0    720\n1    660\nName: count, dtype: int64  Train ratio: 0.4782608695652174\n\nVal label counts:\nlabel\n0    180\n1    128\nName: count, dtype: int64  Val ratio: 0.4155844155844156\n\n✅ Saved split CSV: /kaggle/working/data/Malayalam_16k/malayalam_metadata_with_split.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# MFCC","metadata":{}},{"cell_type":"code","source":"# =============================\n# 0️⃣ Seeds (strong + dataloader-safe)\n# =============================\nimport os, random\nimport numpy as np\nimport torch\n\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef seed_worker(worker_id):\n    worker_seed = SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =============================\n# 1️⃣ Imports\n# =============================\nimport pandas as pd\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nimport librosa\n\n# =============================\n# 2️⃣ Load leak-safe split CSV\n# =============================\ndata_root = \"/kaggle/working/data/Malayalam_16k\"\n\nsplit_csv = os.path.join(data_root, \"malayalam_metadata_with_split.csv\")\ndf = pd.read_csv(split_csv)\n\ntrain_df = df[df[\"split\"]==\"train\"].reset_index(drop=True)\nval_df   = df[df[\"split\"]==\"val\"].reset_index(drop=True)\n\nprint(\"Train:\", len(train_df), \" Val:\", len(val_df))\nprint(\"Train label ratio:\", train_df[\"label\"].mean(), \" Val label ratio:\", val_df[\"label\"].mean())\n\n# =============================\n# 3️⃣ MFCC extraction (+ optional SpecAugment-like masks)\n# =============================\ndef extract_mfcc_2d(file_path, sr=16000, n_mfcc=20, max_frames=120,\n                    n_fft=400, hop_length=160):\n    \"\"\"\n    n_fft=400, hop=160 -> ~25ms window, 10ms hop at 16kHz (standard speech setup)\n    max_frames ~ 120 => ~1.2 sec of frames if hop=10ms (but depends on trimming).\n    We will pad/truncate to max_frames after feature extraction.\n    \"\"\"\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n\n    # Optional: trim very long audio to reduce overfit on duration\n    # keep first ~6 seconds (tune later)\n    max_samples = sr * 6\n    if len(y) > max_samples:\n        y = y[:max_samples]\n\n    mfcc = librosa.feature.mfcc(\n        y=y, sr=sr, n_mfcc=n_mfcc,\n        n_fft=n_fft, hop_length=hop_length\n    )  # (n_mfcc, T)\n\n    # Pad/truncate time axis\n    T = mfcc.shape[1]\n    if T < max_frames:\n        mfcc = np.pad(mfcc, ((0,0),(0,max_frames-T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_frames]\n\n    # Per-sample normalization (good for MFCC)\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-9)\n\n    return mfcc.astype(np.float32)  # 2D\n\ndef time_freq_mask(mfcc_2d, max_time_mask=12, max_freq_mask=4, p=0.5):\n    \"\"\"\n    Light SpecAugment-style masking on MFCC.\n    \"\"\"\n    if random.random() > p:\n        return mfcc_2d\n\n    mfcc = mfcc_2d.copy()\n    n_mfcc, T = mfcc.shape\n\n    # time mask\n    t = random.randint(0, max_time_mask)\n    if t > 0:\n        t0 = random.randint(0, max(0, T - t))\n        mfcc[:, t0:t0+t] = 0.0\n\n    # freq mask\n    f = random.randint(0, max_freq_mask)\n    if f > 0:\n        f0 = random.randint(0, max(0, n_mfcc - f))\n        mfcc[f0:f0+f, :] = 0.0\n\n    return mfcc\n\n# =============================\n# 4️⃣ Dataset\n# =============================\nclass MFCCDataset(Dataset):\n    def __init__(self, df, augment=False):\n        self.df = df.reset_index(drop=True)\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = row[\"file_path\"]\n        label = int(row[\"label\"])\n\n        mfcc = extract_mfcc_2d(path)\n\n        if self.augment:\n            mfcc = time_freq_mask(mfcc, p=0.6)\n\n        # flatten for MLP\n        x = torch.tensor(mfcc.reshape(-1), dtype=torch.float32)\n        y = torch.tensor(label, dtype=torch.long)\n        return x, y\n\ntrain_ds = MFCCDataset(train_df, augment=True)\nval_ds   = MFCCDataset(val_df, augment=False)\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=32, shuffle=True,\n    num_workers=2, worker_init_fn=seed_worker, generator=g,\n    pin_memory=True\n)\nval_loader = DataLoader(\n    val_ds, batch_size=64, shuffle=False,\n    num_workers=2, worker_init_fn=seed_worker, generator=g,\n    pin_memory=True\n)\n\n# =============================\n# 5️⃣ Class weights\n# =============================\ntrain_labels = train_df[\"label\"].values\nclass_weights = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=train_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n# =============================\n# 6️⃣ MFCC model (MLP + strong regularization)\n# =============================\nclass MFCC_MLP(nn.Module):\n    def __init__(self, input_dim=20*120, hidden=128, drop=0.5):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(hidden, 2)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nmodel = MFCC_MLP().to(device)\n\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)\n\n# =============================\n# 7️⃣ Train + Early stopping on macro-F1\n# =============================\nbest_f1 = -1.0\nbest_state = None\npatience = 5\nbad = 0\nepochs = 30\n\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_loss = 0.0\n\n    for x, y in train_loader:\n        x = x.to(device)\n        y = y.to(device)\n\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = loss_fn(logits, y)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # helps stability\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    train_loss = total_loss / max(1, len(train_loader))\n\n    # ---- validate\n    model.eval()\n    preds, gold = [], []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x = x.to(device)\n            y = y.to(device)\n            logits = model(x)\n            p = torch.argmax(logits, dim=1)\n            preds.extend(p.cpu().numpy().tolist())\n            gold.extend(y.cpu().numpy().tolist())\n\n    val_f1 = f1_score(gold, preds, average=\"macro\")\n\n    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_macroF1={val_f1:.4f}\")\n\n    if val_f1 > best_f1 + 1e-4:\n        best_f1 = val_f1\n        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        bad = 0\n    else:\n        bad += 1\n        if bad >= patience:\n            print(\"Early stopping.\")\n            break\n\n# load best\nmodel.load_state_dict(best_state)\nprint(\"\\n✅ Best MFCC Val macro-F1:\", best_f1)\n\n# after you have best_state\ntorch.save(best_state, \"/kaggle/working/mfcc_mlp.pt\")\nprint(\"✅ Saved MFCC at /kaggle/working/mfcc_mlp.pt\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:38:39.027009Z","iopub.execute_input":"2026-02-27T19:38:39.027824Z","iopub.status.idle":"2026-02-27T19:41:54.849008Z","shell.execute_reply.started":"2026-02-27T19:38:39.027791Z","shell.execute_reply":"2026-02-27T19:41:54.847962Z"}},"outputs":[{"name":"stdout","text":"Train: 1380  Val: 308\nTrain label ratio: 0.4782608695652174  Val label ratio: 0.4155844155844156\nEpoch 01 | train_loss=0.4969 | val_macroF1=0.9933\nEpoch 02 | train_loss=0.1687 | val_macroF1=0.9801\nEpoch 03 | train_loss=0.1237 | val_macroF1=0.9801\nEpoch 04 | train_loss=0.1288 | val_macroF1=0.9967\nEpoch 05 | train_loss=0.0994 | val_macroF1=0.9933\nEpoch 06 | train_loss=0.0790 | val_macroF1=0.9669\nEpoch 07 | train_loss=0.0633 | val_macroF1=0.9247\nEpoch 08 | train_loss=0.0621 | val_macroF1=0.9933\nEpoch 09 | train_loss=0.0538 | val_macroF1=0.9933\nEarly stopping.\n\n✅ Best MFCC Val macro-F1: 0.9966617170480041\n✅ Saved MFCC at /kaggle/working/mfcc_mlp.pt\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Wev2Vec2","metadata":{}},{"cell_type":"code","source":"# =============================\n# 0️⃣ Seeds + imports\n# =============================\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport librosa\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom torch.utils.data import Dataset\n\nfrom transformers import (\n    Wav2Vec2ForSequenceClassification,\n    Wav2Vec2Processor,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback,\n    set_seed\n)\n\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nset_seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =============================\n# 1️⃣ Load leak-safe split CSV\n# =============================\n\ndata_root = \"/kaggle/working/data/Malayalam_16k\"\nsplit_csv = os.path.join(data_root, \"malayalam_metadata_with_split.csv\")\ndf = pd.read_csv(split_csv)\n\ntrain_df = df[df[\"split\"]==\"train\"].reset_index(drop=True)\nval_df   = df[df[\"split\"]==\"val\"].reset_index(drop=True)\n\nprint(\"Train:\", len(train_df), \" Val:\", len(val_df))\nprint(\"Train label ratio:\", train_df[\"label\"].mean(), \" Val label ratio:\", val_df[\"label\"].mean())\n\n# =============================\n# 2️⃣ (Mild) augmentation for training only\n# =============================\ndef augment_audio(y, sr):\n    y = np.asarray(y, dtype=np.float32)\n    if y.ndim > 1:\n        y = np.mean(y, axis=-1)\n\n    # mild speed\n    if random.random() < 0.5:\n        rate = np.random.uniform(0.95, 1.05)\n        y = librosa.effects.time_stretch(y=y, rate=rate)\n\n    # mild pitch\n    if random.random() < 0.5:\n        n_steps = np.random.uniform(-1.0, 1.0)\n        y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n\n    # mild noise\n    if random.random() < 0.5 and y.size > 0:\n        noise_amp = 0.001 * np.random.uniform() * max(1e-6, float(np.max(np.abs(y))))\n        y = y + noise_amp * np.random.normal(size=y.shape[0]).astype(np.float32)\n\n    return y.astype(np.float32)\n\n# =============================\n# 3️⃣ Dataset (truncate to max_seconds)\n# =============================\nclass MalayalamWav2Vec2Dataset(Dataset):\n    def __init__(self, df, max_seconds=6.0, augment=False, sr=16000):\n        self.df = df.reset_index(drop=True)\n        self.max_len = int(max_seconds * sr)\n        self.augment = augment\n        self.sr = sr\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = row[\"file_path\"]\n        label = int(row[\"label\"])\n\n        y, _ = librosa.load(path, sr=self.sr, mono=True)\n\n        # truncate / pad in raw space (padding will be handled by processor)\n        if len(y) > self.max_len:\n            y = y[:self.max_len]\n\n        if self.augment:\n            y = augment_audio(y, self.sr)\n            # clip safety\n            y = np.clip(y, -1.0, 1.0)\n\n        return {\"input_values\": y, \"labels\": label}\n\ntrain_dataset = MalayalamWav2Vec2Dataset(train_df, augment=True)\nval_dataset   = MalayalamWav2Vec2Dataset(val_df, augment=False)\n\n# =============================\n# 4️⃣ Processor + model\n# =============================\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    \"facebook/wav2vec2-base\",\n    num_labels=2,\n    problem_type=\"single_label_classification\"\n)\n\n# Freeze encoder initially (prevents overfit)\nfor p in model.wav2vec2.parameters():\n    p.requires_grad = False\n\n# slightly higher dropout\nmodel.config.hidden_dropout_prob = 0.2\nmodel.config.attention_dropout = 0.2\n\nmodel.to(device)\n\n# =============================\n# 5️⃣ Data collator\n# =============================\ndef data_collator(batch):\n    audio = [b[\"input_values\"] for b in batch]\n    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n\n    inputs = processor(\n        audio,\n        sampling_rate=16000,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    inputs[\"labels\"] = labels\n    return inputs\n\n# =============================\n# 6️⃣ Metrics\n# =============================\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\"f1\": f1_score(labels, preds, average=\"macro\")}\n\n# =============================\n# 7️⃣ Weighted-loss Trainer (correct way)\n# =============================\ntrain_labels = train_df[\"label\"].values\nclass_weights = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=train_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.get(\"labels\")\n        outputs = model(**{k:v for k,v in inputs.items() if k!=\"labels\"})\n        logits = outputs.logits\n        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits, labels.to(logits.device))\n        return (loss, outputs) if return_outputs else loss\n\n# =============================\n# 8️⃣ Training args\n# =============================\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2_model\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,   # effective batch 8 (more stable)\n    num_train_epochs=15,\n    learning_rate=3e-4,              # higher since encoder is frozen\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    logging_steps=25,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    max_grad_norm=1.0,\n    fp16=torch.cuda.is_available(),\n    seed=SEED,\n    data_seed=SEED,\n)\n\n# =============================\n# 9️⃣ Train\n# =============================\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\ntrainer.train()\n\n# =============================\n# 10️⃣ Final eval (val)\n# =============================\nmetrics = trainer.evaluate()\nprint(\"\\n✅ Wav2Vec2 Val F1:\", metrics[\"eval_f1\"])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T19:43:46.615590Z","iopub.execute_input":"2026-02-27T19:43:46.616883Z","iopub.status.idle":"2026-02-27T20:06:34.116180Z","shell.execute_reply.started":"2026-02-27T19:43:46.616847Z","shell.execute_reply":"2026-02-27T20:06:34.115394Z"}},"outputs":[{"name":"stdout","text":"Train: 1380  Val: 308\nTrain label ratio: 0.4782608695652174  Val label ratio: 0.4155844155844156\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"417c4c11d3e547248602982284cda256"}},"metadata":{}},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651834646cac4b5e874bae2e3acfb411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe621174d4f4d5cb66578e4f62e2475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd40ce930a04cf4b1bfd188e839a27c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc53047d8e864e83a47a572b937cd785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d590260a3b4ded9891cf012e5c0af0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/211 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e263793928430ab5712d97fd3704fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9735bf4a0994cec940a45ea12460965"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\nKey                          | Status     | \n-----------------------------+------------+-\nquantizer.weight_proj.bias   | UNEXPECTED | \nproject_q.weight             | UNEXPECTED | \nquantizer.codevectors        | UNEXPECTED | \nproject_hid.weight           | UNEXPECTED | \nproject_hid.bias             | UNEXPECTED | \nquantizer.weight_proj.weight | UNEXPECTED | \nproject_q.bias               | UNEXPECTED | \nclassifier.weight            | MISSING    | \nprojector.bias               | MISSING    | \nclassifier.bias              | MISSING    | \nprojector.weight             | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\nwarmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='783' max='1305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 783/1305 22:17 < 14:54, 0.58 it/s, Epoch 9/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.192762</td>\n      <td>0.539525</td>\n      <td>0.868886</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.353097</td>\n      <td>0.259738</td>\n      <td>0.892448</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.100007</td>\n      <td>0.162296</td>\n      <td>0.947332</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.051018</td>\n      <td>0.233902</td>\n      <td>0.895664</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.060708</td>\n      <td>0.121691</td>\n      <td>0.957104</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.045363</td>\n      <td>0.052139</td>\n      <td>0.990007</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.039116</td>\n      <td>0.063817</td>\n      <td>0.986690</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.028528</td>\n      <td>0.069985</td>\n      <td>0.990007</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.024441</td>\n      <td>0.097692</td>\n      <td>0.966917</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19450bd8c7444e96a1996fca8ca16905"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c690cbef00e746af8913a264da16cfc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c405cf3744d74b6c83e2f7218403ea60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e623742250fe401db994629d0f77f76f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3272c375f754423bec5ff1839fc9f70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb2113caaf14351be3646aed185a5b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309eeee8385449459f29005de8f76a3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"417863ddafee4f7188d4fcc7dde76fbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3dbaed18f3b4f39887b52254ca650b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n✅ Wav2Vec2 Val F1: 0.9900068135362253\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# =============================\n# 11️⃣ Save best model + processor\n# =============================\nsave_dir = \"/kaggle/working/w2v2_best\"\n\ntrainer.save_model(save_dir)      # saves model + config\nprocessor.save_pretrained(save_dir)\n\nprint(\"✅ Saved Wav2Vec2 model + processor to:\", save_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T20:07:06.834367Z","iopub.execute_input":"2026-02-27T20:07:06.835113Z","iopub.status.idle":"2026-02-27T20:07:07.566810Z","shell.execute_reply.started":"2026-02-27T20:07:06.835078Z","shell.execute_reply":"2026-02-27T20:07:07.566114Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee4bf59526c44155b399eec93e483886"}},"metadata":{}},{"name":"stdout","text":"✅ Saved Wav2Vec2 model + processor to: /kaggle/working/w2v2_best\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Ensemble Weight Tuning & Validation\n\nTunes and evaluates an MFCC–Wav2Vec2 ensemble on the validation set to select optimal weighting and enable final depression prediction.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport librosa\nfrom sklearn.metrics import f1_score\nfrom transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# -----------------------------\n# Paths\n# -----------------------------\nSPLIT_CSV = \"/kaggle/working/data/Malayalam_16k/malayalam_metadata_with_split.csv\"\nMFCC_WEIGHTS_PATH = \"/kaggle/working/mfcc_mlp.pt\"\nW2V2_DIR = \"/kaggle/working/w2v2_best\"\n\nassert os.path.exists(SPLIT_CSV), f\"Missing: {SPLIT_CSV}\"\nassert os.path.exists(MFCC_WEIGHTS_PATH), f\"Missing: {MFCC_WEIGHTS_PATH}\"\nassert os.path.isdir(W2V2_DIR), f\"Missing dir: {W2V2_DIR}\"\n\n\n# -----------------------------\n# Load VAL split\n# -----------------------------\ndf = pd.read_csv(SPLIT_CSV)\nval_df = df[df[\"split\"] == \"val\"].reset_index(drop=True)\nprint(\"Val size:\", len(val_df), \" Val ratio:\", float(val_df[\"label\"].mean()))\n\n# -----------------------------\n# MFCC extractor (match training)\n# -----------------------------\ndef extract_mfcc_2d(file_path, sr=16000, n_mfcc=20, max_frames=120, n_fft=400, hop_length=160):\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n    max_samples = sr * 6\n    if len(y) > max_samples:\n        y = y[:max_samples]\n\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n\n    T = mfcc.shape[1]\n    if T < max_frames:\n        mfcc = np.pad(mfcc, ((0,0),(0,max_frames-T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_frames]\n\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-9)\n    return mfcc.astype(np.float32)\n\n# -----------------------------\n# MFCC model class (match training)\n# -----------------------------\nclass MFCC_MLP(nn.Module):\n    def __init__(self, input_dim=20*120, hidden=128, drop=0.5):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(hidden, 2)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n# -----------------------------\n# Load MFCC model\n# -----------------------------\nmfcc_model = MFCC_MLP().to(device)\nmfcc_model.load_state_dict(torch.load(MFCC_WEIGHTS_PATH, map_location=device))\nmfcc_model.eval()\nprint(\"✅ MFCC loaded\")\n\n# -----------------------------\n# Load W2V2 model + processor\n# -----------------------------\nwav2vec_model = Wav2Vec2ForSequenceClassification.from_pretrained(W2V2_DIR).to(device)\nprocessor = Wav2Vec2Processor.from_pretrained(W2V2_DIR)\nwav2vec_model.eval()\nprint(\"✅ W2V2 loaded\")\n\n# -----------------------------\n# Probability functions\n# -----------------------------\n@torch.no_grad()\ndef mfcc_proba(file_path):\n    mfcc = extract_mfcc_2d(file_path)\n    x = torch.tensor(mfcc.reshape(-1), dtype=torch.float32).unsqueeze(0).to(device)\n    logits = mfcc_model(x)\n    return torch.softmax(logits, dim=-1).cpu().numpy()[0]\n\n@torch.no_grad()\ndef w2v2_proba(file_path, sr=16000, max_seconds=6.0):\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n    max_len = int(sr * max_seconds)\n    if len(y) > max_len:\n        y = y[:max_len]\n\n    inputs = processor([y], sampling_rate=sr, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    logits = wav2vec_model(**inputs).logits\n    return torch.softmax(logits, dim=-1).cpu().numpy()[0]\n\n# -----------------------------\n# Evaluate ensemble weights\n# -----------------------------\ndef eval_ensemble(weight_mfcc):\n    preds, gold = [], []\n    for i in range(len(val_df)):\n        path = val_df.loc[i, \"file_path\"]\n        label = int(val_df.loc[i, \"label\"])\n\n        p_m = mfcc_proba(path)\n        p_w = w2v2_proba(path)\n\n        p = weight_mfcc * p_m + (1 - weight_mfcc) * p_w\n        pred = int(np.argmax(p))\n\n        preds.append(pred)\n        gold.append(label)\n\n    return f1_score(gold, preds, average=\"macro\")\n\nweights = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\nbest_w, best_f1 = None, -1.0\n\nprint(\"\\n[Ensemble weight tuning]\")\nfor w in weights:\n    f1 = eval_ensemble(w)\n    print(f\"weight_mfcc={w:.1f} -> val_macroF1={f1:.4f}\")\n    if f1 > best_f1:\n        best_f1, best_w = f1, w\n\nprint(f\"\\n✅ Best weight_mfcc={best_w:.1f} | Best VAL macroF1={best_f1:.4f}\")\n\n# -----------------------------\n# Final predict function for submission use\n# -----------------------------\ndef ensemble_predict(file_path, weight_mfcc=best_w):\n    p_m = mfcc_proba(file_path)\n    p_w = w2v2_proba(file_path)\n    p = weight_mfcc * p_m + (1 - weight_mfcc) * p_w\n    return int(np.argmax(p)), p\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T20:07:27.353961Z","iopub.execute_input":"2026-02-27T20:07:27.354299Z","iopub.status.idle":"2026-02-27T20:08:36.940230Z","shell.execute_reply.started":"2026-02-27T20:07:27.354271Z","shell.execute_reply":"2026-02-27T20:08:36.939250Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nVal size: 308  Val ratio: 0.4155844155844156\n✅ MFCC loaded\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/215 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aaa76734c9641398a1d1b90106d87ad"}},"metadata":{}},{"name":"stdout","text":"✅ W2V2 loaded\n\n[Ensemble weight tuning]\nweight_mfcc=0.0 -> val_macroF1=0.9636\nweight_mfcc=0.2 -> val_macroF1=0.9768\nweight_mfcc=0.4 -> val_macroF1=0.9933\nweight_mfcc=0.6 -> val_macroF1=1.0000\nweight_mfcc=0.8 -> val_macroF1=0.9967\nweight_mfcc=1.0 -> val_macroF1=0.9967\n\n✅ Best weight_mfcc=0.6 | Best VAL macroF1=1.0000\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"# =============================\n# 0) Seeds + imports\n# =============================\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport librosa\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    Wav2Vec2ForSequenceClassification,\n    Wav2Vec2Processor,\n    Trainer,\n    TrainingArguments,\n    set_seed\n)\n\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nset_seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# =============================\n# 1) Load split CSV + MERGE train+val\n# =============================\nSPLIT_CSV = \"/kaggle/working/data/Malayalam_16k/malayalam_metadata_with_split.csv\"\ndf = pd.read_csv(SPLIT_CSV)\n\ntrain_df = df[df[\"split\"].isin([\"train\", \"val\"])].reset_index(drop=True)  # ✅ merge\nprint(\"Final train size (train+val):\", len(train_df))\nprint(\"Final train label ratio:\", float(train_df[\"label\"].mean()))\n\n# =============================\n# 2) Mild augmentation (train only)\n# =============================\ndef augment_audio(y, sr=16000):\n    y = np.asarray(y, dtype=np.float32)\n\n    # mild speed\n    if random.random() < 0.5:\n        rate = np.random.uniform(0.95, 1.05)\n        y = librosa.effects.time_stretch(y=y, rate=rate)\n\n    # mild pitch\n    if random.random() < 0.5:\n        n_steps = np.random.uniform(-1.0, 1.0)\n        y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n\n    # mild noise\n    if random.random() < 0.5 and y.size > 0:\n        noise_amp = 0.001 * np.random.uniform() * max(1e-6, float(np.max(np.abs(y))))\n        y = y + noise_amp * np.random.normal(size=y.shape[0]).astype(np.float32)\n\n    return np.clip(y, -1.0, 1.0).astype(np.float32)\n\n# =============================\n# 3) Dataset (truncate to max_seconds)\n# =============================\nclass MalayalamWav2Vec2Dataset(Dataset):\n    def __init__(self, df, max_seconds=6.0, augment=False, sr=16000):\n        self.df = df.reset_index(drop=True)\n        self.max_len = int(max_seconds * sr)\n        self.augment = augment\n        self.sr = sr\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = row[\"file_path\"]\n        label = int(row[\"label\"])\n\n        y, _ = librosa.load(path, sr=self.sr, mono=True)\n\n        if len(y) > self.max_len:\n            y = y[:self.max_len]\n\n        if self.augment:\n            y = augment_audio(y, self.sr)\n\n        return {\"input_values\": y, \"labels\": label}\n\ntrain_dataset = MalayalamWav2Vec2Dataset(train_df, augment=True)\n\n# =============================\n# 4) Processor + model\n# =============================\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    \"facebook/wav2vec2-base\",\n    num_labels=2,\n    problem_type=\"single_label_classification\"\n)\n\n# freeze encoder (works well for small data)\nfor p in model.wav2vec2.parameters():\n    p.requires_grad = False\n\nmodel.to(device)\n\n# =============================\n# 5) Data collator\n# =============================\ndef data_collator(batch):\n    audio = [b[\"input_values\"] for b in batch]\n    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n    inputs = processor(audio, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n    inputs[\"labels\"] = labels\n    return inputs\n\n# =============================\n# 6) Weighted loss trainer\n# =============================\nclass_weights = compute_class_weight(\n    \"balanced\", classes=np.array([0,1]), y=train_df[\"label\"].values\n)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.get(\"labels\")\n        outputs = model(**{k:v for k,v in inputs.items() if k!=\"labels\"})\n        logits = outputs.logits\n        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits, labels.to(logits.device))\n        return (loss, outputs) if return_outputs else loss\n\n# =============================\n# 7) Training args (NO eval now)\n# =============================\ntraining_args = TrainingArguments(\n    output_dir=\"./w2v2_final_tmp\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    num_train_epochs=10,\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    warmup_steps=0,                 # no warmup_ratio warning\n    logging_steps=25,\n    save_strategy=\"no\",             # we will manually save at end\n    eval_strategy=\"no\",\n    fp16=torch.cuda.is_available(),\n    seed=SEED,\n    data_seed=SEED,\n    max_grad_norm=1.0\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator\n)\n\ntrainer.train()\n\n# =============================\n# 8) Save FINAL model\n# =============================\nFINAL_DIR = \"/kaggle/working/w2v2_malayalam_final\"\nos.makedirs(FINAL_DIR, exist_ok=True)\n\ntrainer.save_model(FINAL_DIR)\nprocessor.save_pretrained(FINAL_DIR)\n\nprint(\"✅ Saved FINAL W2V2 Malayalam model to:\", FINAL_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T20:09:24.402048Z","iopub.execute_input":"2026-02-27T20:09:24.402479Z","iopub.status.idle":"2026-02-27T20:38:42.257100Z","shell.execute_reply.started":"2026-02-27T20:09:24.402444Z","shell.execute_reply":"2026-02-27T20:38:42.256389Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nFinal train size (train+val): 1688\nFinal train label ratio: 0.466824644549763\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/211 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"328a44c6505147f58f2f43b6dce49662"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\nKey                          | Status     | \n-----------------------------+------------+-\nquantizer.weight_proj.bias   | UNEXPECTED | \nproject_q.weight             | UNEXPECTED | \nquantizer.codevectors        | UNEXPECTED | \nproject_hid.weight           | UNEXPECTED | \nproject_hid.bias             | UNEXPECTED | \nquantizer.weight_proj.weight | UNEXPECTED | \nproject_q.bias               | UNEXPECTED | \nclassifier.weight            | MISSING    | \nprojector.bias               | MISSING    | \nclassifier.bias              | MISSING    | \nprojector.weight             | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1060' max='1060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1060/1060 29:13, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.260621</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.954299</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.595837</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.365985</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.270635</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.184506</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.137102</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.128024</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.097563</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.102713</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.082742</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.061165</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.054116</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.057807</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.070203</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.064675</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.077375</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.056773</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.056616</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.048813</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.041746</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.034882</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.046773</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.047473</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.039822</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.030527</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.048408</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.040258</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.051239</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.021133</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.038236</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.021301</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.029792</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.018934</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.015015</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.029697</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>0.028693</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.016325</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>0.024661</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.026714</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>0.016621</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.025009</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e469445f1e1f4af0897d615c7916a164"}},"metadata":{}},{"name":"stdout","text":"✅ Saved FINAL W2V2 Malayalam model to: /kaggle/working/w2v2_malayalam_final\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import os, glob\nimport pandas as pd\nimport torch\nimport librosa\nfrom transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n\nTEAM_NAME = \"TriVector\"\nRUN = \"run1\"\n\n# ✅ dataset test folder (your actual path)\nBASE = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det\"\nTEST_DIR = os.path.join(BASE, \"Test_set_mal\", \"Test_set_mal\")\n\n# ✅ final trained model (saved earlier)\nFINAL_DIR = \"/kaggle/working/w2v2_best\"\n\nID2LABEL = {0: 0, 1: 1}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nassert os.path.isdir(TEST_DIR), f\"Missing test dir: {TEST_DIR}\"\nassert os.path.isdir(FINAL_DIR), f\"Missing model dir: {FINAL_DIR}\"\n\n# collect test wav files\ntest_files = sorted(glob.glob(os.path.join(TEST_DIR, \"**\", \"*.wav\"), recursive=True))\nprint(\"✅ Test wavs:\", len(test_files))\n\n# load model\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(FINAL_DIR).to(device)\nprocessor = Wav2Vec2Processor.from_pretrained(FINAL_DIR)\nmodel.eval()\n\n@torch.no_grad()\ndef predict_label(file_path, sr=16000, max_seconds=6.0):\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n    y = y[: int(sr * max_seconds)]\n\n    inputs = processor([y], sampling_rate=sr, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    logits = model(**inputs).logits\n    return int(torch.argmax(logits, dim=-1).item())\n\nrows = []\nfor fp in test_files:\n    file_id = os.path.splitext(os.path.basename(fp))[0]\n    rows.append({\"file_name\": file_id, \"label\": predict_label(fp)})\n\nmal_df = pd.DataFrame(rows)\n\n# ✅ save CSV in Kaggle working (same name)\ncsv_name = f\"{TEAM_NAME}_Malayalam_{RUN}.csv\"\ncsv_path = os.path.join(\"/kaggle/working\", csv_name)\n\nmal_df.to_csv(csv_path, index=False)\n\nprint(\"✅ CSV created:\", csv_path)\nprint(mal_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T20:43:07.980640Z","iopub.execute_input":"2026-02-27T20:43:07.981512Z","iopub.status.idle":"2026-02-27T20:43:15.751833Z","shell.execute_reply.started":"2026-02-27T20:43:07.981475Z","shell.execute_reply":"2026-02-27T20:43:15.750964Z"}},"outputs":[{"name":"stdout","text":"✅ Test wavs: 200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/215 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce44498053ad425697cd5abced1ca126"}},"metadata":{}},{"name":"stdout","text":"✅ CSV created: /kaggle/working/TriVector_Malayalam_run1.csv\n  file_name  label\n0        m1      1\n1       m10      0\n2      m100      1\n3      m101      0\n4      m102      1\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Getting Test Data","metadata":{}},{"cell_type":"code","source":"import os, glob, zipfile\nimport pandas as pd\nimport torch\nimport librosa\nfrom transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n\nTEAM_NAME = \"TriVector\"\nRUN = \"run1\"\n\n# ✅ Dataset base (your actual dataset root)\nBASE = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det\"\n\n# ✅ Malayalam test folder (from your os.walk output)\nTEST_DIR = os.path.join(BASE, \"Test_set_mal\", \"Test_set_mal\")\n\n# ✅ Your trained/saved model directory in Kaggle working\n# (choose ONE that exists: w2v2_best or w2v2_malayalam_final)\nFINAL_DIR = \"/kaggle/working/w2v2_best\"   # or \"/kaggle/working/w2v2_malayalam_final\"\n\n# Label mapping (keep 0/1 like you used)\nID2LABEL = {0: 0, 1: 1}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nassert os.path.isdir(TEST_DIR), f\"Missing test dir: {TEST_DIR}\"\nassert os.path.isdir(FINAL_DIR), f\"Missing model dir: {FINAL_DIR}\"\n\n# Collect test wavs\ntest_files = sorted(glob.glob(os.path.join(TEST_DIR, \"**\", \"*.wav\"), recursive=True))\nprint(\"✅ Test wavs:\", len(test_files))\nassert len(test_files) > 0, \"No .wav found in TEST_DIR\"\n\n# Load model + processor\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(FINAL_DIR).to(device)\nprocessor = Wav2Vec2Processor.from_pretrained(FINAL_DIR)\nmodel.eval()\n\n@torch.no_grad()\ndef predict_label(file_path, sr=16000, max_seconds=6.0):\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n    max_len = int(sr * max_seconds)\n    if len(y) > max_len:\n        y = y[:max_len]\n\n    inputs = processor([y], sampling_rate=sr, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    logits = model(**inputs).logits\n    pred_id = int(torch.argmax(logits, dim=-1).item())\n    return ID2LABEL[pred_id]\n\n# Predict\nrows = []\nfor fp in test_files:\n    file_id = os.path.splitext(os.path.basename(fp))[0]  # no .wav\n    rows.append({\"file_name\": file_id, \"label\": predict_label(fp)})\n\nmal_df = pd.DataFrame(rows)\n\n# Save CSV in Kaggle working\ncsv_name = f\"{TEAM_NAME}_Malayalam_{RUN}.csv\"\ncsv_path = os.path.join(\"/kaggle/working\", csv_name)\nmal_df.to_csv(csv_path, index=False)\nprint(\"✅ CSV saved:\", csv_path)\nprint(mal_df.head())\n\n# Zip it (submission)\nzip_name = f\"{TEAM_NAME}_Malayalam_{RUN}.zip\"\nzip_path = os.path.join(\"/kaggle/working\", zip_name)\n\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n    z.write(csv_path, arcname=csv_name)\n\nprint(\"✅ ZIP saved:\", zip_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = \"/kaggle/working/TriVector_Malayalam_run1.csv\"  # existing CSV\n\ndf = pd.read_csv(csv_path)\n\n# 0/1 → text labels\ndf[\"label\"] = df[\"label\"].map({0: \"Non-depressed\", 1: \"Depressed\"})\n\n# overwrite same file\ndf.to_csv(csv_path, index=False)\n\nprint(\"✅ Converted to text labels:\", csv_path)\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nTEST_CSV = \"/kaggle/input/datasets/tahmimahoque/malayalam-gt/Malayalam_GT.xlsx - mal.csv\"\nAUDIO_ROOT = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det/Test_set_mal/Test_set_mal\"\n\ntest_df = pd.read_csv(TEST_CSV)\ntest_df.columns = [c.strip() for c in test_df.columns]\n\nprint(\"Columns:\", test_df.columns.tolist())\nprint(test_df.head())\n\n# --- detect label column ---\nlabel_col_candidates = [c for c in test_df.columns if c.lower().strip() in [\"label\", \"labels\", \"gt\", \"ground_truth\", \"target\", \"y\"]]\nif not label_col_candidates:\n    raise KeyError(\"No label column found. Rename the ground-truth column to 'label'.\")\nlabel_col = label_col_candidates[0]\n\n# --- detect filename/id column ---\nfname_candidates = []\nfor c in test_df.columns:\n    if c == label_col:\n        continue\n    cl = c.lower().strip()\n    if any(k in cl for k in [\"file\", \"fname\", \"filename\", \"name\", \"utt\", \"id\"]):\n        fname_candidates.append(c)\n\nfname_col = fname_candidates[0] if fname_candidates else [c for c in test_df.columns if c != label_col][0]\n\nprint(\"Using label column:\", label_col)\nprint(\"Using filename column:\", fname_col)\n\n# --- build file_path ---\ndef to_wav_path(x):\n    s = str(x).strip()\n    if s.lower().endswith(\".wav\"):\n        return os.path.join(AUDIO_ROOT, s)\n    return os.path.join(AUDIO_ROOT, s + \".wav\")\n\ntest_df[\"file_path\"] = test_df[fname_col].apply(to_wav_path)\n\n# --- map labels to 0/1 ---\ndef normalize_label(v):\n    s = str(v).strip().lower()\n    if s in [\"1\", \"1.0\", \"d\", \"dep\", \"depressed\", \"depression\", \"positive\", \"yes\", \"true\"]:\n        return 1\n    if s in [\"0\", \"0.0\", \"nd\", \"nondepressed\", \"non_depressed\", \"non-depressed\", \"negative\", \"no\", \"false\"]:\n        return 0\n    return np.nan\n\ntest_df[\"label\"] = test_df[label_col].apply(normalize_label)\n\nbad = test_df[\"label\"].isna().sum()\nprint(\"Unmapped labels:\", bad)\nif bad > 0:\n    print(\"Unique raw labels:\", sorted(test_df[label_col].astype(str).str.strip().unique().tolist())[:50])\n    raise ValueError(\"Some labels could not be mapped. Update normalize_label() mapping.\")\n\ntest_df[\"label\"] = test_df[\"label\"].astype(int)\n\nprint(\"\\nSample file paths:\")\nprint(test_df[\"file_path\"].head())\nprint(\"\\nLabel distribution:\")\nprint(test_df[\"label\"].value_counts())\n\n# optional existence check\nmissing = [p for p in test_df[\"file_path\"].head(20).tolist() if not os.path.exists(p)]\nprint(\"\\nMissing among first 20 paths:\", len(missing))\nif missing:\n    print(\"Example missing path:\", missing[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T21:16:47.092738Z","iopub.execute_input":"2026-02-27T21:16:47.093473Z","iopub.status.idle":"2026-02-27T21:16:47.151825Z","shell.execute_reply.started":"2026-02-27T21:16:47.093438Z","shell.execute_reply":"2026-02-27T21:16:47.151175Z"}},"outputs":[{"name":"stdout","text":"Columns: ['filename', 'Label']\n  filename Label\n0   m1.wav     D\n1   m2.wav    ND\n2   m3.wav     D\n3   m4.wav    ND\n4   m5.wav     D\nUsing label column: Label\nUsing filename column: filename\nUnmapped labels: 0\n\nSample file paths:\n0    /kaggle/input/datasets/tahmimahoque/depression...\n1    /kaggle/input/datasets/tahmimahoque/depression...\n2    /kaggle/input/datasets/tahmimahoque/depression...\n3    /kaggle/input/datasets/tahmimahoque/depression...\n4    /kaggle/input/datasets/tahmimahoque/depression...\nName: file_path, dtype: object\n\nLabel distribution:\nlabel\n0    102\n1     98\nName: count, dtype: int64\n\nMissing among first 20 paths: 0\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# Classification Report on Labelled Test data","metadata":{}},{"cell_type":"code","source":"import torch\nimport librosa\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nwav2vec_model.eval()\n\n@torch.no_grad()\ndef predict_id(file_path, sr=16000, max_seconds=6.0):\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n    y = y[: int(sr * max_seconds)]\n\n    inputs = processor([y], sampling_rate=sr, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    logits = wav2vec_model(**inputs).logits\n    return int(torch.argmax(logits, dim=-1).item())\n\n# predictions\ny_true = test_df[\"label\"].astype(int).tolist()\ny_pred = [predict_id(p) for p in test_df[\"file_path\"].tolist()]\n\n# ======================\n# Classification Report\n# ======================\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(\n    y_true, y_pred,\n    target_names=[\"Non-depressed\", \"Depressed\"],\n    digits=4\n))\n\n# ======================\n# Confusion Matrix\n# ======================\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(4,4))\nplt.imshow(cm, cmap=\"Blues\")\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.colorbar()\n\nclasses = [\"Non-depressed\", \"Depressed\"]\nplt.xticks([0,1], classes)\nplt.yticks([0,1], classes)\n\n# numbers inside cells\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, cm[i, j],\n                 ha=\"center\", va=\"center\",\n                 color=\"white\" if cm[i,j] > cm.max()/2 else \"black\",\n                 fontsize=12,\n                 fontweight=\"bold\")\n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T21:21:50.930165Z","iopub.execute_input":"2026-02-27T21:21:50.931030Z","iopub.status.idle":"2026-02-27T21:21:58.462966Z","shell.execute_reply.started":"2026-02-27T21:21:50.930997Z","shell.execute_reply":"2026-02-27T21:21:58.462386Z"}},"outputs":[{"name":"stdout","text":"\nClassification Report:\n\n               precision    recall  f1-score   support\n\nNon-depressed     1.0000    0.9804    0.9901       102\n    Depressed     0.9800    1.0000    0.9899        98\n\n     accuracy                         0.9900       200\n    macro avg     0.9900    0.9902    0.9900       200\n weighted avg     0.9902    0.9900    0.9900       200\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 400x400 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}