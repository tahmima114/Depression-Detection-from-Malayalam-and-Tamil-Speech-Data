{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14985459,"datasetId":9592660,"databundleVersionId":15859169},{"sourceType":"datasetVersion","sourceId":14986512,"datasetId":9593253,"databundleVersionId":15860320}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nBASE = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det\"\n\nprint(\"Malayalam dep train:\", len(os.listdir(BASE + \"/Malayalam/Malayalam/Depressed/Train_set\")))\nprint(\"Malayalam nondep train:\", len(os.listdir(BASE + \"/Malayalam/Malayalam/Non_depressed/Train_set\")))\nprint(\"Tamil dep train:\", len(os.listdir(BASE + \"/Tamil/Tamil/Depressed/Train_set\")))\nprint(\"Tamil nondep train:\", len(os.listdir(BASE + \"/Tamil/Tamil/Non-depressed/Train_set\")))\nprint(\"Malayalam test:\", len(os.listdir(BASE + \"/Test_set_mal/Test_set_mal\")))\nprint(\"Tamil test:\", len(os.listdir(BASE + \"/Test-set-tamil/Test-set-tamil\")))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:12:53.635953Z","iopub.execute_input":"2026-03-01T09:12:53.636184Z","iopub.status.idle":"2026-03-01T09:12:53.778793Z","shell.execute_reply.started":"2026-03-01T09:12:53.636149Z","shell.execute_reply":"2026-03-01T09:12:53.778035Z"}},"outputs":[{"name":"stdout","text":"Malayalam dep train: 788\nMalayalam nondep train: 900\nTamil dep train: 454\nTamil nondep train: 920\nMalayalam test: 200\nTamil test: 160\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Reproducibility\n\nRandom seeds are fixed for Python, NumPy, and PyTorch to ensure deterministic and reproducible results. CUDA determinism is enabled, and a worker seed function is used for DataLoader consistency","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport torch\n\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef seed_worker(worker_id):\n    worker_seed = SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:13:03.280261Z","iopub.execute_input":"2026-03-01T09:13:03.280561Z","iopub.status.idle":"2026-03-01T09:13:03.287416Z","shell.execute_reply.started":"2026-03-01T09:13:03.280534Z","shell.execute_reply":"2026-03-01T09:13:03.286719Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Resampling\n\nResamples Malayalam depression dataset audio to 16 kHz mono while preserving folder structure and logging errors.","metadata":{}},{"cell_type":"code","source":"import os, glob, logging\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom tqdm import tqdm\n\n\n# Paths (Kaggle)\n\nBASE = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det\"\nRAW_ROOT = os.path.join(BASE, \"Malayalam\", \"Malayalam\")      # contains Depressed/ and Non_depressed/\nOUT_ROOT = \"/kaggle/working/data/Malayalam_16k\"              # writable output\nos.makedirs(OUT_ROOT, exist_ok=True)\n\n\n# Logging\nlogging.basicConfig(\n    filename=\"resample_errors.log\",\n    level=logging.ERROR,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\n\ndef resample_audio(in_path, out_path, target_sr=16000):\n    try:\n        y, sr = librosa.load(in_path, sr=None, mono=True)\n\n        if y is None or len(y) == 0:\n            raise ValueError(\"Empty audio\")\n\n        if float(np.sqrt(np.mean(y**2))) < 1e-6:\n            raise ValueError(\"Near-silent audio\")\n\n        if sr != target_sr:\n            try:\n                y = librosa.resample(y, orig_sr=sr, target_sr=target_sr, res_type=\"soxr_hq\")\n            except Exception:\n                y = librosa.resample(y, orig_sr=sr, target_sr=target_sr, res_type=\"kaiser_best\")\n\n        y = np.clip(y, -1.0, 1.0)\n\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n        sf.write(out_path, y, target_sr)\n\n    except Exception as e:\n        logging.error(f\"Error processing {in_path}: {e}\")\n        print(f\"[Resample error] {in_path} -> {e}\")\n\ndef process_folder(src_folder, dst_folder):\n    wav_files = glob.glob(os.path.join(src_folder, \"**/*.wav\"), recursive=True)\n    if len(wav_files) == 0:\n        print(f\"[Skip] No wav files found in: {src_folder}\")\n        return\n\n    for in_path in tqdm(wav_files, desc=f\"Resampling {os.path.basename(src_folder)}\"):\n        \n        rel = os.path.relpath(in_path, src_folder)\n        out_path = os.path.join(dst_folder, rel)\n\n        if os.path.exists(out_path):\n            continue\n\n        resample_audio(in_path, out_path, target_sr=16000)\n\npairs = [\n    (\"Depressed\", \"Depressed\"),\n    (\"Non_depressed\", \"Non_depressed\"),\n]\n\nfor src_sub, dst_sub in pairs:\n    src_path = os.path.join(RAW_ROOT, src_sub)\n    dst_path = os.path.join(OUT_ROOT, dst_sub)\n    process_folder(src_path, dst_path)\nprint(\"\\n Resampling complete. Output root:\", OUT_ROOT)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:13:08.884716Z","iopub.execute_input":"2026-03-01T09:13:08.885295Z","iopub.status.idle":"2026-03-01T09:14:07.678551Z","shell.execute_reply.started":"2026-03-01T09:13:08.885267Z","shell.execute_reply":"2026-03-01T09:14:07.677728Z"}},"outputs":[{"name":"stderr","text":"Resampling Depressed: 100%|██████████| 788/788 [00:29<00:00, 26.61it/s]\nResampling Non_depressed: 100%|██████████| 900/900 [00:24<00:00, 36.51it/s]","output_type":"stream"},{"name":"stdout","text":"\n Resampling complete. Output root: /kaggle/working/data/Malayalam_16k\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Creating Metadata CSV","metadata":{}},{"cell_type":"code","source":"import os, glob\nimport pandas as pd\n\nDATA_ROOT = \"/kaggle/working/data/Malayalam_16k\"\n\nrows = []\ndef collect(folder, label):\n    files = glob.glob(os.path.join(folder, \"**/*.wav\"), recursive=True)\n    for f in files:\n        rows.append({\n            \"file_path\": f,\n            \"label\": label,\n            \"fname\": os.path.splitext(os.path.basename(f))[0]\n        })\ncollect(os.path.join(DATA_ROOT, \"Depressed\"), 1)\ncollect(os.path.join(DATA_ROOT, \"Non_depressed\"), 0)\ndf = pd.DataFrame(rows)\nassert len(df) > 0, \"No wav files found.\"\n\n# Deterministic shuffle\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nmetadata_path = os.path.join(DATA_ROOT, \"malayalam_metadata.csv\")\ndf.to_csv(metadata_path, index=False)\n\nprint(\"CSV saved at:\", metadata_path)\nprint(df.head())\nprint(\"\\n Label counts:\")\nprint(df[\"label\"].value_counts())\nprint(\"\\n Total files:\", len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:14:13.706068Z","iopub.execute_input":"2026-03-01T09:14:13.706549Z","iopub.status.idle":"2026-03-01T09:14:14.034382Z","shell.execute_reply.started":"2026-03-01T09:14:13.706518Z","shell.execute_reply":"2026-03-01T09:14:14.033718Z"}},"outputs":[{"name":"stdout","text":"CSV saved at: /kaggle/working/data/Malayalam_16k/malayalam_metadata.csv\n                                           file_path  label        fname\n0  /kaggle/working/data/Malayalam_16k/Non_depress...      0     ND5_0165\n1  /kaggle/working/data/Malayalam_16k/Depressed/T...      1  D_F001_41_4\n2  /kaggle/working/data/Malayalam_16k/Depressed/T...      1  D_F001_13_2\n3  /kaggle/working/data/Malayalam_16k/Depressed/T...      1  D_F001_12_4\n4  /kaggle/working/data/Malayalam_16k/Non_depress...      0     ND5_0166\n\n Label counts:\nlabel\n0    900\n1    788\nName: count, dtype: int64\n\n Total files: 1688\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Sanity Check of Data","metadata":{}},{"cell_type":"code","source":"import os, re\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nfrom tqdm import tqdm\nfrom sklearn.model_selection import GroupShuffleSplit\n\n\n# Load metadata\nDATA_ROOT = \"/kaggle/working/data/Malayalam_16k\"\ncsv_path = os.path.join(DATA_ROOT, \"malayalam_metadata.csv\")\ndf = pd.read_csv(csv_path)\nprint(\"Loaded CSV:\", len(df))\n\n\n# Speaker & utterance parsing \ndef extract_speaker(fname):\n   \n    parts = fname.split(\"_\")\n    if fname.startswith(\"ND\"):\n        return parts[0]            # ND1, ND3, ...\n    if fname.startswith(\"D_\"):\n        return \"_\".join(parts[:2]) # D_F001, D_A032, ...\n    return parts[0]                # fallback\n\ndef extract_utt_base(fname):\n  \n    return re.sub(r\"_\\d+$\", \"\", fname)\n\ndf[\"speaker\"] = df[\"fname\"].apply(extract_speaker)\ndf[\"utt_base\"] = df[\"fname\"].apply(extract_utt_base)\n\n\n# Basic dataset stats\nprint(\"\\n[Label distribution]\")\nprint(df[\"label\"].value_counts())\n\nprint(\"\\n[Unique speakers per class]\")\nprint(df.groupby(\"label\")[\"speaker\"].nunique().rename({0:\"Non-depressed\", 1:\"Depressed\"}))\nprint(\"\\n[Top speakers by sample count]\")\nprint(df[\"speaker\"].value_counts().head(10))\n\n\n# Repeat-utterance analysis\n\nrepeat_counts = (\n    df.groupby([\"speaker\", \"utt_base\"])\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values(\"count\", ascending=False)\n)\n\nrepeats = repeat_counts[repeat_counts[\"count\"] > 1]\n\nprint(\"\\n[Repeated utterances by same speaker]\")\nprint(\"Number of repeated (speaker, utt_base) pairs:\", len(repeats))\nif len(repeats) > 0:\n    print(repeats.head(10))\n\n\n# Audio health checks\ndef audio_stats(path):\n    try:\n        info = sf.info(path)\n        sr = info.samplerate\n        dur = info.frames / float(sr)\n\n        y, _ = sf.read(path, dtype=\"float32\", always_2d=False)\n        if y.ndim > 1:\n            y = np.mean(y, axis=1)\n\n        sil_ratio = float(np.mean(np.abs(y) < 1e-4))\n        rms = float(np.sqrt(np.mean(y**2)) + 1e-12)\n\n        return sr, dur, sil_ratio, rms, None\n    except Exception as e:\n        return None, None, None, None, str(e)\n\nrows = []\nfor p in tqdm(df[\"file_path\"], desc=\"Scanning audio\"):\n    rows.append(audio_stats(p))\n\ndf[[\"sr\",\"dur_sec\",\"sil_ratio\",\"rms\",\"err\"]] = pd.DataFrame(rows, index=df.index)\n\nprint(\"\\n[Read errors]:\", df[\"err\"].notna().sum())\n\nprint(\"\\n[Sample rate distribution]\")\nprint(df[\"sr\"].value_counts())\n\nprint(\"\\n[Duration stats (sec)]\")\nprint(df[\"dur_sec\"].describe(percentiles=[0.01,0.05,0.5,0.95,0.99]))\n\nprint(\"\\n[Silence ratio stats]\")\nprint(df[\"sil_ratio\"].describe(percentiles=[0.01,0.05,0.5,0.95,0.99]))\n\nprint(\"\\n[Potential issues]\")\nprint(\"Too short (<0.3s):\", (df[\"dur_sec\"] < 0.3).sum())\nprint(\"Very silent (sil_ratio > 0.98):\", (df[\"sil_ratio\"] > 0.98).sum())\n\n\n# Speaker-safe split simulation\n\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, val_idx = next(gss.split(df, y=df[\"label\"], groups=df[\"speaker\"]))\n\ntrain_df = df.iloc[train_idx]\nval_df   = df.iloc[val_idx]\n\nspeaker_overlap = set(train_df[\"speaker\"]).intersection(set(val_df[\"speaker\"]))\nutt_overlap = set(zip(train_df[\"speaker\"], train_df[\"utt_base\"])) & \\\n              set(zip(val_df[\"speaker\"], val_df[\"utt_base\"]))\n\nprint(\"\\n[Speaker-safe split check]\")\nprint(\"Train speakers:\", train_df[\"speaker\"].nunique())\nprint(\"Val speakers:\", val_df[\"speaker\"].nunique())\nprint(\"Speaker overlap:\", len(speaker_overlap))\n\nprint(\"Overlap of (speaker, utt_base) across splits:\", len(utt_overlap))\nif len(utt_overlap) > 0:\n    print(\"Example overlaps:\", list(utt_overlap)[:10])\n\nprint(\"\\n Sanity check done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:14:22.775222Z","iopub.execute_input":"2026-03-01T09:14:22.775919Z","iopub.status.idle":"2026-03-01T09:14:24.167956Z","shell.execute_reply.started":"2026-03-01T09:14:22.775889Z","shell.execute_reply":"2026-03-01T09:14:24.167119Z"}},"outputs":[{"name":"stdout","text":"Loaded CSV: 1688\n\n[Label distribution]\nlabel\n0    900\n1    788\nName: count, dtype: int64\n\n[Unique speakers per class]\nlabel\nNon-depressed      5\nDepressed        127\nName: speaker, dtype: int64\n\n[Top speakers by sample count]\nspeaker\nD_F001     284\nND5        180\nND3        180\nND4        180\nND2        180\nND1        180\nD_S0035      4\nD_S0053      4\nD_S0021      4\nD_A044       4\nName: count, dtype: int64\n\n[Repeated utterances by same speaker]\nNumber of repeated (speaker, utt_base) pairs: 138\n    speaker utt_base  count\n389     ND1      ND1    180\n392     ND4      ND4    180\n390     ND2      ND2    180\n391     ND3      ND3    180\n393     ND5      ND5    180\n28   D_A029   D_A029      4\n26   D_A027   D_A027      4\n27   D_A028   D_A028      4\n24   D_A025   D_A025      4\n25   D_A026   D_A026      4\n","output_type":"stream"},{"name":"stderr","text":"Scanning audio: 100%|██████████| 1688/1688 [00:01<00:00, 1426.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n[Read errors]: 0\n\n[Sample rate distribution]\nsr\n16000    1688\nName: count, dtype: int64\n\n[Duration stats (sec)]\ncount    1688.000000\nmean        4.714041\nstd         1.417860\nmin         1.522625\n1%          1.979884\n5%          2.449813\n50%         4.644813\n95%         6.943516\n99%         8.261785\nmax        10.360875\nName: dur_sec, dtype: float64\n\n[Silence ratio stats]\ncount    1688.000000\nmean        0.052202\nstd         0.100364\nmin         0.000746\n1%          0.000975\n5%          0.001190\n50%         0.005252\n95%         0.294750\n99%         0.388547\nmax         0.579842\nName: sil_ratio, dtype: float64\n\n[Potential issues]\nToo short (<0.3s): 0\nVery silent (sil_ratio > 0.98): 0\n\n[Speaker-safe split check]\nTrain speakers: 105\nVal speakers: 27\nSpeaker overlap: 0\nOverlap of (speaker, utt_base) across splits: 0\n\n Sanity check done.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/data/Malayalam_16k/malayalam_metadata.csv\")\n\ndef extract_speaker(fname):\n    # Non-depressed speakers: ND1, ND2\n    m = re.match(r\"^(ND\\d+)_\", fname, flags=re.IGNORECASE)\n    if m:\n        return m.group(1).upper()\n\n    \n    m = re.match(r\"^(D)_([FMS]\\d{3})_\", fname, flags=re.IGNORECASE)\n    if m:\n        return f\"{m.group(1).upper()}_{m.group(2).upper()}\"\n\n    # If it's depressed but doesn't match known speaker pattern, \n    if fname.upper().startswith(\"D_\"):\n        return \"D_OTHER\"\n\n    return \"UNKNOWN\"\n\ndef extract_utt_base(fname):\n    # remove trailing repeat suffix\n    return re.sub(r\"_\\d+$\", \"\", fname.upper())\n\ndf[\"speaker_fixed\"] = df[\"fname\"].apply(extract_speaker)\ndf[\"utt_base_fixed\"] = df[\"fname\"].apply(extract_utt_base)\n\nprint(\"Unique speakers per class (fixed):\")\nprint(df.groupby(\"label\")[\"speaker_fixed\"].nunique().rename({0:\"Non-depressed\", 1:\"Depressed\"}))\n\nprint(\"\\nTop speakers (fixed):\")\nprint(df[\"speaker_fixed\"].value_counts().head(20))\n\nprint(\"\\nHow many D_OTHER?\")\nprint((df[\"speaker_fixed\"]==\"D_OTHER\").sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:14:54.283686Z","iopub.execute_input":"2026-03-01T09:14:54.283997Z","iopub.status.idle":"2026-03-01T09:14:54.307230Z","shell.execute_reply.started":"2026-03-01T09:14:54.283971Z","shell.execute_reply":"2026-03-01T09:14:54.306467Z"}},"outputs":[{"name":"stdout","text":"Unique speakers per class (fixed):\nlabel\nNon-depressed     5\nDepressed        11\nName: speaker_fixed, dtype: int64\n\nTop speakers (fixed):\nspeaker_fixed\nD_OTHER    477\nD_F001     284\nND3        180\nND5        180\nND4        180\nND2        180\nND1        180\nD_S005       3\nD_S009       3\nD_S007       3\nD_S006       3\nD_S003       3\nD_S002       3\nD_S008       3\nD_S001       3\nD_S004       3\nName: count, dtype: int64\n\nHow many D_OTHER?\n477\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Speaker-Aware Leakage-Free Train–Validation Split\nCreates speaker-aware, leakage-free stratified train–validation splits for the Malayalam dataset using grouped cross-validation.","metadata":{}},{"cell_type":"code","source":"import os, re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedGroupKFold\n\n\nIN_PATH  = \"/kaggle/working/data/Malayalam_16k/malayalam_metadata.csv\"\nOUT_PATH = \"/kaggle/working/data/Malayalam_16k/malayalam_metadata_with_split.csv\"\n\ndf = pd.read_csv(IN_PATH)\ndef extract_speaker(fname: str) -> str:\n    \n    base = os.path.basename(str(fname))\n    stem = os.path.splitext(base)[0]  # remove .wav/.WAV etc\n    s = stem.upper()\n\n    # ND speakers: \n    m = re.match(r\"^(ND\\d+)_\", s)\n    if m:\n        return m.group(1)\n\n    # Depressed explicit:\n    m = re.match(r\"^(D)_([A-Z]\\d{3,4})_\", s)\n    if m:\n        return f\"{m.group(1)}_{m.group(2)}\"\n\n    \n    # This prevents collapsing many files into D_OTHER if token exists.\n    m = re.match(r\"^(D)_(.+?)_\", s)\n    if m:\n        token = re.sub(r\"[^A-Z0-9]+\", \"\", m.group(2))  # sanitize\n        if token:\n            return f\"D_{token}\"\n\n    # If it starts with D_ but no second underscore \n    if s.startswith(\"D_\"):\n        return \"D_UNK\"\n\n    return \"UNKNOWN\"\n\n\ndef utt_base(fname: str) -> str:\n    \n    base = os.path.basename(str(fname))\n    stem = os.path.splitext(base)[0].upper()    \n    stem = re.sub(r\"(_SEG)?_?\\d{1,4}$\", \"\", stem)                  \n    stem = re.sub(r\"_(CHUNK|PART|CLIP|UTT|SEGMENT)_?\\d{1,4}$\", \"\", stem)\n\n    return stem\n\n\ndf[\"speaker_fixed\"] = df[\"fname\"].apply(extract_speaker)\ndf[\"utt_base_fixed\"] = df[\"fname\"].apply(utt_base)\n\n\n# Grouping strategy\n\ndef make_group(row):\n   \n    spk = row[\"speaker_fixed\"]\n    if spk in (\"UNKNOWN\", \"D_UNK\"):\n        return \"UTT_\" + row[\"utt_base_fixed\"]\n    return spk\n\ndf[\"group_id\"] = df.apply(make_group, axis=1)\n\n\n# Basic sanity checks (compliance)\n\nprint(\"Loaded CSV:\", len(df))\nprint(\"\\n[Label distribution]\")\nprint(df[\"label\"].value_counts())\n\nprint(\"\\n[Top speakers]\")\nprint(df[\"speaker_fixed\"].value_counts().head(20))\n\nunknown_rate = (df[\"speaker_fixed\"].isin([\"UNKNOWN\", \"D_UNK\"])).mean()\nprint(f\"\\nUNKNOWN/D_UNK rate: {unknown_rate:.3f}\")\n# If too many unknowns, parsing likely wrong \nif unknown_rate > 0.05:\n    print(\"Many UNKNOWN/D_UNK speakers.\")\n\noverall_ratio = df[\"label\"].mean()\nprint(\"\\nOverall depressed ratio:\", overall_ratio)\n\n\n# StratifiedGroupKFold split\n\nsgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n\ncandidates = []\nX = np.zeros((len(df), 1))  # dummy features\n\nfor fold, (train_idx, val_idx) in enumerate(sgkf.split(X, df[\"label\"], groups=df[\"group_id\"])):\n    val = df.iloc[val_idx]\n    train = df.iloc[train_idx]\n\n    val_counts = val[\"label\"].value_counts().to_dict()\n    train_counts = train[\"label\"].value_counts().to_dict()\n\n    val_size = len(val)\n    has_both = (0 in val_counts) and (1 in val_counts)\n    val_ratio = val[\"label\"].mean() if val_size > 0 else 0.0\n\n    score = abs(val_ratio - overall_ratio)\n    if not has_both:\n        score += 10.0\n    # keep val reasonably sized \n    if val_size < 200:\n        score += (200 - val_size) / 50.0\n\n    candidates.append((score, fold, val_size, val_counts, val_ratio, train_counts))\n\nprint(\"\\nFold candidates:\")\nfor score, fold, val_size, val_counts, val_ratio, train_counts in sorted(candidates, key=lambda x: x[0]):\n    print(f\"fold={fold}  score={score:.3f}  val_size={val_size}  val_counts={val_counts}  val_ratio={val_ratio:.3f}\")\n\nbest = sorted(candidates, key=lambda x: x[0])[0]\n_, best_fold, *_ = best\nprint(\"\\n Choosing best fold:\", best_fold)\n\n# rerun to get indices for chosen fold\nfor fold, (train_idx, val_idx) in enumerate(sgkf.split(X, df[\"label\"], groups=df[\"group_id\"])):\n    if fold == best_fold:\n        break\n\ndf[\"split\"] = \"train\"\ndf.loc[val_idx, \"split\"] = \"val\"\n\n\n# Compliance verification\n\ntrain_df = df[df[\"split\"] == \"train\"]\nval_df = df[df[\"split\"] == \"val\"]\n\ngroup_overlap = set(train_df[\"group_id\"]) & set(val_df[\"group_id\"])\nspeaker_overlap = set(train_df[\"speaker_fixed\"]) & set(val_df[\"speaker_fixed\"])\n\nprint(\"\\nGroup overlap:\", len(group_overlap))\nprint(\"Speaker overlap:\", len(speaker_overlap))\n\n# Extra: prevent exact same utt_base overlap \nutt_overlap = set(train_df[\"utt_base_fixed\"]) & set(val_df[\"utt_base_fixed\"])\nprint(\"Utt_base overlap:\", len(utt_overlap))\n\nprint(\"\\nTrain label counts:\")\nprint(train_df[\"label\"].value_counts(), \" Train ratio:\", train_df[\"label\"].mean())\n\nprint(\"\\nVal label counts:\")\nprint(val_df[\"label\"].value_counts(), \" Val ratio:\", val_df[\"label\"].mean())\n\n\n# Save\n\ndf.to_csv(OUT_PATH, index=False)\nprint(\"\\n Saved split CSV:\", OUT_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:15:32.237962Z","iopub.execute_input":"2026-03-01T09:15:32.238505Z","iopub.status.idle":"2026-03-01T09:15:32.381424Z","shell.execute_reply.started":"2026-03-01T09:15:32.238477Z","shell.execute_reply":"2026-03-01T09:15:32.380482Z"}},"outputs":[{"name":"stdout","text":"Loaded CSV: 1688\n\n[Label distribution]\nlabel\n0    900\n1    788\nName: count, dtype: int64\n\n[Top speakers]\nspeaker_fixed\nD_F001    284\nND5       180\nND3       180\nND4       180\nND2       180\nND1       180\nD_UNK      64\nD_A013      4\nD_A021      4\nD_A034      4\nD_A006      4\nD_A002      4\nD_A044      4\nD_A029      4\nD_A001      4\nD_A037      4\nD_A045      4\nD_A056      4\nD_A035      4\nD_A016      4\nName: count, dtype: int64\n\nUNKNOWN/D_UNK rate: 0.038\n\nOverall depressed ratio: 0.466824644549763\n\nFold candidates:\nfold=3  score=0.053  val_size=307  val_counts={0: 180, 1: 127}  val_ratio=0.414\nfold=2  score=0.059  val_size=759  val_counts={1: 399, 0: 360}  val_ratio=0.526\nfold=1  score=0.314  val_size=425  val_counts={0: 360, 1: 65}  val_ratio=0.153\nfold=4  score=10.733  val_size=190  val_counts={1: 190}  val_ratio=1.000\nfold=0  score=14.393  val_size=7  val_counts={1: 7}  val_ratio=1.000\n\n Choosing best fold: 3\n\nGroup overlap: 0\nSpeaker overlap: 0\nUtt_base overlap: 0\n\nTrain label counts:\nlabel\n0    720\n1    661\nName: count, dtype: int64  Train ratio: 0.4786386676321506\n\nVal label counts:\nlabel\n0    180\n1    127\nName: count, dtype: int64  Val ratio: 0.41368078175895767\n\n Saved split CSV: /kaggle/working/data/Malayalam_16k/malayalam_metadata_with_split.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# MFCC","metadata":{}},{"cell_type":"code","source":"\n\nimport os, random, json\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef seed_worker(worker_id: int):\n    worker_seed = SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nDATA_ROOT = \"/kaggle/working/data/Malayalam_16k\"  # change if needed\nSPLIT_CSV = os.path.join(DATA_ROOT, \"malayalam_metadata_with_split.csv\")\n\n# output files\nOUT_DIR = \"/kaggle/working\"\nMODEL_PT = os.path.join(OUT_DIR, \"mfcc_mlp.pt\")\nMETA_JSON = os.path.join(OUT_DIR, \"mfcc_mlp_meta.json\")\nVAL_PRED_CSV = os.path.join(OUT_DIR, \"mfcc_val_preds.csv\")\n\nassert os.path.isfile(SPLIT_CSV), f\"Missing split CSV: {SPLIT_CSV}\"\n\n# MFCC params (keep these consistent for train/val/test)\nSR = 16000\nMAX_SECONDS = 6\nN_MFCC = 20\nMAX_FRAMES = 120\nN_FFT = 400\nHOP_LENGTH = 160\n\n# training params\nBATCH_TRAIN = 32\nBATCH_VAL = 64\nNUM_WORKERS = 2\n\nLR = 5e-4\nWEIGHT_DECAY = 1e-2\nEPOCHS = 30\nPATIENCE = 5\nGRAD_CLIP = 1.0\n\n# augmentation (only on train)\nAUG_PROB = 0.6\nMAX_TIME_MASK = 12\nMAX_FREQ_MASK = 4\n\n# model\nHIDDEN = 128\nDROPOUT = 0.5\n\n\ndf = pd.read_csv(SPLIT_CSV)\nrequired_cols = {\"file_path\", \"label\", \"split\"}\nmissing_cols = required_cols - set(df.columns)\nassert not missing_cols, f\"Split CSV missing columns: {missing_cols}. Found: {list(df.columns)}\"\n\n# basic sanitize\ndf[\"file_path\"] = df[\"file_path\"].astype(str)\ndf[\"split\"] = df[\"split\"].astype(str)\ndf[\"label\"] = df[\"label\"].astype(int)\n\n# check files exist\nbad_paths = df[~df[\"file_path\"].apply(os.path.exists)]\nprint(\"Missing audio rows:\", len(bad_paths))\nif len(bad_paths) > 0:\n    print(bad_paths[[\"file_path\", \"label\", \"split\"]].head(10))\ndf = df[df[\"file_path\"].apply(os.path.exists)].reset_index(drop=True)\n\ntrain_df = df[df[\"split\"].str.lower() == \"train\"].reset_index(drop=True)\nval_df   = df[df[\"split\"].str.lower() == \"val\"].reset_index(drop=True)\n\nassert len(train_df) > 0, \"No train rows found in split CSV.\"\nassert len(val_df) > 0, \"No val rows found in split CSV.\"\n\nprint(\"Train:\", len(train_df), \"| Val:\", len(val_df))\nprint(\"Train label counts:\\n\", train_df[\"label\"].value_counts())\nprint(\"Val label counts:\\n\", val_df[\"label\"].value_counts())\n\n\ndef extract_mfcc_2d(\n    file_path: str,\n    sr: int = SR,\n    n_mfcc: int = N_MFCC,\n    max_frames: int = MAX_FRAMES,\n    n_fft: int = N_FFT,\n    hop_length: int = HOP_LENGTH,\n    max_seconds: int = MAX_SECONDS,\n) -> np.ndarray:\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n\n    # truncate to fixed seconds\n    max_samples = int(sr * max_seconds)\n    if len(y) > max_samples:\n        y = y[:max_samples]\n\n    mfcc = librosa.feature.mfcc(\n        y=y, sr=sr, n_mfcc=n_mfcc,\n        n_fft=n_fft, hop_length=hop_length\n    )  # (n_mfcc, T)\n\n    # pad/truncate time axis\n    T = mfcc.shape[1]\n    if T < max_frames:\n        mfcc = np.pad(mfcc, ((0, 0), (0, max_frames - T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_frames]\n\n    # per-sample normalize\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-9)\n    return mfcc.astype(np.float32)\n\ndef time_freq_mask(\n    mfcc_2d: np.ndarray,\n    max_time_mask: int = MAX_TIME_MASK,\n    max_freq_mask: int = MAX_FREQ_MASK,\n    p: float = AUG_PROB\n) -> np.ndarray:\n    # light SpecAugment-style masking\n    if random.random() > p:\n        return mfcc_2d\n\n    mfcc = mfcc_2d.copy()\n    n_mfcc, T = mfcc.shape\n\n    # time mask\n    t = random.randint(0, max_time_mask)\n    if t > 0:\n        t0 = random.randint(0, max(0, T - t))\n        mfcc[:, t0:t0 + t] = 0.0\n\n    # freq mask\n    f = random.randint(0, max_freq_mask)\n    if f > 0:\n        f0 = random.randint(0, max(0, n_mfcc - f))\n        mfcc[f0:f0 + f, :] = 0.0\n\n    return mfcc\n\n\nclass MFCCDataset(Dataset):\n    def __init__(self, df_: pd.DataFrame, augment: bool = False):\n        self.df = df_.reset_index(drop=True)\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n        path = row[\"file_path\"]\n        label = int(row[\"label\"])\n\n        mfcc = extract_mfcc_2d(path)\n        if self.augment:\n            mfcc = time_freq_mask(mfcc)\n\n        # flatten for MLP\n        x = torch.tensor(mfcc.reshape(-1), dtype=torch.float32)\n        y = torch.tensor(label, dtype=torch.long)\n        return x, y\n\ntrain_ds = MFCCDataset(train_df, augment=True)\nval_ds   = MFCCDataset(val_df, augment=False)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH_TRAIN,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    worker_init_fn=seed_worker,\n    generator=g,\n    pin_memory=True,\n)\nval_loader = DataLoader(\n    val_ds,\n    batch_size=BATCH_VAL,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    worker_init_fn=seed_worker,\n    generator=g,\n    pin_memory=True,\n)\n\n\ntrain_labels = train_df[\"label\"].values\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.array([0, 1]),\n    y=train_labels\n)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\nprint(\"Class weights:\", class_weights.detach().cpu().numpy())\n\n\nINPUT_DIM = N_MFCC * MAX_FRAMES\n\nclass MFCC_MLP(nn.Module):\n    def __init__(self, input_dim: int = INPUT_DIM, hidden: int = HIDDEN, drop: float = DROPOUT):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(hidden, 2),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nmodel = MFCC_MLP().to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n\nbest_f1 = -1.0\nbest_state = None\nbad = 0\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n\n    for x, y in train_loader:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(x)\n        loss = loss_fn(logits, y)\n        loss.backward()\n\n        if GRAD_CLIP is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n\n        optimizer.step()\n        total_loss += float(loss.item())\n\n    train_loss = total_loss / max(1, len(train_loader))\n\n    # validate\n    model.eval()\n    preds, gold = [], []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            logits = model(x)\n            p = torch.argmax(logits, dim=1)\n            preds.extend(p.detach().cpu().numpy().tolist())\n            gold.extend(y.detach().cpu().numpy().tolist())\n\n    val_f1 = f1_score(gold, preds, average=\"macro\")\n    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_macroF1={val_f1:.4f}\")\n\n    if val_f1 > best_f1 + 1e-4:\n        best_f1 = float(val_f1)\n        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        bad = 0\n    else:\n        bad += 1\n        if bad >= PATIENCE:\n            print(\"Early stopping.\")\n            break\n\nassert best_state is not None, \"best_state is None — something went wrong (no improvement on val).\"\n\n# load best\nmodel.load_state_dict(best_state)\nprint(\"\\n Best MFCC Val macro-F1:\", best_f1)\n\n# -----------------------------\n# 8) Save weights + metadata\n# -----------------------------\ntorch.save(best_state, MODEL_PT)\nprint(\"Saved MFCC weights:\", MODEL_PT)\n\nmeta = {\n    \"seed\": SEED,\n    \"sr\": SR,\n    \"max_seconds\": MAX_SECONDS,\n    \"n_mfcc\": N_MFCC,\n    \"max_frames\": MAX_FRAMES,\n    \"n_fft\": N_FFT,\n    \"hop_length\": HOP_LENGTH,\n    \"input_dim\": INPUT_DIM,\n    \"hidden\": HIDDEN,\n    \"dropout\": DROPOUT,\n    \"lr\": LR,\n    \"weight_decay\": WEIGHT_DECAY,\n    \"batch_train\": BATCH_TRAIN,\n    \"batch_val\": BATCH_VAL,\n    \"epochs_max\": EPOCHS,\n    \"patience\": PATIENCE,\n    \"grad_clip\": GRAD_CLIP,\n    \"augment_prob\": AUG_PROB,\n    \"max_time_mask\": MAX_TIME_MASK,\n    \"max_freq_mask\": MAX_FREQ_MASK,\n    \"best_val_macro_f1\": best_f1,\n    \"split_csv\": SPLIT_CSV,\n}\nwith open(META_JSON, \"w\") as f:\n    json.dump(meta, f, indent=2)\nprint(\" Saved meta:\", META_JSON)\n\n# -----------------------------\n# 9) Save VAL predictions (so report can be reproduced without retraining)\n# -----------------------------\nval_preds_df = val_df.copy()\nval_preds_df[\"pred\"] = preds\nval_preds_df.to_csv(VAL_PRED_CSV, index=False)\nprint(\"Saved val predictions:\", VAL_PRED_CSV)\n\nprint(\"\\nVAL Classification Report:\")\nprint(classification_report(gold, preds, digits=4, target_names=[\"Non-Depressed(0)\", \"Depressed(1)\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:45:50.592173Z","iopub.execute_input":"2026-03-01T09:45:50.593174Z","iopub.status.idle":"2026-03-01T09:49:35.349421Z","shell.execute_reply.started":"2026-03-01T09:45:50.593140Z","shell.execute_reply":"2026-03-01T09:49:35.348616Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nMissing audio rows: 0\nTrain: 1381 | Val: 307\nTrain label counts:\n label\n0    720\n1    661\nName: count, dtype: int64\nVal label counts:\n label\n0    180\n1    127\nName: count, dtype: int64\nClass weights: [0.95902777 1.0446293 ]\nEpoch 01 | train_loss=0.5362 | val_macroF1=0.9017\nEpoch 02 | train_loss=0.2319 | val_macroF1=0.9668\nEpoch 03 | train_loss=0.1683 | val_macroF1=0.9800\nEpoch 04 | train_loss=0.1137 | val_macroF1=0.9701\nEpoch 05 | train_loss=0.0842 | val_macroF1=0.9800\nEpoch 06 | train_loss=0.0857 | val_macroF1=0.9800\nEpoch 07 | train_loss=0.0907 | val_macroF1=0.9833\nEpoch 08 | train_loss=0.0934 | val_macroF1=0.9668\nEpoch 09 | train_loss=0.0685 | val_macroF1=0.9833\nEpoch 10 | train_loss=0.0697 | val_macroF1=0.9701\nEpoch 11 | train_loss=0.0643 | val_macroF1=0.9701\nEpoch 12 | train_loss=0.0544 | val_macroF1=0.9833\nEarly stopping.\n\n Best MFCC Val macro-F1: 0.9833052368263636\nSaved MFCC weights: /kaggle/working/mfcc_mlp.pt\n Saved meta: /kaggle/working/mfcc_mlp_meta.json\nSaved val predictions: /kaggle/working/mfcc_val_preds.csv\n\nVAL Classification Report:\n                  precision    recall  f1-score   support\n\nNon-Depressed(0)     1.0000    0.9722    0.9859       180\n    Depressed(1)     0.9621    1.0000    0.9807       127\n\n        accuracy                         0.9837       307\n       macro avg     0.9811    0.9861    0.9833       307\n    weighted avg     0.9843    0.9837    0.9838       307\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Wav2Vec2","metadata":{}},{"cell_type":"code","source":"\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport librosa\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import Dataset\n\nfrom transformers import (\n    Wav2Vec2ForSequenceClassification,\n    Wav2Vec2Processor,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback,\n    set_seed\n)\n\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nset_seed(SEED)\n\n# Load leak-safe split CSV\n\ndata_root = \"/kaggle/working/data/Malayalam_16k\"\nsplit_csv = os.path.join(data_root, \"malayalam_metadata_with_split.csv\")\n\ndf = pd.read_csv(split_csv)\nassert \"file_path\" in df.columns, f\"Missing 'file_path' column. Found: {df.columns.tolist()}\"\nassert \"label\" in df.columns, f\"Missing 'label' column. Found: {df.columns.tolist()}\"\nassert \"split\" in df.columns, f\"Missing 'split' column. Found: {df.columns.tolist()}\"\n\ntrain_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\nval_df   = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n\nprint(\"Train:\", len(train_df), \" Val:\", len(val_df))\nprint(\"Train label ratio:\", train_df[\"label\"].mean(), \" Val label ratio:\", val_df[\"label\"].mean())\n\n\n# augmentation for training only\n\ndef augment_audio(y, sr):\n    y = np.asarray(y, dtype=np.float32)\n    if y.ndim > 1:\n        y = np.mean(y, axis=-1)\n\n    # mild speed\n    if random.random() < 0.5 and len(y) > 0:\n        rate = np.random.uniform(0.95, 1.05)\n        y = librosa.effects.time_stretch(y=y, rate=rate)\n\n    # mild pitch\n    if random.random() < 0.5 and len(y) > 0:\n        n_steps = np.random.uniform(-1.0, 1.0)\n        y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n\n    # mild noise\n    if random.random() < 0.5 and y.size > 0:\n        noise_amp = 0.001 * np.random.uniform() * max(1e-6, float(np.max(np.abs(y))))\n        y = y + noise_amp * np.random.normal(size=y.shape[0]).astype(np.float32)\n\n    return y.astype(np.float32)\n\n#  Dataset \n\nclass MalayalamWav2Vec2Dataset(Dataset):\n    def __init__(self, df, max_seconds=6.0, augment=False, sr=16000):\n        self.df = df.reset_index(drop=True)\n        self.max_len = int(max_seconds * sr)\n        self.augment = augment\n        self.sr = sr\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = row[\"file_path\"]\n        label = int(row[\"label\"])\n\n        y, _ = librosa.load(path, sr=self.sr, mono=True)\n\n        # truncate\n        if len(y) > self.max_len:\n            y = y[:self.max_len]\n\n        if self.augment:\n            y = augment_audio(y, self.sr)\n            y = np.clip(y, -1.0, 1.0)\n\n        \n        return {\"input_values\": y, \"labels\": label}\n\ntrain_dataset = MalayalamWav2Vec2Dataset(train_df, augment=True)\nval_dataset   = MalayalamWav2Vec2Dataset(val_df, augment=False)\n\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    \"facebook/wav2vec2-base\",\n    num_labels=2,\n    problem_type=\"single_label_classification\"\n)\n\n# Freeze encoder \nfor p in model.wav2vec2.parameters():\n    p.requires_grad = False\n\n# slightly higher \nmodel.config.hidden_dropout_prob = 0.2\nmodel.config.attention_dropout = 0.2\n\n\n#  Data collator\n\ndef data_collator(batch):\n    audio = [b[\"input_values\"] for b in batch]\n    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n\n    inputs = processor(\n        audio,\n        sampling_rate=16000,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    inputs[\"labels\"] = labels\n    return inputs\n\ndef compute_metrics(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    preds = np.argmax(logits, axis=-1)\n    return {\"f1\": f1_score(labels, preds, average=\"macro\")}\n\n#  Weighted-loss Trainer\n\ntrain_labels = train_df[\"label\"].values\ncw = compute_class_weight(\"balanced\", classes=np.array([0, 1]), y=train_labels)\nclass_weights = torch.tensor(cw, dtype=torch.float32)  \n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.get(\"labels\")\n        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n        logits = outputs.logits\n\n        weight = class_weights.to(logits.device)\n        loss_fct = nn.CrossEntropyLoss(weight=weight)\n        loss = loss_fct(logits, labels.to(logits.device))\n\n        return (loss, outputs) if return_outputs else loss\n\n\n#  Training args \n\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2_model\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,   # effective batch 8\n    num_train_epochs=15,\n    learning_rate=3e-4,              # higher since encoder frozen\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    logging_steps=25,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",     # compatible key\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    max_grad_norm=1.0,\n    fp16=torch.cuda.is_available(),\n    seed=SEED,\n    data_seed=SEED,\n    remove_unused_columns=False,     # important for custom dict dataset\n    report_to=\"none\"                 # avoids wandb issues on Kaggle\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\ntrainer.train()\n\nmetrics = trainer.evaluate()\nprint(\"\\n Wav2Vec2 Val F1:\", metrics[\"eval_f1\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T10:14:38.946274Z","iopub.execute_input":"2026-03-01T10:14:38.947019Z","iopub.status.idle":"2026-03-01T10:28:13.414581Z","shell.execute_reply.started":"2026-03-01T10:14:38.946987Z","shell.execute_reply":"2026-03-01T10:28:13.414029Z"}},"outputs":[{"name":"stdout","text":"Train: 1381  Val: 307\nTrain label ratio: 0.4786386676321506  Val label ratio: 0.41368078175895767\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/211 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b99aae9a364bc5a631f07055192220"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\nKey                          | Status     | \n-----------------------------+------------+-\nproject_q.bias               | UNEXPECTED | \nproject_hid.weight           | UNEXPECTED | \nquantizer.codevectors        | UNEXPECTED | \nproject_q.weight             | UNEXPECTED | \nproject_hid.bias             | UNEXPECTED | \nquantizer.weight_proj.weight | UNEXPECTED | \nquantizer.weight_proj.bias   | UNEXPECTED | \nprojector.bias               | MISSING    | \nprojector.weight             | MISSING    | \nclassifier.weight            | MISSING    | \nclassifier.bias              | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\nwarmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='522' max='1305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 522/1305 13:25 < 20:12, 0.65 it/s, Epoch 6/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.200700</td>\n      <td>0.439701</td>\n      <td>0.923790</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.414648</td>\n      <td>0.076466</td>\n      <td>0.983111</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.106917</td>\n      <td>0.021069</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.070894</td>\n      <td>0.008755</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.057370</td>\n      <td>0.006497</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.080866</td>\n      <td>0.006631</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad22ffb5a944bbe95ff55f36d79497f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b124d132ac7c4fcbbdb7ba92c695b3e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e089f65a2e94c3986d1747cba7309bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b0e63a9e6db4e679c84c09c937c5714"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa1274d23e3c48f8ba0a836105144c05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8bcfe49f62543dfa69f0529755cd625"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n Wav2Vec2 Val F1: 1.0\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"save_dir = \"/kaggle/working/w2v2_best\"\ntrainer.save_model(save_dir)      # saves model + config\nprocessor.save_pretrained(save_dir)\n\nprint(\" Saved Wav2Vec2 model + processor to:\", save_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T10:28:27.913926Z","iopub.execute_input":"2026-03-01T10:28:27.914501Z","iopub.status.idle":"2026-03-01T10:28:28.776167Z","shell.execute_reply.started":"2026-03-01T10:28:27.914470Z","shell.execute_reply":"2026-03-01T10:28:28.775591Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7d5e3d2bb24fb8bb3e45cfe7e42126"}},"metadata":{}},{"name":"stdout","text":" Saved Wav2Vec2 model + processor to: /kaggle/working/w2v2_best\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\n\nGT_CSV = \"/kaggle/input/datasets/tahmimahoque/malayalam-gt/Malayalam_GT.xlsx - mal.csv\"\nassert \"GT_CSV\" in globals(), \"GT_CSV variable not found. Define GT_CSV first.\"\n\nBASE = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det\"\nTEST_WAV_DIR = os.path.join(BASE, \"Test_set_mal\", \"Test_set_mal\")\n\n\ngt = pd.read_csv(GT_CSV)\nprint(\" GT columns:\", list(gt.columns))\nprint(gt.head(3))\n\nPATH_COL = \"filename\"\nLABEL_COL = \"Label\"\n\nif PATH_COL not in gt.columns:\n    raise KeyError(f\" PATH_COL='{PATH_COL}' not found in GT columns: {list(gt.columns)}\")\nif LABEL_COL not in gt.columns:\n    raise KeyError(f\"LABEL_COL='{LABEL_COL}' not found in GT columns: {list(gt.columns)}\")\n\n\nlabel_map = {\n    \"ND\": 0, \"NON_DEPRESSED\": 0, \"NON-DEPRESSED\": 0, \"0\": 0, 0: 0,\n    \"D\": 1,  \"DEPRESSED\": 1,     \"1\": 1, 1: 1\n}\n\ndef map_label(v):\n    s = str(v).strip().upper()\n    if s in label_map:\n        return int(label_map[s])\n    try:\n        return int(float(s))\n    except:\n        raise ValueError(f\"Unknown label value: {v}\")\n\n\ndef resolve_wav_path(fname):\n    f = str(fname).strip()\n    p = os.path.join(TEST_WAV_DIR, f)\n    if os.path.exists(p):\n        return p\n\n    stem, ext = os.path.splitext(f)\n    for e in [\".wav\", \".WAV\"]:\n        alt = os.path.join(TEST_WAV_DIR, stem + e)\n        if os.path.exists(alt):\n            return alt\n\n    # fallback \n    return p\n\ngt[\"label\"] = gt[LABEL_COL].apply(map_label)\ngt[\"file_path\"] = gt[PATH_COL].apply(resolve_wav_path)\n\nmissing = gt[~gt[\"file_path\"].apply(os.path.exists)]\nprint(\"\\nMissing wavs:\", len(missing))\nif len(missing) > 0:\n    print(missing[[PATH_COL, \"file_path\"]].head(10))\n\ntest_df = gt[gt[\"file_path\"].apply(os.path.exists)].reset_index(drop=True)\nprint(\"\\n Test size:\", len(test_df), \" | label ratio:\", float(test_df[\"label\"].mean()))\n\nclass MalayalamWav2Vec2TestDataset(Dataset):\n    def __init__(self, df, max_seconds=6.0, sr=16000):\n        self.df = df.reset_index(drop=True)\n        self.max_len = int(max_seconds * sr)\n        self.sr = sr\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        y, _ = librosa.load(row[\"file_path\"], sr=self.sr, mono=True)\n        if len(y) > self.max_len:\n            y = y[:self.max_len]\n        return {\"input_values\": y, \"labels\": int(row[\"label\"])}\n\ntest_dataset = MalayalamWav2Vec2TestDataset(test_df, max_seconds=6.0, sr=16000)\n\n\nassert \"trainer\" in globals(), \" trainer not found. Run training cell first.\"\n\nout = trainer.predict(test_dataset)\n\nlogits = out.predictions\ngold = out.label_ids\npreds = np.argmax(logits, axis=-1)\n\nacc = accuracy_score(gold, preds)\nmacro = f1_score(gold, preds, average=\"macro\")\nweighted = f1_score(gold, preds, average=\"weighted\")\n\nprint(\"\\n TEST Accuracy :\", acc)\nprint(\" TEST Macro-F1 :\", macro)\nprint(\" TEST Weighted-F1 :\", weighted)\n\nprint(\"\\n Classification Report (Test):\\n\")\nprint(classification_report(\n    gold, preds,\n    target_names=[\"Non-Depressed\", \"Depressed\"],\n    digits=4\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T10:28:30.832266Z","iopub.execute_input":"2026-03-01T10:28:30.832537Z","iopub.status.idle":"2026-03-01T10:28:35.982522Z","shell.execute_reply.started":"2026-03-01T10:28:30.832513Z","shell.execute_reply":"2026-03-01T10:28:35.981954Z"}},"outputs":[{"name":"stdout","text":" GT columns: ['filename', 'Label']\n  filename Label\n0   m1.wav     D\n1   m2.wav    ND\n2   m3.wav     D\n\nMissing wavs: 0\n\n Test size: 200  | label ratio: 0.49\n\n TEST Accuracy : 0.99\n TEST Macro-F1 : 0.9899959983993598\n TEST Weighted-F1 : 0.99\n\n Classification Report (Test):\n\n               precision    recall  f1-score   support\n\nNon-Depressed     0.9902    0.9902    0.9902       102\n    Depressed     0.9898    0.9898    0.9898        98\n\n     accuracy                         0.9900       200\n    macro avg     0.9900    0.9900    0.9900       200\n weighted avg     0.9900    0.9900    0.9900       200\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Ensemble Weight Tuning & Validation\n\nTunes and evaluates an MFCC–Wav2Vec2 ensemble on the validation set to select optimal weighting and enable final depression prediction.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport librosa\nfrom sklearn.metrics import f1_score\nfrom transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nSPLIT_CSV = \"/kaggle/working/data/Malayalam_16k/malayalam_metadata_with_split.csv\"\nMFCC_WEIGHTS_PATH = \"/kaggle/working/mfcc_mlp.pt\"\nW2V2_DIR = \"/kaggle/working/w2v2_best\"\n\nassert os.path.exists(SPLIT_CSV), f\"Missing: {SPLIT_CSV}\"\nassert os.path.exists(MFCC_WEIGHTS_PATH), f\"Missing: {MFCC_WEIGHTS_PATH}\"\nassert os.path.isdir(W2V2_DIR), f\"Missing dir: {W2V2_DIR}\"\n\ndf = pd.read_csv(SPLIT_CSV)\nneed_cols = {\"split\", \"file_path\", \"label\"}\nmissing_cols = need_cols - set(df.columns)\nassert not missing_cols, f\"CSV missing columns: {missing_cols}. Found: {df.columns.tolist()}\"\n\nval_df = df[df[\"split\"] == \"val\"].reset_index(drop=True)\nprint(\"Val size:\", len(val_df), \" Val ratio:\", float(val_df[\"label\"].mean()))\n\n\ndef extract_mfcc_2d(file_path, sr=16000, n_mfcc=20, max_frames=120, n_fft=400, hop_length=160):\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n    max_samples = sr * 6\n    if len(y) > max_samples:\n        y = y[:max_samples]\n\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n\n    T = mfcc.shape[1]\n    if T < max_frames:\n        mfcc = np.pad(mfcc, ((0, 0), (0, max_frames - T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_frames]\n\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-9)\n    return mfcc.astype(np.float32)\n\n\nclass MFCC_MLP(nn.Module):\n    def __init__(self, input_dim=20*120, hidden=128, drop=0.5):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(hidden, 2)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nmfcc_model = MFCC_MLP().to(device)\nmfcc_state = torch.load(MFCC_WEIGHTS_PATH, map_location=device)\nmfcc_model.load_state_dict(mfcc_state)\nmfcc_model.eval()\nprint(\" MFCC loaded\")\n\n\nwav2vec_model = Wav2Vec2ForSequenceClassification.from_pretrained(W2V2_DIR).to(device)\n\n\ntry:\n    processor = Wav2Vec2Processor.from_pretrained(W2V2_DIR)\nexcept Exception:\n    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n\nwav2vec_model.eval()\nprint(\" W2V2 loaded\")\n\n@torch.no_grad()\ndef mfcc_proba(file_path):\n    mfcc = extract_mfcc_2d(file_path)\n    x = torch.tensor(mfcc.reshape(-1), dtype=torch.float32).unsqueeze(0).to(device)\n    logits = mfcc_model(x)\n    return torch.softmax(logits, dim=-1).detach().cpu().numpy()[0]  # (2,)\n\n@torch.no_grad()\ndef w2v2_proba(file_path, sr=16000, max_seconds=6.0):\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n    max_len = int(sr * max_seconds)\n    if len(y) > max_len:\n        y = y[:max_len]\n\n    inputs = processor([y], sampling_rate=sr, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    logits = wav2vec_model(**inputs).logits\n    return torch.softmax(logits, dim=-1).detach().cpu().numpy()[0]  # (2,)\n\n\nprint(\"\\nPrecomputing VAL probabilities (this is the only slow part)...\")\nP_mfcc, P_w2v2, GOLD = [], [], []\n\nfor i in range(len(val_df)):\n    path = val_df.loc[i, \"file_path\"]\n    label = int(val_df.loc[i, \"label\"])\n\n    P_mfcc.append(mfcc_proba(path))\n    P_w2v2.append(w2v2_proba(path))\n    GOLD.append(label)\n\nP_mfcc = np.vstack(P_mfcc)   # (N,2)\nP_w2v2 = np.vstack(P_w2v2)   # (N,2)\nGOLD = np.array(GOLD)        # (N,)\n\nprint(\" Cached:\", P_mfcc.shape, P_w2v2.shape)\n\ndef eval_ensemble_cached(weight_mfcc):\n    P = weight_mfcc * P_mfcc + (1.0 - weight_mfcc) * P_w2v2\n    preds = np.argmax(P, axis=1)\n    return f1_score(GOLD, preds, average=\"macro\")\n\nweights = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\nbest_w, best_f1 = None, -1.0\n\nprint(\"\\n[Ensemble weight tuning - cached]\")\nfor w in weights:\n    f1 = eval_ensemble_cached(w)\n    print(f\"weight_mfcc={w:.1f} -> val_macroF1={f1:.4f}\")\n    if f1 > best_f1:\n        best_f1, best_w = f1, w\n\nprint(f\"\\n Best weight_mfcc={best_w:.1f} | Best VAL macroF1={best_f1:.4f}\")\n\n\n@torch.no_grad()\ndef ensemble_predict(file_path, weight_mfcc=best_w):\n    p_m = mfcc_proba(file_path)\n    p_w = w2v2_proba(file_path)\n    p = weight_mfcc * p_m + (1.0 - weight_mfcc) * p_w\n    return int(np.argmax(p)), p  # (pred_label, prob_vec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:52:34.431064Z","iopub.execute_input":"2026-03-01T09:52:34.431371Z","iopub.status.idle":"2026-03-01T09:52:44.911006Z","shell.execute_reply.started":"2026-03-01T09:52:34.431344Z","shell.execute_reply":"2026-03-01T09:52:44.907550Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nVal size: 307  Val ratio: 0.41368078175895767\n MFCC loaded\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/215 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2484acedc7e6411da808e79d45e9dc35"}},"metadata":{}},{"name":"stdout","text":" W2V2 loaded\n\nPrecomputing VAL probabilities (this is the only slow part)...\n Cached: (307, 2) (307, 2)\n\n[Ensemble weight tuning - cached]\nweight_mfcc=0.0 -> val_macroF1=1.0000\nweight_mfcc=0.2 -> val_macroF1=1.0000\nweight_mfcc=0.4 -> val_macroF1=1.0000\nweight_mfcc=0.6 -> val_macroF1=0.9866\nweight_mfcc=0.8 -> val_macroF1=0.9833\nweight_mfcc=1.0 -> val_macroF1=0.9833\n\n Best weight_mfcc=0.0 | Best VAL macroF1=1.0000\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport librosa\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    Wav2Vec2ForSequenceClassification,\n    Wav2Vec2Processor,\n    Trainer,\n    TrainingArguments,\n    set_seed\n)\n\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nset_seed(SEED)\n\nprint(\"CUDA:\", torch.cuda.is_available())\n\nSPLIT_CSV = \"/kaggle/working/data/Malayalam_16k/malayalam_metadata_with_split.csv\"\ndf = pd.read_csv(SPLIT_CSV)\n\nneed_cols = {\"split\", \"file_path\", \"label\"}\nassert need_cols.issubset(df.columns), f\"Missing cols: {need_cols - set(df.columns)}\"\n\ntrain_df = df[df[\"split\"].isin([\"train\", \"val\"])].reset_index(drop=True)\nprint(\"Final train size (train+val):\", len(train_df))\nprint(\"Final train label ratio:\", float(train_df[\"label\"].mean()))\n\n\ndef augment_audio(y, sr=16000):\n    y = np.asarray(y, dtype=np.float32)\n\n    # reduce augmentation frequency a bit for speed\n    if random.random() < 0.35:\n        rate = np.random.uniform(0.97, 1.03)\n        y = librosa.effects.time_stretch(y=y, rate=rate)\n\n    if random.random() < 0.35:\n        n_steps = np.random.uniform(-0.5, 0.5)\n        y = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=n_steps)\n\n    if random.random() < 0.35 and y.size > 0:\n        noise_amp = 0.001 * np.random.uniform() * max(1e-6, float(np.max(np.abs(y))))\n        y = y + noise_amp * np.random.normal(size=y.shape[0]).astype(np.float32)\n\n    return np.clip(y, -1.0, 1.0).astype(np.float32)\n\n\nclass MalayalamWav2Vec2Dataset(Dataset):\n    def __init__(self, df, max_seconds=6.0, augment=False, sr=16000):\n        self.df = df.reset_index(drop=True)\n        self.max_len = int(max_seconds * sr)\n        self.augment = augment\n        self.sr = sr\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = row[\"file_path\"]\n        label = int(row[\"label\"])\n\n        y, _ = librosa.load(path, sr=self.sr, mono=True)\n\n        if len(y) > self.max_len:\n            y = y[:self.max_len]\n\n        if self.augment:\n            y = augment_audio(y, self.sr)\n\n        return {\"input_values\": y, \"labels\": label}\n\ntrain_dataset = MalayalamWav2Vec2Dataset(train_df, augment=True)\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    \"facebook/wav2vec2-base\",\n    num_labels=2,\n    problem_type=\"single_label_classification\"\n)\n\n# freeze encoder for speed + small data stability\nfor p in model.wav2vec2.parameters():\n    p.requires_grad = False\n\ndef data_collator(batch):\n    audio = [b[\"input_values\"] for b in batch]\n    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n    inputs = processor(audio, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n    inputs[\"labels\"] = labels\n    return inputs\n\ncw = compute_class_weight(\"balanced\", classes=np.array([0, 1]), y=train_df[\"label\"].values)\nclass_weights = torch.tensor(cw, dtype=torch.float32)  # move inside loss\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.get(\"labels\")\n        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n        logits = outputs.logits\n\n        weight = class_weights.to(logits.device)\n        loss_fct = nn.CrossEntropyLoss(weight=weight)\n        loss = loss_fct(logits, labels.to(logits.device))\n\n        return (loss, outputs) if return_outputs else loss\n\ntraining_args = TrainingArguments(\n    output_dir=\"./w2v2_final_tmp\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    num_train_epochs=10,\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    warmup_steps=0,\n    logging_steps=25,\n    save_strategy=\"no\",\n    eval_strategy=\"no\",   #  correct key\n    fp16=torch.cuda.is_available(),\n    seed=SEED,\n    data_seed=SEED,\n    max_grad_norm=1.0,\n    remove_unused_columns=False,\n    report_to=\"none\",\n    dataloader_num_workers=2,\n    dataloader_pin_memory=True\n)\n\ntrainer = WeightedTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator\n)\n\ntrainer.train()\n\nFINAL_DIR = \"/kaggle/working/w2v2_malayalam_final\"\nos.makedirs(FINAL_DIR, exist_ok=True)\n\ntrainer.save_model(FINAL_DIR)\nprocessor.save_pretrained(FINAL_DIR)\n\nprint(\"Saved FINAL W2V2 Malayalam model to:\", FINAL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:52:52.894147Z","iopub.execute_input":"2026-03-01T09:52:52.894705Z","iopub.status.idle":"2026-03-01T10:13:39.929446Z","shell.execute_reply.started":"2026-03-01T09:52:52.894677Z","shell.execute_reply":"2026-03-01T10:13:39.928653Z"}},"outputs":[{"name":"stdout","text":"CUDA: True\nFinal train size (train+val): 1688\nFinal train label ratio: 0.466824644549763\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/211 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1b3df9a7af54ac4a4b0f24daf301ba1"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\nKey                          | Status     | \n-----------------------------+------------+-\nproject_q.bias               | UNEXPECTED | \nproject_hid.weight           | UNEXPECTED | \nquantizer.codevectors        | UNEXPECTED | \nproject_q.weight             | UNEXPECTED | \nproject_hid.bias             | UNEXPECTED | \nquantizer.weight_proj.weight | UNEXPECTED | \nquantizer.weight_proj.bias   | UNEXPECTED | \nprojector.bias               | MISSING    | \nprojector.weight             | MISSING    | \nclassifier.weight            | MISSING    | \nclassifier.bias              | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1060' max='1060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1060/1060 20:42, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.246976</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.899047</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.521540</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.357456</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.215283</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.170077</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.115854</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.094548</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.084726</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.064348</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.068797</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.056948</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.038266</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.067630</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.041367</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.033421</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.048106</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.036685</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.036701</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.031509</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.035290</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.026650</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.059123</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.031993</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.033865</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.023917</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.020669</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.034675</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.022398</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.028483</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.024775</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.034772</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.028415</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.015591</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.022354</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.019649</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>0.030228</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.025371</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>0.025765</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.026195</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>0.023460</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.018654</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180eaadb9e624de9b9c0583e899c2026"}},"metadata":{}},{"name":"stdout","text":"Saved FINAL W2V2 Malayalam model to: /kaggle/working/w2v2_malayalam_final\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\n\n\nweight_mfcc = best_w  # e.g., 0.6\n\ndef ensemble_predict_id(file_path, weight_mfcc=weight_mfcc):\n    p_m = mfcc_proba(file_path)\n    p_w = w2v2_proba(file_path)\n    p = weight_mfcc * p_m + (1.0 - weight_mfcc) * p_w\n    return int(np.argmax(p))\n\ny_true = test_df[\"label\"].astype(int).tolist()\ny_pred = [ensemble_predict_id(p) for p in test_df[\"file_path\"].tolist()]\n\nprint(\"\\nEnsemble Classification Report (Test):\\n\")\nprint(classification_report(\n    y_true, y_pred,\n    target_names=[\"Non-depressed\", \"Depressed\"],\n    digits=4\n))\n\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(4,4))\nplt.imshow(cm, cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\n\nclasses = [\"Non-depressed\", \"Depressed\"]\nplt.xticks([0,1], classes)\nplt.yticks([0,1], classes)\n\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, cm[i, j],\n                 ha=\"center\", va=\"center\",\n                 color=\"white\" if cm[i,j] > cm.max()/2 else \"black\",\n                 fontsize=12, fontweight=\"bold\")\n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T10:13:57.896553Z","iopub.execute_input":"2026-03-01T10:13:57.896900Z","iopub.status.idle":"2026-03-01T10:14:07.619375Z","shell.execute_reply.started":"2026-03-01T10:13:57.896870Z","shell.execute_reply":"2026-03-01T10:14:07.618799Z"}},"outputs":[{"name":"stdout","text":"\nEnsemble Classification Report (Test):\n\n               precision    recall  f1-score   support\n\nNon-depressed     1.0000    0.9902    0.9951       102\n    Depressed     0.9899    1.0000    0.9949        98\n\n     accuracy                         0.9950       200\n    macro avg     0.9949    0.9951    0.9950       200\n weighted avg     0.9951    0.9950    0.9950       200\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 400x400 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# Final Submission","metadata":{}},{"cell_type":"code","source":"import os, glob, zipfile\nimport pandas as pd\nimport torch\nimport librosa\nfrom transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n\nTEAM_NAME = \"TriVector\"\nRUN = \"run1\"\n\n#  Dataset base \nBASE = \"/kaggle/input/datasets/tahmimahoque/depression-detection-speech-dataset/Depression_det\"\n\n#  Malayalam test folder \nTEST_DIR = os.path.join(BASE, \"Test_set_mal\", \"Test_set_mal\")\n\n\nFINAL_DIR = \"/kaggle/working/w2v2_best\"   \n\n# Label mapping \nID2LABEL = {0: 0, 1: 1}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nassert os.path.isdir(TEST_DIR), f\"Missing test dir: {TEST_DIR}\"\nassert os.path.isdir(FINAL_DIR), f\"Missing model dir: {FINAL_DIR}\"\n\n# Collect test wavs\ntest_files = sorted(glob.glob(os.path.join(TEST_DIR, \"**\", \"*.wav\"), recursive=True))\nprint(\" Test wavs:\", len(test_files))\nassert len(test_files) > 0, \"No .wav found in TEST_DIR\"\n\n# Load model + processor\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(FINAL_DIR).to(device)\nprocessor = Wav2Vec2Processor.from_pretrained(FINAL_DIR)\nmodel.eval()\n\n@torch.no_grad()\ndef predict_label(file_path, sr=16000, max_seconds=6.0):\n    y, _ = librosa.load(file_path, sr=sr, mono=True)\n    max_len = int(sr * max_seconds)\n    if len(y) > max_len:\n        y = y[:max_len]\n\n    inputs = processor([y], sampling_rate=sr, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    logits = model(**inputs).logits\n    pred_id = int(torch.argmax(logits, dim=-1).item())\n    return ID2LABEL[pred_id]\n\n# Predict\nrows = []\nfor fp in test_files:\n    file_id = os.path.splitext(os.path.basename(fp))[0]  # no .wav\n    rows.append({\"file_name\": file_id, \"label\": predict_label(fp)})\n\nmal_df = pd.DataFrame(rows)\n\n# Save CSV in Kaggle working\ncsv_name = f\"{TEAM_NAME}_Malayalam_{RUN}.csv\"\ncsv_path = os.path.join(\"/kaggle/working\", csv_name)\nmal_df.to_csv(csv_path, index=False)\nprint(\" CSV saved:\", csv_path)\nprint(mal_df.head())\n\n\nzip_name = f\"{TEAM_NAME}_Malayalam_{RUN}.zip\"\nzip_path = os.path.join(\"/kaggle/working\", zip_name)\n\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n    z.write(csv_path, arcname=csv_name)\n\nprint(\" ZIP saved:\", zip_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T10:35:18.663516Z","iopub.execute_input":"2026-03-01T10:35:18.664207Z","iopub.status.idle":"2026-03-01T10:35:25.326849Z","shell.execute_reply.started":"2026-03-01T10:35:18.664174Z","shell.execute_reply":"2026-03-01T10:35:25.326124Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n Test wavs: 200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/215 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55996353759b413a90b3493b2fafe983"}},"metadata":{}},{"name":"stdout","text":" CSV saved: /kaggle/working/TriVector_Malayalam_run1.csv\n  file_name  label\n0        m1      1\n1       m10      0\n2      m100      1\n3      m101      0\n4      m102      1\n ZIP saved: /kaggle/working/TriVector_Malayalam_run1.zip\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = \"/kaggle/working/TriVector_Malayalam_run1.csv\"  # existing CSV\n\ndf = pd.read_csv(csv_path)\n\n# 0/1 → text labels\ndf[\"label\"] = df[\"label\"].map({0: \"Non-depressed\", 1: \"Depressed\"})\n\n# overwrite same file\ndf.to_csv(csv_path, index=False)\n\nprint(\" Converted to text labels:\", csv_path)\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T10:35:27.825303Z","iopub.execute_input":"2026-03-01T10:35:27.825901Z","iopub.status.idle":"2026-03-01T10:35:27.839376Z","shell.execute_reply.started":"2026-03-01T10:35:27.825866Z","shell.execute_reply":"2026-03-01T10:35:27.838516Z"}},"outputs":[{"name":"stdout","text":" Converted to text labels: /kaggle/working/TriVector_Malayalam_run1.csv\n  file_name          label\n0        m1      Depressed\n1       m10  Non-depressed\n2      m100      Depressed\n3      m101  Non-depressed\n4      m102      Depressed\n","output_type":"stream"}],"execution_count":24}]}