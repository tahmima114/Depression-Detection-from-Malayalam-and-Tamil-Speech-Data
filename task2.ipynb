{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14990500,"datasetId":9595634,"databundleVersionId":15864649},{"sourceType":"datasetVersion","sourceId":14996549,"datasetId":9599530,"databundleVersionId":15871270}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport torch\n\nSEED = 42\n\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ndef seed_worker(worker_id):\n    worker_seed = SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:06:48.546210Z","iopub.execute_input":"2026-03-01T08:06:48.546510Z","iopub.status.idle":"2026-03-01T08:06:52.563097Z","shell.execute_reply.started":"2026-03-01T08:06:48.546484Z","shell.execute_reply":"2026-03-01T08:06:52.562108Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Resampling\nRead Tamil from Kaggle input and save 16k audio to working directory","metadata":{}},{"cell_type":"code","source":"\nimport os, glob\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom tqdm import tqdm\nimport random\n\nSRC_ROOT = \"/kaggle/input/datasets/tahmimahoque/depression-det/Depression_det/Depression_det/Tamil/Tamil\"\nDST_ROOT = \"/kaggle/working/Tamil_16k\"\nTARGET_SR = 16000\n\npairs = [\n    (os.path.join(SRC_ROOT, \"Depressed\", \"Train_set\"),\n     os.path.join(DST_ROOT, \"Depressed\", \"Train_set\")),\n    (os.path.join(SRC_ROOT, \"Non-depressed\", \"Train_set\"),\n     os.path.join(DST_ROOT, \"Non-depressed\", \"Train_set\")),\n]\n\nos.makedirs(DST_ROOT, exist_ok=True)\n\ntotal = converted = same_sr = failed = 0\n\nfor src_dir, dst_dir in pairs:\n\n    if not os.path.isdir(src_dir):\n        print(\"Missing:\", src_dir)\n        continue\n\n    os.makedirs(dst_dir, exist_ok=True)\n\n    files = glob.glob(os.path.join(src_dir, \"**\", \"*.wav\"), recursive=True) + \\\n            glob.glob(os.path.join(src_dir, \"**\", \"*.WAV\"), recursive=True)\n\n    print(f\"Found {len(files)} files in {src_dir}\")\n    total += len(files)\n\n    for fp in tqdm(files, desc=f\"Resampling {os.path.basename(src_dir)}\"):\n        try:\n            y, sr = librosa.load(fp, sr=None, mono=True)\n\n            # Trim silence\n            y, _ = librosa.effects.trim(y, top_db=30)\n\n            if y.size == 0:\n                failed += 1\n                continue\n\n            #  Resample if needed\n            if sr != TARGET_SR:\n                y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n                converted += 1\n            else:\n                same_sr += 1\n\n            # RMS normalize\n            rms = float(np.sqrt(np.mean(y**2) + 1e-8))\n            y = y / max(rms, 1e-3)\n            y = np.clip(y, -1.0, 1.0)\n\n            out_path = os.path.join(dst_dir, os.path.basename(fp))\n            sf.write(out_path, y, TARGET_SR, subtype=\"PCM_16\")\n\n        except Exception as e:\n            failed += 1\n\nprint(\"\\n==== DONE ====\")\nprint(\"Total files:\", total)\nprint(\"Already 16k:\", same_sr)\nprint(\"Resampled:\", converted)\nprint(\"Failed:\", failed)\n\n#  Quick verify SR\ncheck = glob.glob(os.path.join(DST_ROOT, \"**\", \"*.wav\"), recursive=True)\nprint(\"Saved files:\", len(check))\n\nsample = random.sample(check, min(20, len(check))) if check else []\n\nsrs = []\nfor fp in sample:\n    _, sr = librosa.load(fp, sr=None, mono=True)\n    srs.append(sr)\n\nif srs:\n    vals, cnts = np.unique(srs, return_counts=True)\n    print(\"SR counts:\", dict(zip(vals.tolist(), cnts.tolist())))\n\nprint(\" Output folder:\", DST_ROOT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:07:08.734052Z","iopub.execute_input":"2026-03-01T08:07:08.734449Z","iopub.status.idle":"2026-03-01T08:07:58.597115Z","shell.execute_reply.started":"2026-03-01T08:07:08.734412Z","shell.execute_reply":"2026-03-01T08:07:58.596427Z"}},"outputs":[{"name":"stdout","text":"Found 454 files in /kaggle/input/datasets/tahmimahoque/depression-det/Depression_det/Depression_det/Tamil/Tamil/Depressed/Train_set\n","output_type":"stream"},{"name":"stderr","text":"Resampling Train_set: 100%|██████████| 454/454 [00:22<00:00, 19.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Found 920 files in /kaggle/input/datasets/tahmimahoque/depression-det/Depression_det/Depression_det/Tamil/Tamil/Non-depressed/Train_set\n","output_type":"stream"},{"name":"stderr","text":"Resampling Train_set: 100%|██████████| 920/920 [00:22<00:00, 41.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n==== DONE ====\nTotal files: 1374\nAlready 16k: 454\nResampled: 920\nFailed: 0\nSaved files: 1374\nSR counts: {16000: 20}\n Output folder: /kaggle/working/Tamil_16k\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Build Metadata\n\nCreate CSV from resampled Tamil_16k with labels and speaker IDs","metadata":{}},{"cell_type":"code","source":"\nimport os, glob, re\nimport pandas as pd\n\nDATA_ROOT_16K = \"/kaggle/working/Tamil_16k\"\ndep_train = os.path.join(DATA_ROOT_16K, \"Depressed\", \"Train_set\")\nnd_train  = os.path.join(DATA_ROOT_16K, \"Non-depressed\", \"Train_set\")\n\ndef extract_speaker_from_path(fp: str) -> str:\n    base = os.path.splitext(os.path.basename(fp))[0]\n\n    # Non-depressed: \n    m = re.match(r\"^(ND\\d+)\", base, flags=re.IGNORECASE)\n    if m:\n        return m.group(1).upper()\n\n    # Depressed: \n    if base.startswith(\"D_\"):\n        rest = base[2:]                 # remove D_\n        tok  = rest.split(\"_\")[0]       # first token after D_\n        tok  = re.sub(r\"[-_][0-9a-zA-Z]+$\", \"\", tok)  # remove trailing \n        return tok.upper()\n\n    # fallback\n    tok = base.split(\"_\")[0]\n    tok = re.sub(r\"[-_][0-9a-zA-Z]+$\", \"\", tok)\n    return tok.upper()\n\nrows = []\n\ndef collect(folder, label):\n    if not os.path.isdir(folder):\n        print(f\" Missing folder: {folder}\")\n        return\n\n    wavs = glob.glob(os.path.join(folder, \"**\", \"*.wav\"), recursive=True) + \\\n           glob.glob(os.path.join(folder, \"**\", \"*.WAV\"), recursive=True)\n\n    print(f\"{folder} -> {len(wavs)} files\")\n\n    for f in wavs:\n        fname = os.path.splitext(os.path.basename(f))[0]\n        spk   = extract_speaker_from_path(f)\n        rows.append({\n            \"file_path\": f,\n            \"label\": int(label),\n            \"fname\": fname,\n            \"speaker\": spk\n        })\n\ncollect(dep_train, 1)\ncollect(nd_train, 0)\n\ndf = pd.DataFrame(rows)\n\nprint(\"\\n Total samples:\", len(df))\nif len(df):\n    print(\" Label counts:\\n\", df[\"label\"].value_counts())\n    print(\" Unique speakers:\", df[\"speaker\"].nunique())\n    print(\" Speakers per label:\\n\", df.groupby(\"label\")[\"speaker\"].nunique())\n\nOUT_CSV = os.path.join(DATA_ROOT_16K, \"tamil_trainval_16k.csv\")\ndf.to_csv(OUT_CSV, index=False)\nprint(\"\\n Saved:\", OUT_CSV)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:08:13.010814Z","iopub.execute_input":"2026-03-01T08:08:13.011703Z","iopub.status.idle":"2026-03-01T08:08:13.077908Z","shell.execute_reply.started":"2026-03-01T08:08:13.011674Z","shell.execute_reply":"2026-03-01T08:08:13.077353Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Tamil_16k/Depressed/Train_set -> 454 files\n/kaggle/working/Tamil_16k/Non-depressed/Train_set -> 920 files\n\n Total samples: 1374\n Label counts:\n label\n0    920\n1    454\nName: count, dtype: int64\n Unique speakers: 44\n Speakers per label:\n label\n0     5\n1    39\nName: speaker, dtype: int64\n\n Saved: /kaggle/working/Tamil_16k/tamil_trainval_16k.csv\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                           file_path  label       fname  \\\n0  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_35-2   \n1  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_38-4   \n2  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_27-2   \n3  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_07-2   \n4  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_52-4   \n\n  speaker  \n0     S00  \n1     S00  \n2     S00  \n3     S00  \n4     S00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_path</th>\n      <th>label</th>\n      <th>fname</th>\n      <th>speaker</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_35-2</td>\n      <td>S00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_38-4</td>\n      <td>S00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_27-2</td>\n      <td>S00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_07-2</td>\n      <td>S00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_52-4</td>\n      <td>S00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Speaker Sanity Check","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Kaggle paths\nCSV_PATH = \"/kaggle/working/Tamil_16k/tamil_trainval_16k.csv\"\ndf = pd.read_csv(CSV_PATH)\n\nprint(\" Loaded:\", CSV_PATH)\nprint(\"Total rows:\", len(df))\nprint(\"Unique speakers:\", df[\"speaker\"].nunique())\nprint(\"Speakers per label:\\n\", df.groupby(\"label\")[\"speaker\"].nunique())\n\n# 1) Mixed-label speakers check (speaker leakage risk)\nmix = df.groupby(\"speaker\")[\"label\"].nunique()\nmixed = mix[mix > 1]\nprint(\"\\nMixed-label speakers (should be 0):\", len(mixed))\nif len(mixed):\n    print(mixed.head(20))\n\n# 2) Show top speakers by #files (helps spot parsing bug)\nprint(\"\\nTop 15 speakers by file-count:\")\nprint(df[\"speaker\"].value_counts().head(15))\n\n# 3) Pattern check: \ndef is_ok_spk(s):\n    s = str(s)\n    if re.fullmatch(r\"ND\\d+\", s): return True\n    if re.fullmatch(r\"S\\d+\", s): return True          # S00, S01...\n    if re.fullmatch(r\"F\\d+\", s): return True          # F2006...\n    if re.fullmatch(r\"[A-Z]\\d+\", s): return True      # A1 type (just in case)\n    return False\n\nbad = df[~df[\"speaker\"].apply(is_ok_spk)]\nprint(\"\\nSuspicious speaker IDs:\", bad[\"speaker\"].nunique())\nprint(bad[[\"fname\",\"speaker\",\"label\"]].head(30))\n\n# 4) Depressed files \nbad_dep = df[(df[\"label\"]==1) & (df[\"speaker\"].astype(str).str.contains(r\"[-_]\", regex=True))]\nprint(\"\\nDepressed speakers containing '-' or '_' (should be 0 ideally):\", bad_dep[\"speaker\"].nunique())\nprint(bad_dep[[\"fname\",\"speaker\"]].head(30))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:09:04.158184Z","iopub.execute_input":"2026-03-01T08:09:04.158533Z","iopub.status.idle":"2026-03-01T08:09:04.183647Z","shell.execute_reply.started":"2026-03-01T08:09:04.158504Z","shell.execute_reply":"2026-03-01T08:09:04.183000Z"}},"outputs":[{"name":"stdout","text":" Loaded: /kaggle/working/Tamil_16k/tamil_trainval_16k.csv\nTotal rows: 1374\nUnique speakers: 44\nSpeakers per label:\n label\n0     5\n1    39\nName: speaker, dtype: int64\n\nMixed-label speakers (should be 0): 0\n\nTop 15 speakers by file-count:\nspeaker\nS00       241\nND1       184\nND5       184\nND4       184\nND2       184\nND3       184\nA00        80\nF10018      4\nF20011      4\nF10017      4\nF2001       4\nF10015      4\nF10010      4\nF1008       4\nF20012      4\nName: count, dtype: int64\n\nSuspicious speaker IDs: 0\nEmpty DataFrame\nColumns: [fname, speaker, label]\nIndex: []\n\nDepressed speakers containing '-' or '_' (should be 0 ideally): 0\nEmpty DataFrame\nColumns: [fname, speaker]\nIndex: []\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# # Speaker Split\n\nSpeaker-stratified train/val split","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport os\n\nIN_CSV  = \"/kaggle/working/Tamil_16k/tamil_trainval_16k.csv\"\nOUT_CSV = \"/kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\"\n\nassert os.path.isfile(IN_CSV), f\" Missing IN_CSV: {IN_CSV}. Run BLOCK 1 first.\"\n\ndf = pd.read_csv(IN_CSV)\n\nmix = df.groupby(\"speaker\")[\"label\"].nunique()\nmixed = mix[mix > 1]\nassert len(mixed) == 0, f\" Mixed-label speakers found: {len(mixed)}. Fix speaker parsing first.\"\n\n# 1) Speaker -> label \nspk_label = df.groupby(\"speaker\")[\"label\"].agg(lambda x: int(x.value_counts().idxmax())).reset_index()\ndep_spk = spk_label[spk_label[\"label\"] == 1][\"speaker\"].tolist()\nnd_spk  = spk_label[spk_label[\"label\"] == 0][\"speaker\"].tolist()\n\nprint(\" Dep speakers:\", len(dep_spk), \"| ND speakers:\", len(nd_spk))\n\n# 2) Choose VAL speakers\nrng = np.random.default_rng(42)\n\nn_nd_val = 1 if len(nd_spk) >= 2 else len(nd_spk)\nval_frac_dep = 0.20\nn_dep_val = int(round(len(dep_spk) * val_frac_dep))\nn_dep_val = max(2, n_dep_val) if len(dep_spk) >= 2 else len(dep_spk)\n\nval_nd  = set(rng.choice(nd_spk,  size=n_nd_val, replace=False).tolist()) if n_nd_val > 0 else set()\nval_dep = set(rng.choice(dep_spk, size=n_dep_val, replace=False).tolist()) if n_dep_val > 0 else set()\n\nval_spk = val_nd | val_dep\n\n# 3) Assign split\ndf[\"split\"] = \"train\"\ndf.loc[df[\"speaker\"].isin(val_spk), \"split\"] = \"val\"\n\n# 4) Checks\nprint(\"\\n Split counts:\\n\", df[\"split\"].value_counts())\nprint(\"\\n Label counts by split:\\n\", df.groupby([\"split\",\"label\"]).size())\nprint(\"\\n Speakers by split:\\n\", df.groupby(\"split\")[\"speaker\"].nunique())\n\ntrain_spk = set(df[df[\"split\"]==\"train\"][\"speaker\"])\nval_spk2  = set(df[df[\"split\"]==\"val\"][\"speaker\"])\noverlap = train_spk & val_spk2\nprint(\"\\n Speaker overlap (MUST be empty):\", overlap)\n\nassert len(overlap) == 0, \" Speaker leakage: overlap is NOT empty!\"\n\n# 5) Save\nos.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\ndf.to_csv(OUT_CSV, index=False)\nprint(\"\\n Saved:\", OUT_CSV)\n\nprint(\"\\n VAL speakers (for record):\", sorted(list(val_spk2)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:09:41.286089Z","iopub.execute_input":"2026-03-01T08:09:41.286615Z","iopub.status.idle":"2026-03-01T08:09:41.333669Z","shell.execute_reply.started":"2026-03-01T08:09:41.286587Z","shell.execute_reply":"2026-03-01T08:09:41.333021Z"}},"outputs":[{"name":"stdout","text":" Dep speakers: 39 | ND speakers: 5\n\n Split counts:\n split\ntrain    1158\nval       216\nName: count, dtype: int64\n\n Label counts by split:\n split  label\ntrain  0        736\n       1        422\nval    0        184\n       1         32\ndtype: int64\n\n Speakers by split:\n split\ntrain    35\nval       9\nName: speaker, dtype: int64\n\n Speaker overlap (MUST be empty): set()\n\n Saved: /kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\n\n VAL speakers (for record): ['F10011', 'F10015', 'F10021', 'F10022', 'F1007', 'F2001', 'F20011', 'F20015', 'ND1']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# MFCC","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import f1_score\n\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#  Kaggle paths\nCSV_PATH = \"/kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\"\nSAVE_DIR = \"/kaggle/working/models_tamil\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nassert os.path.isfile(CSV_PATH), f\" Missing CSV: {CSV_PATH}. Run BLOCK 1→3 first.\"\n\ndf = pd.read_csv(CSV_PATH)\n\n\ndef extract_mfcc(file_path, target_sr=16000, n_mfcc=40, max_len=320):\n    y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n\n    \n    y, _ = librosa.effects.trim(y, top_db=30)\n    if y is None or len(y) == 0:\n        y = np.zeros(int(target_sr * 0.5), dtype=np.float32)  # 0.5s silence fallback\n\n    mfcc = librosa.feature.mfcc(y=y, sr=target_sr, n_mfcc=n_mfcc)  # [n_mfcc, T]\n\n  \n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n\n    T = mfcc.shape[1]\n    if T < max_len:\n        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_len]\n\n    return mfcc.astype(np.float32)\n\nclass TamilMFCCDataset(Dataset):\n    def __init__(self, df, split):\n        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        x = extract_mfcc(row[\"file_path\"])\n        y = int(row[\"label\"])\n        return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n\ntrain_ds = TamilMFCCDataset(df, \"train\")\nval_ds   = TamilMFCCDataset(df, \"val\")\n\ndef seed_worker(worker_id):\n    s = SEED + worker_id\n    np.random.seed(s); random.seed(s)\n\ng = torch.Generator()\ng.manual_seed(SEED)\n\n# WeightedRandomSampler for TRAIN only (handles skew better than only class weights)\ntrain_labels = df[df[\"split\"]==\"train\"][\"label\"].astype(int).values\nclass_counts = np.bincount(train_labels, minlength=2)          # [count0, count1]\nclass_weights = 1.0 / np.maximum(class_counts, 1)             # inverse freq\nsample_weights = class_weights[train_labels]                  # per sample weight\n\nsampler = WeightedRandomSampler(\n    weights=torch.tensor(sample_weights, dtype=torch.double),\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=32,\n    sampler=sampler,              # use sampler instead of shuffle\n    shuffle=False,\n    num_workers=2,                \n    worker_init_fn=seed_worker,\n    generator=g,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=64,\n    shuffle=False,\n    num_workers=2,                \n    worker_init_fn=seed_worker,\n    generator=g,\n    pin_memory=True\n)\n\nclass MFCC_CNN(nn.Module):\n    def __init__(self, n_classes=2):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1,1)),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.4),\n            nn.Linear(64, n_classes)\n        )\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # [B,1,40,320]\n        x = self.features(x)\n        return self.classifier(x)\n\nmodel = MFCC_CNN().to(device)\n\n# Loss: still keep class weights (fine), even though sampler already helps\ncw = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=train_labels)\ncw = torch.tensor(cw, dtype=torch.float32).to(device)\ncriterion = nn.CrossEntropyLoss(weight=cw)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n# LR scheduler improves stability\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"max\", factor=0.5, patience=2\n)\n\ndef eval_macro_f1(model):\n    model.eval()\n    ys, ps = [], []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            logits = model(x)\n            pred = torch.argmax(logits, dim=1)\n            ys.extend(y.cpu().numpy().tolist())\n            ps.extend(pred.cpu().numpy().tolist())\n    return f1_score(ys, ps, average=\"macro\")\n\n\nbest_f1 = -1.0\nbest_state = None\npatience = 6\nbad = 0\nEPOCHS = 30\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    total_loss = 0.0\n\n    for x, y in train_loader:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    val_f1 = eval_macro_f1(model)\n    scheduler.step(val_f1)\n\n    print(f\"Epoch {epoch:02d} | train_loss={total_loss/len(train_loader):.4f} | val_macroF1={val_f1:.4f}\")\n\n    if val_f1 > best_f1 + 1e-4:\n        best_f1 = val_f1\n        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        bad = 0\n    else:\n        bad += 1\n        if bad >= patience:\n            print(\"Early stopping.\")\n            break\n\n# Safety: best_state fallback\nif best_state is None:\n    print(\" No improvement detected; saving last epoch weights.\")\nelse:\n    model.load_state_dict(best_state)\n\nprint(\" Best MFCC val macro-F1:\", best_f1)\n\nmfcc_path = os.path.join(SAVE_DIR, \"mfcc_cnn_tamil_16k_stratspk.pt\")\ntorch.save(model.state_dict(), mfcc_path)\nprint(\" Saved:\", mfcc_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:10:12.036866Z","iopub.execute_input":"2026-03-01T08:10:12.037591Z","iopub.status.idle":"2026-03-01T08:17:17.856083Z","shell.execute_reply.started":"2026-03-01T08:10:12.037560Z","shell.execute_reply":"2026-03-01T08:17:17.855245Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | train_loss=0.5841 | val_macroF1=0.1805\nEpoch 02 | train_loss=0.3904 | val_macroF1=0.7859\nEpoch 03 | train_loss=0.1994 | val_macroF1=0.8344\nEpoch 04 | train_loss=0.1586 | val_macroF1=0.8345\nEpoch 05 | train_loss=0.1256 | val_macroF1=0.8522\nEpoch 06 | train_loss=0.1173 | val_macroF1=0.8951\nEpoch 07 | train_loss=0.1163 | val_macroF1=0.8522\nEpoch 08 | train_loss=0.0970 | val_macroF1=0.9382\nEpoch 09 | train_loss=0.1003 | val_macroF1=0.8583\nEpoch 10 | train_loss=0.1058 | val_macroF1=0.9042\nEpoch 11 | train_loss=0.0826 | val_macroF1=0.9489\nEpoch 12 | train_loss=0.0882 | val_macroF1=0.8403\nEpoch 13 | train_loss=0.1039 | val_macroF1=0.8838\nEpoch 14 | train_loss=0.0825 | val_macroF1=0.9489\nEpoch 15 | train_loss=0.0652 | val_macroF1=0.8973\nEpoch 16 | train_loss=0.0641 | val_macroF1=0.9410\nEpoch 17 | train_loss=0.0824 | val_macroF1=0.9113\nEarly stopping.\n Best MFCC val macro-F1: 0.9488555643251776\n Saved: /kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink, display\n\n\nmodel_path = \"/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\"\n\nassert os.path.isfile(model_path), f\" Model not found: {model_path}\"\n\nprint(\"Model ready:\", model_path)\ndisplay(FileLink(model_path)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:17:56.041628Z","iopub.execute_input":"2026-03-01T08:17:56.042615Z","iopub.status.idle":"2026-03-01T08:17:56.049152Z","shell.execute_reply.started":"2026-03-01T08:17:56.042582Z","shell.execute_reply":"2026-03-01T08:17:56.048593Z"}},"outputs":[{"name":"stdout","text":"Model ready: /kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt","text/html":"<a href='/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt' target='_blank'>/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt</a><br>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import classification_report\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nGT_CSV    = \"/kaggle/input/datasets/tahmimahoque/tamil-test/Tamil_GT.xlsx - tam.csv\"\nMODEL_PATH = \"/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\"\nSEARCH_ROOT = \"/kaggle/input\"   # search wavs anywhere in kaggle input\n\nassert os.path.isfile(GT_CSV), f\" Missing GT csv: {GT_CSV}\"\nassert os.path.isfile(MODEL_PATH), f\" Missing MFCC model: {MODEL_PATH}\"\n\ngt = pd.read_csv(GT_CSV)\nassert {\"filename\",\"label\"}.issubset(gt.columns), f\" Need columns filename,label; got {gt.columns.tolist()}\"\n\n# labels: D->1, ND->0\ngt[\"label_num\"] = gt[\"label\"].map({\"D\": 1, \"ND\": 0})\nif gt[\"label_num\"].isna().any():\n    bad = gt[gt[\"label_num\"].isna()][[\"filename\",\"label\"]].head(10)\n    raise ValueError(f\" Unknown labels found. Examples:\\n{bad}\")\n\ntargets = set(gt[\"filename\"].astype(str).str.lower().tolist())\n\nfound = {}\nfor root, dirs, files in os.walk(SEARCH_ROOT):\n    for fn in files:\n        low = fn.lower()\n        if low in targets:  # exact filename match\n            found[low] = os.path.join(root, fn)\n\nprint(f\" Found audio: {len(found)}/{len(gt)}\")\nif len(found) < len(gt):\n    missing = [f for f in gt[\"filename\"].tolist() if str(f).lower() not in found]\n    print(\" Missing examples (first 20):\", missing[:20])\n\n# If no audio found, stop early (no point running model)\nassert len(found) > 0, (\n    \" No matching wav found in /kaggle/input. \"\n    \"We need to add the dataset that contains t1.wav, t2.wav, ... to this notebook.\"\n)\n\n\ndef extract_mfcc(file_path, target_sr=16000, n_mfcc=40, max_len=320):\n    y, _ = librosa.load(file_path, sr=target_sr, mono=True)\n    y, _ = librosa.effects.trim(y, top_db=30)\n\n    if y is None or len(y) == 0:\n        y = np.zeros(int(target_sr * 0.5), dtype=np.float32)\n\n    mfcc = librosa.feature.mfcc(y=y, sr=target_sr, n_mfcc=n_mfcc)\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n\n    T = mfcc.shape[1]\n    if T < max_len:\n        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_len]\n\n    return mfcc.astype(np.float32)\n\n\nclass MFCC_CNN(nn.Module):\n    def __init__(self, n_classes=2):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1,1)),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.4),\n            nn.Linear(64, n_classes)\n        )\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.features(x)\n        return self.classifier(x)\n\nmodel = MFCC_CNN().to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device))\nmodel.eval()\n\ny_true, y_pred = [], []\nmissing_audio = 0\n\nwith torch.no_grad():\n    for _, row in gt.iterrows():\n        key = str(row[\"filename\"]).lower()\n        if key not in found:\n            missing_audio += 1\n            continue\n\n        fp = found[key]\n        x = torch.from_numpy(extract_mfcc(fp)).unsqueeze(0).to(device)  # [1,40,320]\n        logits = model(x)\n        pred = int(torch.argmax(logits, dim=1).item())\n\n        y_true.append(int(row[\"label_num\"]))\n        y_pred.append(pred)\n\nprint(\"Missing audio skipped:\", missing_audio)\nprint(\"\\n MFCC CLASSIFICATION REPORT (TEST)\")\nprint(classification_report(y_true, y_pred, digits=4, target_names=[\"ND(0)\", \"D(1)\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:18:01.416106Z","iopub.execute_input":"2026-03-01T08:18:01.416857Z","iopub.status.idle":"2026-03-01T08:18:17.754318Z","shell.execute_reply.started":"2026-03-01T08:18:01.416828Z","shell.execute_reply":"2026-03-01T08:18:17.753575Z"}},"outputs":[{"name":"stdout","text":" Found audio: 160/160\nMissing audio skipped: 0\n\n MFCC CLASSIFICATION REPORT (TEST)\n              precision    recall  f1-score   support\n\n       ND(0)     0.7018    1.0000    0.8247        80\n        D(1)     1.0000    0.5750    0.7302        80\n\n    accuracy                         0.7875       160\n   macro avg     0.8509    0.7875    0.7775       160\nweighted avg     0.8509    0.7875    0.7775       160\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Wav2Vec2","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nimport torch, librosa\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    Wav2Vec2FeatureExtractor,\n    Wav2Vec2ForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback\n)\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\ntorch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Kaggle paths\nCSV_PATH = \"/kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\"\nassert os.path.isfile(CSV_PATH), f\" Missing CSV: {CSV_PATH}. Run BLOCK 1→3 first.\"\n\ndf = pd.read_csv(CSV_PATH)\n\nW2V2_DIR = \"/kaggle/working/wav2vec2_tamil_16k_stratspk\"\nos.makedirs(W2V2_DIR, exist_ok=True)\n\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n    \"facebook/wav2vec2-xls-r-300m\",\n    sampling_rate=16000\n)\n\n\nclass TamilW2V2Dataset(Dataset):\n    def __init__(self, df, split):\n        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        audio, _ = librosa.load(row[\"file_path\"], sr=16000, mono=True)\n        audio, _ = librosa.effects.trim(audio, top_db=30)\n        if audio is None or len(audio) == 0:\n            audio = np.zeros(int(16000 * 0.5), dtype=np.float32)  # 0.5s fallback\n        return {\"input_values\": audio, \"labels\": int(row[\"label\"])}\n\ntrain_ds = TamilW2V2Dataset(df, \"train\")\nval_ds   = TamilW2V2Dataset(df, \"val\")\n\ndef collate_fn(batch):\n    inputs = [b[\"input_values\"] for b in batch]\n    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n    feats = feature_extractor(inputs, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n    feats[\"labels\"] = labels\n    return feats\n\n\ntrain_labels = df[df[\"split\"]==\"train\"][\"label\"].astype(int).values\ncw = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=train_labels)\ncw = torch.tensor(cw, dtype=torch.float32)  # keep CPU; move inside loss\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        w = cw.to(logits.device)\n        loss = torch.nn.functional.cross_entropy(logits, labels, weight=w)\n        return (loss, outputs) if return_outputs else loss\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    if isinstance(logits, (tuple, list)):\n        logits = logits[0]\n    preds = np.argmax(logits, axis=1)\n    return {\"f1\": f1_score(labels, preds, average=\"macro\")}\n\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    \"facebook/wav2vec2-xls-r-300m\",\n    num_labels=2,\n    problem_type=\"single_label_classification\"\n).to(device)\n\n\nfor p in model.wav2vec2.parameters():\n    p.requires_grad = False\nfor p in model.classifier.parameters():\n    p.requires_grad = True\n\nargs_stage1 = TrainingArguments(\n    output_dir=os.path.join(W2V2_DIR, \"stage1\"),\n\n    # HF version-safe: some versions use evaluation_strategy\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n\n    learning_rate=5e-5,\n    num_train_epochs=8,\n    warmup_steps=50,\n    weight_decay=0.01,\n\n    fp16=True,\n    max_grad_norm=1.0,\n    logging_steps=50,\n    save_total_limit=2,\n    report_to=\"none\",\n    seed=SEED,\n    remove_unused_columns=False\n)\n\ntrainer1 = WeightedTrainer(\n    model=model,\n    args=args_stage1,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\ntrainer1.train()\nprint(\" Stage1 best F1:\", trainer1.state.best_metric)\n\nfor p in model.wav2vec2.parameters():\n    p.requires_grad = False\n\nfor layer in model.wav2vec2.encoder.layers[-2:]:\n    for p in layer.parameters():\n        p.requires_grad = True\n\nfor p in model.classifier.parameters():\n    p.requires_grad = True\n\nargs_stage2 = TrainingArguments(\n    output_dir=os.path.join(W2V2_DIR, \"stage2\"),\n\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n\n    learning_rate=1e-5,\n    num_train_epochs=8,\n    warmup_steps=30,\n    weight_decay=0.01,\n\n    fp16=False,                 #  stability fix\n    max_grad_norm=1.0,\n    logging_steps=50,\n    save_total_limit=2,\n    report_to=\"none\",\n    seed=SEED,\n    remove_unused_columns=False\n)\n\ntrainer2 = WeightedTrainer(\n    model=model,\n    args=args_stage2,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\ntrainer2.train()\nprint(\" Stage2 best F1:\", trainer2.state.best_metric)\n\nFINAL_DIR = os.path.join(W2V2_DIR, \"final\")\nos.makedirs(FINAL_DIR, exist_ok=True)\ntrainer2.model.save_pretrained(FINAL_DIR)\nfeature_extractor.save_pretrained(FINAL_DIR)\nprint(\" Saved FINAL W2V2:\", FINAL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:18:59.618568Z","iopub.execute_input":"2026-03-01T08:18:59.619156Z","iopub.status.idle":"2026-03-01T08:55:51.676579Z","shell.execute_reply.started":"2026-03-01T08:18:59.619128Z","shell.execute_reply":"2026-03-01T08:55:51.675674Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a8e9361a1a445e8d4a1573206a8341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d439ac76ea9439ba1e0a2242663f13e"}},"metadata":{}},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4396821de5bb4a099fc24f8c4ed3937b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/422 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601dbfec5dab49c1ad5cb809c5568bc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56aece376da54cc3b7692b34f2168de5"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-xls-r-300m\nKey                          | Status     | \n-----------------------------+------------+-\nproject_hid.bias             | UNEXPECTED | \nquantizer.codevectors        | UNEXPECTED | \nproject_q.weight             | UNEXPECTED | \nproject_hid.weight           | UNEXPECTED | \nproject_q.bias               | UNEXPECTED | \nquantizer.weight_proj.weight | UNEXPECTED | \nquantizer.weight_proj.bias   | UNEXPECTED | \nprojector.weight             | MISSING    | \nprojector.bias               | MISSING    | \nclassifier.weight            | MISSING    | \nclassifier.bias              | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='511' max='584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [511/584 25:26 < 03:38, 0.33 it/s, Epoch 7/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.377445</td>\n      <td>0.732292</td>\n      <td>0.169429</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.317038</td>\n      <td>0.717086</td>\n      <td>0.248389</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.211592</td>\n      <td>0.680204</td>\n      <td>0.412111</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.136489</td>\n      <td>0.619760</td>\n      <td>0.594648</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.032616</td>\n      <td>0.587912</td>\n      <td>0.648690</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.003026</td>\n      <td>0.581541</td>\n      <td>0.648690</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.932726</td>\n      <td>0.606039</td>\n      <td>0.590869</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"995f302ca1d1466d9273211e40aa2b0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cc1f5d13f744c16955c2fab56bed937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b93c97f1b8f43d59f20c561f8ff33db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d41ccc65fc429fab827e78ea37e265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3673ec9f8d13484b9b077ba08281a2eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5f54015a61343468b40a39bc15ef69a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cfd67dbd98643748cdc5401429f53bc"}},"metadata":{}},{"name":"stdout","text":" Stage1 best F1: 0.6486898154124177\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='219' max='584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [219/584 10:52 < 18:17, 0.33 it/s, Epoch 3/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.992047</td>\n      <td>0.645755</td>\n      <td>0.530999</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.908761</td>\n      <td>0.688552</td>\n      <td>0.431978</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.818890</td>\n      <td>0.751537</td>\n      <td>0.371104</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5741c5d7733d4a17aaafbc356c3e39a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a98e0258a2473b85b85a5137d699c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150067a76c124fcea32c77229f529dfa"}},"metadata":{}},{"name":"stdout","text":" Stage2 best F1: 0.5309987988542918\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"014d1de980634701aa45a4ed42134d02"}},"metadata":{}},{"name":"stdout","text":" Saved FINAL W2V2: /kaggle/working/wav2vec2_tamil_16k_stratspk/final\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nfrom sklearn.metrics import classification_report\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nGT_CSV     = \"/kaggle/input/datasets/tahmimahoque/tamil-test/Tamil_GT.xlsx - tam.csv\"\nFINAL_DIR  = \"/kaggle/working/wav2vec2_tamil_16k_stratspk/final\"\nSEARCH_ROOT = \"/kaggle/input\"   # search wavs anywhere in kaggle input\n\nassert os.path.isfile(GT_CSV), f\" Missing GT csv: {GT_CSV}\"\nassert os.path.isdir(FINAL_DIR), f\" Missing W2V2 model dir: {FINAL_DIR} (train+save first)\"\n\n\ngt = pd.read_csv(GT_CSV)\nassert {\"filename\",\"label\"}.issubset(gt.columns), f\" Need columns filename,label; got {gt.columns.tolist()}\"\n\n# labels: D->1, ND->0\ngt[\"label_num\"] = gt[\"label\"].map({\"D\": 1, \"ND\": 0})\nif gt[\"label_num\"].isna().any():\n    bad = gt[gt[\"label_num\"].isna()][[\"filename\",\"label\"]].head(10)\n    raise ValueError(f\" Unknown labels found. Examples:\\n{bad}\")\n\ntargets = set(gt[\"filename\"].astype(str).str.lower().tolist())\n\n\nfound = {}\nfor root, dirs, files in os.walk(SEARCH_ROOT):\n    for fn in files:\n        low = fn.lower()\n        if low in targets:  # exact filename match\n            found[low] = os.path.join(root, fn)\n\nprint(f\" Found audio: {len(found)}/{len(gt)}\")\nif len(found) < len(gt):\n    missing = [f for f in gt[\"filename\"].tolist() if str(f).lower() not in found]\n    print(\" Missing examples (first 20):\", missing[:20])\n\nassert len(found) > 0, (\n    \" No matching wav found in /kaggle/input. \"\n    \"We need to add the dataset that contains t1.wav, t2.wav, ... to this notebook.\"\n)\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(FINAL_DIR)\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(FINAL_DIR).to(device)\nmodel.eval()\n\n\n@torch.no_grad()\ndef predict_one(audio_path):\n    audio, _ = librosa.load(audio_path, sr=16000, mono=True)\n    audio, _ = librosa.effects.trim(audio, top_db=30)\n\n    if audio is None or len(audio) == 0:\n        audio = np.zeros(int(16000 * 0.5), dtype=np.float32)\n\n    feats = feature_extractor([audio], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n    feats = {k: v.to(device) for k, v in feats.items()}\n    logits = model(**feats).logits\n    return int(torch.argmax(logits, dim=1).item())\n\ny_true, y_pred = [], []\nmissing_audio = 0\n\nfor _, row in gt.iterrows():\n    key = str(row[\"filename\"]).lower()\n    if key not in found:\n        missing_audio += 1\n        continue\n\n    fp = found[key]\n    pred = predict_one(fp)\n\n    y_true.append(int(row[\"label_num\"]))\n    y_pred.append(pred)\n\nprint(\"Missing audio skipped:\", missing_audio)\nprint(\"\\n W2V2 CLASSIFICATION REPORT (TEST)\")\nprint(classification_report(y_true, y_pred, digits=4, target_names=[\"ND(0)\", \"D(1)\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T08:57:04.161252Z","iopub.execute_input":"2026-03-01T08:57:04.161598Z","iopub.status.idle":"2026-03-01T08:57:25.655758Z","shell.execute_reply.started":"2026-03-01T08:57:04.161571Z","shell.execute_reply":"2026-03-01T08:57:25.654981Z"}},"outputs":[{"name":"stdout","text":" Found audio: 160/160\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/426 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fd659b286ad4404872b25c3139fa254"}},"metadata":{}},{"name":"stdout","text":"Missing audio skipped: 0\n\n W2V2 CLASSIFICATION REPORT (TEST)\n              precision    recall  f1-score   support\n\n       ND(0)     1.0000    0.1500    0.2609        80\n        D(1)     0.5405    1.0000    0.7018        80\n\n    accuracy                         0.5750       160\n   macro avg     0.7703    0.5750    0.4813       160\nweighted avg     0.7703    0.5750    0.4813       160\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Ensemble Optimization with Threshold Adjustment\n\nFine-tunes MFCC + Wav2Vec2 ensemble weight and slightly increases the decision threshold to improve Non-Depressed predictions while maintaining balanced overall performance.","metadata":{}},{"cell_type":"code","source":"import os, re, glob, json\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    f1_score,\n    fbeta_score\n)\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nGT_CSV      = \"/kaggle/input/datasets/tahmimahoque/tamil-test/Tamil_GT.xlsx - tam.csv\"\nSEARCH_ROOT = \"/kaggle/input\"\n\nCSV_VAL     = \"/kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\"\nMFCC_PATH   = \"/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\"\nSTAGE2_DIR  = \"/kaggle/working/wav2vec2_tamil_16k_stratspk/stage2\"\nBASE_W2V2   = \"facebook/wav2vec2-xls-r-300m\"\n\nassert os.path.isfile(GT_CSV), f\" Missing GT CSV: {GT_CSV}\"\nassert os.path.isfile(CSV_VAL), f\" Missing VAL CSV: {CSV_VAL}\"\nassert os.path.isfile(MFCC_PATH), f\" Missing MFCC model: {MFCC_PATH}\"\nassert os.path.isdir(STAGE2_DIR), f\" Missing W2V2 stage2 dir: {STAGE2_DIR}\"\n\n\ndef find_best_checkpoint(stage2_dir: str) -> str:\n    state_path = os.path.join(stage2_dir, \"trainer_state.json\")\n    if os.path.isfile(state_path):\n        with open(state_path, \"r\") as f:\n            state = json.load(f)\n        best_ckpt = state.get(\"best_model_checkpoint\", None)\n        if best_ckpt and os.path.isdir(best_ckpt):\n            return best_ckpt\n\n    ckpts = glob.glob(os.path.join(stage2_dir, \"checkpoint-*\"))\n    if not ckpts:\n        raise FileNotFoundError(f\" No checkpoints found under {stage2_dir}\")\n\n    def step_num(p):\n        m = re.search(r\"checkpoint-(\\d+)$\", p)\n        return int(m.group(1)) if m else -1\n\n    return sorted(ckpts, key=step_num)[-1]\n\nW2V2_CKPT = find_best_checkpoint(STAGE2_DIR)\nprint(\" Using W2V2 checkpoint:\", W2V2_CKPT)\n\n\ndef extract_mfcc(file_path, target_sr=16000, n_mfcc=40, max_len=320):\n    y, _ = librosa.load(file_path, sr=target_sr, mono=True)\n    y, _ = librosa.effects.trim(y, top_db=30)\n    if y is None or len(y) == 0:\n        y = np.zeros(int(target_sr * 0.5), dtype=np.float32)\n\n    mfcc = librosa.feature.mfcc(y=y, sr=target_sr, n_mfcc=n_mfcc)\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n\n    T = mfcc.shape[1]\n    if T < max_len:\n        mfcc = np.pad(mfcc, ((0,0),(0, max_len-T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_len]\n    return mfcc.astype(np.float32)\n\nclass MFCC_CNN(nn.Module):\n    def __init__(self, n_classes=2):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d((2,2)), nn.Dropout(0.25),\n\n            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d((2,2)), nn.Dropout(0.25),\n\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1,1)),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.4),\n            nn.Linear(64, n_classes)\n        )\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.features(x)\n        return self.classifier(x)\n\nmfcc_model = MFCC_CNN().to(device)\nmfcc_model.load_state_dict(torch.load(MFCC_PATH, map_location=device))\nmfcc_model.eval()\n\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(BASE_W2V2, sampling_rate=16000)\nw2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(W2V2_CKPT).to(device)\nw2v2_model.eval()\n\nsoftmax = torch.nn.Softmax(dim=1)\n\n@torch.no_grad()\ndef mfcc_prob(file_path):\n    x = torch.from_numpy(extract_mfcc(file_path)).unsqueeze(0).to(device)\n    logits = mfcc_model(x)\n    return softmax(logits).squeeze(0).cpu().numpy()  # [2]\n\n@torch.no_grad()\ndef w2v2_prob(file_path):\n    audio, _ = librosa.load(file_path, sr=16000, mono=True)\n    audio, _ = librosa.effects.trim(audio, top_db=30)\n    if audio is None or len(audio) == 0:\n        audio = np.zeros(int(16000 * 0.5), dtype=np.float32)\n\n    feats = feature_extractor([audio], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n    feats = {k: v.to(device) for k, v in feats.items()}\n    logits = w2v2_model(**feats).logits\n    return softmax(logits).squeeze(0).cpu().numpy()  # [2]\n\n\ndf_all = pd.read_csv(CSV_VAL)\nval_df = df_all[df_all[\"split\"]==\"val\"].reset_index(drop=True)\nval_y  = val_df[\"label\"].astype(int).values\nval_fp = val_df[\"file_path\"].tolist()\n\nprint(\" VAL samples:\", len(val_fp))\n\nval_m, val_w = [], []\nfor fp in val_fp:\n    if not os.path.exists(fp):\n        raise FileNotFoundError(f\" Missing val audio: {fp}\")\n    val_m.append(mfcc_prob(fp))\n    val_w.append(w2v2_prob(fp))\n\nval_m = np.stack(val_m)  # [N,2]\nval_w = np.stack(val_w)  # [N,2]\n\n\nOBJECTIVE = \"f2\"  # keep your working setup\nbest = {\"score\": -1, \"w\": None, \"t\": None}\n\nw_grid = np.linspace(0.0, 1.0, 101)\nt_grid = np.linspace(0.10, 0.90, 81)\n\nfor w in w_grid:\n    p = w * val_m + (1 - w) * val_w\n    pD = p[:, 1]\n    for t in t_grid:\n        preds = (pD >= t).astype(int)\n\n        if OBJECTIVE == \"macro_f1\":\n            score = f1_score(val_y, preds, average=\"macro\")\n        else:\n            score = fbeta_score(val_y, preds, beta=2, pos_label=1)\n\n        if score > best[\"score\"]:\n            best.update({\"score\": float(score), \"w\": float(w), \"t\": float(t)})\n\nprint(f\" Tuned on VAL | objective={OBJECTIVE}: w_mfcc={best['w']:.2f}, thr={best['t']:.2f}, score={best['score']:.4f}\")\n\n\nTHR_BUMP = 0.03  \nthr_used = min(0.95, best[\"t\"] + THR_BUMP)\nprint(f\" Threshold used for TEST: {thr_used:.2f} (tuned thr={best['t']:.2f} + bump={THR_BUMP:.2f})\")\n\n@torch.no_grad()\ndef ensemble_predict(file_path):\n    p1 = mfcc_prob(file_path)\n    p2 = w2v2_prob(file_path)\n\n    p = best[\"w\"] * p1 + (1 - best[\"w\"]) * p2\n    pred = int(p[1] >= thr_used)\n\n    return pred, p\n\ngt = pd.read_csv(GT_CSV)\ngt[\"label_num\"] = gt[\"label\"].map({\"D\":1, \"ND\":0})\nif gt[\"label_num\"].isna().any():\n    bad = gt[gt[\"label_num\"].isna()][[\"filename\",\"label\"]].head(10)\n    raise ValueError(f\" Unknown labels found. Examples:\\n{bad}\")\n\ntargets = set(gt[\"filename\"].astype(str).str.lower().tolist())\n\nfound = {}\nfor root, _, files in os.walk(SEARCH_ROOT):\n    for fn in files:\n        low = fn.lower()\n        if low in targets:\n            found[low] = os.path.join(root, fn)\n\nprint(f\" Found TEST audio: {len(found)}/{len(gt)}\")\nassert len(found) > 0, \" No matching test wav found in /kaggle/input\"\n\ny_true, y_pred = [], []\nmissing_audio = 0\n\nfor _, row in gt.iterrows():\n    key = str(row[\"filename\"]).lower()\n    if key not in found:\n        missing_audio += 1\n        continue\n\n    fp = found[key]\n    p1 = mfcc_prob(fp)\n    p2 = w2v2_prob(fp)\n    p  = best[\"w\"] * p1 + (1 - best[\"w\"]) * p2\n\n    pred = int(p[1] >= thr_used)   # bumped threshold\n    y_true.append(int(row[\"label_num\"]))\n    y_pred.append(pred)\n\nprint(\"Missing audio skipped:\", missing_audio)\nprint(f\"\\n ENSEMBLE TEST REPORT | w_mfcc={best['w']:.2f}, thr_used={thr_used:.2f}\")\nprint(classification_report(y_true, y_pred, digits=4, target_names=[\"ND(0)\", \"D(1)\"]))\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[\"ND\",\"D\"], yticklabels=[\"ND\",\"D\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(f\"Confusion Matrix \")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:02:16.606230Z","iopub.execute_input":"2026-03-01T09:02:16.606928Z","iopub.status.idle":"2026-03-01T09:03:07.513443Z","shell.execute_reply.started":"2026-03-01T09:02:16.606897Z","shell.execute_reply":"2026-03-01T09:03:07.512698Z"}},"outputs":[{"name":"stdout","text":" Using W2V2 checkpoint: /kaggle/working/wav2vec2_tamil_16k_stratspk/stage2/checkpoint-219\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/426 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12bd49ebcbc24a24a6d7613119b70e31"}},"metadata":{}},{"name":"stdout","text":" VAL samples: 216\n Tuned on VAL | objective=f2: w_mfcc=0.00, thr=0.78, score=0.9938\n Threshold used for TEST: 0.81 (tuned thr=0.78 + bump=0.03)\n Found TEST audio: 160/160\nMissing audio skipped: 0\n\n ENSEMBLE TEST REPORT | w_mfcc=0.00, thr_used=0.81\n              precision    recall  f1-score   support\n\n       ND(0)     0.9277    0.9625    0.9448        80\n        D(1)     0.9610    0.9250    0.9427        80\n\n    accuracy                         0.9437       160\n   macro avg     0.9444    0.9438    0.9437       160\nweighted avg     0.9444    0.9437    0.9437       160\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x400 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# Prediction on test data (submission)","metadata":{}},{"cell_type":"code","source":"\n\nimport os, glob\nimport pandas as pd\n\nTEAM_NAME = \"TriVector\"\nRUN = \"run2\"\n\nTEST_DIR = \"/kaggle/input/datasets/tahmimahoque/depression-det/Depression_det/Depression_det/Test-set-tamil/Test-set-tamil\"\nSAVE_DIR = \"/kaggle/working/submissions\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# 1) Collect wav files\nwavs = glob.glob(os.path.join(TEST_DIR, \"**\", \"*.wav\"), recursive=True) + \\\n       glob.glob(os.path.join(TEST_DIR, \"**\", \"*.WAV\"), recursive=True)\n\nassert len(wavs) > 0, f\" No wav found in: {TEST_DIR}\"\nwavs = sorted(wavs)\nprint(\" Total test wavs:\", len(wavs))\n\n# 2) Predict (ensemble_predict MUST already be defined above)\ndef to_text_label(pred_int):\n    return \"Depressed\" if int(pred_int) == 1 else \"Non-depressed\"\n\nrows = []\nfor fp in wavs:\n    pred, _ = ensemble_predict(fp)   #  ensemble\n    file_id = os.path.splitext(os.path.basename(fp))[0]\n    rows.append({\"file_name\": file_id, \"labels\": to_text_label(pred)})\n\n# 3) Save CSV\ncsv_name = f\"{TEAM_NAME}_Tamil_{RUN}.csv\"\ncsv_path = os.path.join(SAVE_DIR, csv_name)\npd.DataFrame(rows).to_csv(csv_path, index=False)\n\nprint(\" Saved Tamil RUN2 CSV:\", csv_path)\nprint(pd.read_csv(csv_path).head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T09:03:19.734534Z","iopub.execute_input":"2026-03-01T09:03:19.734860Z","iopub.status.idle":"2026-03-01T09:03:34.623819Z","shell.execute_reply.started":"2026-03-01T09:03:19.734834Z","shell.execute_reply":"2026-03-01T09:03:34.622618Z"}},"outputs":[{"name":"stdout","text":" Total test wavs: 160\n Saved Tamil RUN2 CSV: /kaggle/working/submissions/TriVector_Tamil_run2.csv\n  file_name         labels\n0        t1      Depressed\n1       t10  Non-depressed\n2      t100      Depressed\n3      t101      Depressed\n4      t102      Depressed\n","output_type":"stream"}],"execution_count":15}]}