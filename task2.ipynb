{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14990500,"datasetId":9595634,"databundleVersionId":15864649},{"sourceType":"datasetVersion","sourceId":14996549,"datasetId":9599530,"databundleVersionId":15871270}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport torch\n\nSEED = 42\n\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Optional: stronger determinism (may error if some ops are nondeterministic)\n# torch.use_deterministic_algorithms(True)\n\ndef seed_worker(worker_id):\n    worker_seed = SEED + worker_id\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-28T18:04:13.930512Z","iopub.execute_input":"2026-02-28T18:04:13.930815Z","iopub.status.idle":"2026-02-28T18:04:18.116366Z","shell.execute_reply.started":"2026-02-28T18:04:13.930744Z","shell.execute_reply":"2026-02-28T18:04:18.115448Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Resampling\nRead Tamil from Kaggle input and save 16k audio to working directory","metadata":{}},{"cell_type":"code","source":"# =============================\n# ✅ BLOCK 2: Resample Tamil dataset to 16k safely (KAGGLE)\n# =============================\nimport os, glob\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom tqdm import tqdm\nimport random\n\nSRC_ROOT = \"/kaggle/input/datasets/tahmimahoque/depression-det/Depression_det/Depression_det/Tamil/Tamil\"\nDST_ROOT = \"/kaggle/working/Tamil_16k\"\nTARGET_SR = 16000\n\npairs = [\n    (os.path.join(SRC_ROOT, \"Depressed\", \"Train_set\"),\n     os.path.join(DST_ROOT, \"Depressed\", \"Train_set\")),\n    (os.path.join(SRC_ROOT, \"Non-depressed\", \"Train_set\"),\n     os.path.join(DST_ROOT, \"Non-depressed\", \"Train_set\")),\n]\n\nos.makedirs(DST_ROOT, exist_ok=True)\n\ntotal = converted = same_sr = failed = 0\n\nfor src_dir, dst_dir in pairs:\n\n    if not os.path.isdir(src_dir):\n        print(\"❌ Missing:\", src_dir)\n        continue\n\n    os.makedirs(dst_dir, exist_ok=True)\n\n    files = glob.glob(os.path.join(src_dir, \"**\", \"*.wav\"), recursive=True) + \\\n            glob.glob(os.path.join(src_dir, \"**\", \"*.WAV\"), recursive=True)\n\n    print(f\"Found {len(files)} files in {src_dir}\")\n    total += len(files)\n\n    for fp in tqdm(files, desc=f\"Resampling {os.path.basename(src_dir)}\"):\n        try:\n            y, sr = librosa.load(fp, sr=None, mono=True)\n\n            # ✅ Trim silence\n            y, _ = librosa.effects.trim(y, top_db=30)\n\n            if y.size == 0:\n                failed += 1\n                continue\n\n            # ✅ Resample if needed\n            if sr != TARGET_SR:\n                y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n                converted += 1\n            else:\n                same_sr += 1\n\n            # ✅ RMS normalize\n            rms = float(np.sqrt(np.mean(y**2) + 1e-8))\n            y = y / max(rms, 1e-3)\n            y = np.clip(y, -1.0, 1.0)\n\n            out_path = os.path.join(dst_dir, os.path.basename(fp))\n            sf.write(out_path, y, TARGET_SR, subtype=\"PCM_16\")\n\n        except Exception as e:\n            failed += 1\n\nprint(\"\\n==== DONE ====\")\nprint(\"Total files:\", total)\nprint(\"Already 16k:\", same_sr)\nprint(\"Resampled:\", converted)\nprint(\"Failed:\", failed)\n\n# ✅ Quick verify SR\ncheck = glob.glob(os.path.join(DST_ROOT, \"**\", \"*.wav\"), recursive=True)\nprint(\"Saved files:\", len(check))\n\nsample = random.sample(check, min(20, len(check))) if check else []\n\nsrs = []\nfor fp in sample:\n    _, sr = librosa.load(fp, sr=None, mono=True)\n    srs.append(sr)\n\nif srs:\n    vals, cnts = np.unique(srs, return_counts=True)\n    print(\"SR counts:\", dict(zip(vals.tolist(), cnts.tolist())))\n\nprint(\"✅ Output folder:\", DST_ROOT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T18:05:35.016105Z","iopub.execute_input":"2026-02-28T18:05:35.016732Z","iopub.status.idle":"2026-02-28T18:06:17.476502Z","shell.execute_reply.started":"2026-02-28T18:05:35.016700Z","shell.execute_reply":"2026-02-28T18:06:17.475699Z"}},"outputs":[{"name":"stdout","text":"Found 454 files in /kaggle/input/datasets/tahmimahoque/depression-det/Depression_det/Depression_det/Tamil/Tamil/Depressed/Train_set\n","output_type":"stream"},{"name":"stderr","text":"Resampling Train_set: 100%|██████████| 454/454 [00:22<00:00, 20.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Found 920 files in /kaggle/input/datasets/tahmimahoque/depression-det/Depression_det/Depression_det/Tamil/Tamil/Non-depressed/Train_set\n","output_type":"stream"},{"name":"stderr","text":"Resampling Train_set: 100%|██████████| 920/920 [00:18<00:00, 50.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n==== DONE ====\nTotal files: 1374\nAlready 16k: 454\nResampled: 920\nFailed: 0\nSaved files: 1374\nSR counts: {16000: 20}\n✅ Output folder: /kaggle/working/Tamil_16k\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Build Metadata\n\nCreate CSV from resampled Tamil_16k with labels and speaker IDs","metadata":{}},{"cell_type":"code","source":"# =============================\n# ✅ BLOCK 1 (KAGGLE): Build metadata CSV (from Tamil_16k)\n# - Reads from /kaggle/working/Tamil_16k\n# - Writes tamil_trainval_16k.csv into same root\n# =============================\nimport os, glob, re\nimport pandas as pd\n\nDATA_ROOT_16K = \"/kaggle/working/Tamil_16k\"\ndep_train = os.path.join(DATA_ROOT_16K, \"Depressed\", \"Train_set\")\nnd_train  = os.path.join(DATA_ROOT_16K, \"Non-depressed\", \"Train_set\")\n\ndef extract_speaker_from_path(fp: str) -> str:\n    base = os.path.splitext(os.path.basename(fp))[0]\n\n    # Non-depressed: ND1, ND2...\n    m = re.match(r\"^(ND\\d+)\", base, flags=re.IGNORECASE)\n    if m:\n        return m.group(1).upper()\n\n    # Depressed: D_<SPK>_... OR D_<SPK>-<rep>\n    if base.startswith(\"D_\"):\n        rest = base[2:]                 # remove D_\n        tok  = rest.split(\"_\")[0]       # first token after D_\n        tok  = re.sub(r\"[-_][0-9a-zA-Z]+$\", \"\", tok)  # remove trailing -3, _c, -2 etc.\n        return tok.upper()\n\n    # fallback\n    tok = base.split(\"_\")[0]\n    tok = re.sub(r\"[-_][0-9a-zA-Z]+$\", \"\", tok)\n    return tok.upper()\n\nrows = []\n\ndef collect(folder, label):\n    if not os.path.isdir(folder):\n        print(f\"❌ Missing folder: {folder}\")\n        return\n\n    wavs = glob.glob(os.path.join(folder, \"**\", \"*.wav\"), recursive=True) + \\\n           glob.glob(os.path.join(folder, \"**\", \"*.WAV\"), recursive=True)\n\n    print(f\"{folder} -> {len(wavs)} files\")\n\n    for f in wavs:\n        fname = os.path.splitext(os.path.basename(f))[0]\n        spk   = extract_speaker_from_path(f)\n        rows.append({\n            \"file_path\": f,\n            \"label\": int(label),\n            \"fname\": fname,\n            \"speaker\": spk\n        })\n\ncollect(dep_train, 1)\ncollect(nd_train, 0)\n\ndf = pd.DataFrame(rows)\n\nprint(\"\\n✅ Total samples:\", len(df))\nif len(df):\n    print(\"✅ Label counts:\\n\", df[\"label\"].value_counts())\n    print(\"✅ Unique speakers:\", df[\"speaker\"].nunique())\n    print(\"✅ Speakers per label:\\n\", df.groupby(\"label\")[\"speaker\"].nunique())\n\nOUT_CSV = os.path.join(DATA_ROOT_16K, \"tamil_trainval_16k.csv\")\ndf.to_csv(OUT_CSV, index=False)\nprint(\"\\n✅ Saved:\", OUT_CSV)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T18:07:20.477823Z","iopub.execute_input":"2026-02-28T18:07:20.478322Z","iopub.status.idle":"2026-02-28T18:07:20.543795Z","shell.execute_reply.started":"2026-02-28T18:07:20.478296Z","shell.execute_reply":"2026-02-28T18:07:20.543215Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Tamil_16k/Depressed/Train_set -> 454 files\n/kaggle/working/Tamil_16k/Non-depressed/Train_set -> 920 files\n\n✅ Total samples: 1374\n✅ Label counts:\n label\n0    920\n1    454\nName: count, dtype: int64\n✅ Unique speakers: 44\n✅ Speakers per label:\n label\n0     5\n1    39\nName: speaker, dtype: int64\n\n✅ Saved: /kaggle/working/Tamil_16k/tamil_trainval_16k.csv\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                           file_path  label       fname  \\\n0  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_23-2   \n1  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_44-3   \n2  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_24-1   \n3  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_F20013_c   \n4  /kaggle/working/Tamil_16k/Depressed/Train_set/...      1  D_S00_09-2   \n\n  speaker  \n0     S00  \n1     S00  \n2     S00  \n3  F20013  \n4     S00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_path</th>\n      <th>label</th>\n      <th>fname</th>\n      <th>speaker</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_23-2</td>\n      <td>S00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_44-3</td>\n      <td>S00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_24-1</td>\n      <td>S00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_F20013_c</td>\n      <td>F20013</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/working/Tamil_16k/Depressed/Train_set/...</td>\n      <td>1</td>\n      <td>D_S00_09-2</td>\n      <td>S00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Speaker Sanity Check","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# ✅ Kaggle paths\nCSV_PATH = \"/kaggle/working/Tamil_16k/tamil_trainval_16k.csv\"\ndf = pd.read_csv(CSV_PATH)\n\nprint(\"✅ Loaded:\", CSV_PATH)\nprint(\"Total rows:\", len(df))\nprint(\"Unique speakers:\", df[\"speaker\"].nunique())\nprint(\"Speakers per label:\\n\", df.groupby(\"label\")[\"speaker\"].nunique())\n\n# 1) Mixed-label speakers check (speaker leakage risk)\nmix = df.groupby(\"speaker\")[\"label\"].nunique()\nmixed = mix[mix > 1]\nprint(\"\\nMixed-label speakers (should be 0):\", len(mixed))\nif len(mixed):\n    print(mixed.head(20))\n\n# 2) Show top speakers by #files (helps spot parsing bug)\nprint(\"\\nTop 15 speakers by file-count:\")\nprint(df[\"speaker\"].value_counts().head(15))\n\n# 3) Pattern check: speakers that look suspicious (not ND#, not S##, not F#### etc.)\ndef is_ok_spk(s):\n    s = str(s)\n    if re.fullmatch(r\"ND\\d+\", s): return True\n    if re.fullmatch(r\"S\\d+\", s): return True          # S00, S01...\n    if re.fullmatch(r\"F\\d+\", s): return True          # F2006...\n    if re.fullmatch(r\"[A-Z]\\d+\", s): return True      # A1 type (just in case)\n    return False\n\nbad = df[~df[\"speaker\"].apply(is_ok_spk)]\nprint(\"\\nSuspicious speaker IDs:\", bad[\"speaker\"].nunique())\nprint(bad[[\"fname\",\"speaker\",\"label\"]].head(30))\n\n# 4) Depressed files where speaker still contains '-' or '_' at end (bad trimming)\n# (your check: any '-' or '_' anywhere)\nbad_dep = df[(df[\"label\"]==1) & (df[\"speaker\"].astype(str).str.contains(r\"[-_]\", regex=True))]\nprint(\"\\nDepressed speakers containing '-' or '_' (should be 0 ideally):\", bad_dep[\"speaker\"].nunique())\nprint(bad_dep[[\"fname\",\"speaker\"]].head(30))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T18:08:11.875326Z","iopub.execute_input":"2026-02-28T18:08:11.876066Z","iopub.status.idle":"2026-02-28T18:08:11.899513Z","shell.execute_reply.started":"2026-02-28T18:08:11.876036Z","shell.execute_reply":"2026-02-28T18:08:11.898991Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded: /kaggle/working/Tamil_16k/tamil_trainval_16k.csv\nTotal rows: 1374\nUnique speakers: 44\nSpeakers per label:\n label\n0     5\n1    39\nName: speaker, dtype: int64\n\nMixed-label speakers (should be 0): 0\n\nTop 15 speakers by file-count:\nspeaker\nS00       241\nND2       184\nND1       184\nND5       184\nND4       184\nND3       184\nA00        80\nF2009       4\nF10019      4\nF10010      4\nF10013      4\nF20013      4\nF10018      4\nF1008       4\nF2007       4\nName: count, dtype: int64\n\nSuspicious speaker IDs: 0\nEmpty DataFrame\nColumns: [fname, speaker, label]\nIndex: []\n\nDepressed speakers containing '-' or '_' (should be 0 ideally): 0\nEmpty DataFrame\nColumns: [fname, speaker]\nIndex: []\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Speaker Split\n\nSpeaker-stratified train/val split","metadata":{}},{"cell_type":"code","source":"# =============================\n# ✅ BLOCK 3 (KAGGLE, BEST): Speaker-stratified Train/Val split (safe + balanced)\n# - VAL gets: 1 ND speaker + ~20% Dep speakers (min 2)\n# - Deterministic (seed=42)\n# Output: /kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\n# =============================\nimport numpy as np\nimport pandas as pd\nimport os\n\nIN_CSV  = \"/kaggle/working/Tamil_16k/tamil_trainval_16k.csv\"\nOUT_CSV = \"/kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\"\n\nassert os.path.isfile(IN_CSV), f\"❌ Missing IN_CSV: {IN_CSV}. Run BLOCK 1 first.\"\n\ndf = pd.read_csv(IN_CSV)\n\n# 0) Hard sanity: no mixed-label speakers\nmix = df.groupby(\"speaker\")[\"label\"].nunique()\nmixed = mix[mix > 1]\nassert len(mixed) == 0, f\"❌ Mixed-label speakers found: {len(mixed)}. Fix speaker parsing first.\"\n\n# 1) Speaker -> label (majority label; but mixed already asserted 0)\nspk_label = df.groupby(\"speaker\")[\"label\"].agg(lambda x: int(x.value_counts().idxmax())).reset_index()\ndep_spk = spk_label[spk_label[\"label\"] == 1][\"speaker\"].tolist()\nnd_spk  = spk_label[spk_label[\"label\"] == 0][\"speaker\"].tolist()\n\nprint(\"✅ Dep speakers:\", len(dep_spk), \"| ND speakers:\", len(nd_spk))\n\n# 2) Choose VAL speakers\nrng = np.random.default_rng(42)\n\n# ND speakers are only 5 -> keep VAL ND = 1 (best compromise)\nn_nd_val = 1 if len(nd_spk) >= 2 else len(nd_spk)\n\n# Dep speakers are many -> take 20% but at least 2\nval_frac_dep = 0.20\nn_dep_val = int(round(len(dep_spk) * val_frac_dep))\nn_dep_val = max(2, n_dep_val) if len(dep_spk) >= 2 else len(dep_spk)\n\nval_nd  = set(rng.choice(nd_spk,  size=n_nd_val, replace=False).tolist()) if n_nd_val > 0 else set()\nval_dep = set(rng.choice(dep_spk, size=n_dep_val, replace=False).tolist()) if n_dep_val > 0 else set()\n\nval_spk = val_nd | val_dep\n\n# 3) Assign split\ndf[\"split\"] = \"train\"\ndf.loc[df[\"speaker\"].isin(val_spk), \"split\"] = \"val\"\n\n# 4) Checks\nprint(\"\\n✅ Split counts:\\n\", df[\"split\"].value_counts())\nprint(\"\\n✅ Label counts by split:\\n\", df.groupby([\"split\",\"label\"]).size())\nprint(\"\\n✅ Speakers by split:\\n\", df.groupby(\"split\")[\"speaker\"].nunique())\n\ntrain_spk = set(df[df[\"split\"]==\"train\"][\"speaker\"])\nval_spk2  = set(df[df[\"split\"]==\"val\"][\"speaker\"])\noverlap = train_spk & val_spk2\nprint(\"\\n✅ Speaker overlap (MUST be empty):\", overlap)\n\nassert len(overlap) == 0, \"❌ Speaker leakage: overlap is NOT empty!\"\n\n# 5) Save\nos.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\ndf.to_csv(OUT_CSV, index=False)\nprint(\"\\n✅ Saved:\", OUT_CSV)\n\nprint(\"\\nVAL speakers (for record):\", sorted(list(val_spk2)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T18:09:30.685509Z","iopub.execute_input":"2026-02-28T18:09:30.686268Z","iopub.status.idle":"2026-02-28T18:09:30.733902Z","shell.execute_reply.started":"2026-02-28T18:09:30.686237Z","shell.execute_reply":"2026-02-28T18:09:30.733025Z"}},"outputs":[{"name":"stdout","text":"✅ Dep speakers: 39 | ND speakers: 5\n\n✅ Split counts:\n split\ntrain    1158\nval       216\nName: count, dtype: int64\n\n✅ Label counts by split:\n split  label\ntrain  0        736\n       1        422\nval    0        184\n       1         32\ndtype: int64\n\n✅ Speakers by split:\n split\ntrain    35\nval       9\nName: speaker, dtype: int64\n\n✅ Speaker overlap (MUST be empty): set()\n\n✅ Saved: /kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\n\nVAL speakers (for record): ['F10011', 'F10015', 'F10021', 'F10022', 'F1007', 'F2001', 'F20011', 'F20015', 'ND1']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# MFCC","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import f1_score\n\n# =============================\n# 0) Reproducibility (IMPORTANT)\n# =============================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ✅ Kaggle paths\nCSV_PATH = \"/kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\"\nSAVE_DIR = \"/kaggle/working/models_tamil\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nassert os.path.isfile(CSV_PATH), f\"❌ Missing CSV: {CSV_PATH}. Run BLOCK 1→3 first.\"\n\ndf = pd.read_csv(CSV_PATH)\n\n# =============================\n# 1) MFCC extraction\n# =============================\ndef extract_mfcc(file_path, target_sr=16000, n_mfcc=40, max_len=320):\n    y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n\n    # (optional) trim silence to reduce padding waste (safe)\n    y, _ = librosa.effects.trim(y, top_db=30)\n\n    # guard: avoid crash if empty after trim\n    if y is None or len(y) == 0:\n        y = np.zeros(int(target_sr * 0.5), dtype=np.float32)  # 0.5s silence fallback\n\n    mfcc = librosa.feature.mfcc(y=y, sr=target_sr, n_mfcc=n_mfcc)  # [n_mfcc, T]\n\n    # utterance-level CMVN (safe)\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n\n    T = mfcc.shape[1]\n    if T < max_len:\n        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_len]\n\n    return mfcc.astype(np.float32)\n\nclass TamilMFCCDataset(Dataset):\n    def __init__(self, df, split):\n        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        x = extract_mfcc(row[\"file_path\"])\n        y = int(row[\"label\"])\n        return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n\ntrain_ds = TamilMFCCDataset(df, \"train\")\nval_ds   = TamilMFCCDataset(df, \"val\")\n\n# =============================\n# 2) DataLoader (balanced sampling helps a lot)\n# =============================\ndef seed_worker(worker_id):\n    s = SEED + worker_id\n    np.random.seed(s); random.seed(s)\n\ng = torch.Generator()\ng.manual_seed(SEED)\n\n# WeightedRandomSampler for TRAIN only (handles skew better than only class weights)\ntrain_labels = df[df[\"split\"]==\"train\"][\"label\"].astype(int).values\nclass_counts = np.bincount(train_labels, minlength=2)          # [count0, count1]\nclass_weights = 1.0 / np.maximum(class_counts, 1)             # inverse freq\nsample_weights = class_weights[train_labels]                  # per sample weight\n\nsampler = WeightedRandomSampler(\n    weights=torch.tensor(sample_weights, dtype=torch.double),\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=32,\n    sampler=sampler,              # ✅ use sampler instead of shuffle\n    shuffle=False,\n    num_workers=2,                # if Kaggle stuck, set 0\n    worker_init_fn=seed_worker,\n    generator=g,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=64,\n    shuffle=False,\n    num_workers=2,                # if Kaggle stuck, set 0\n    worker_init_fn=seed_worker,\n    generator=g,\n    pin_memory=True\n)\n\n# =============================\n# 3) Model\n# =============================\nclass MFCC_CNN(nn.Module):\n    def __init__(self, n_classes=2):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1,1)),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.4),\n            nn.Linear(64, n_classes)\n        )\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # [B,1,40,320]\n        x = self.features(x)\n        return self.classifier(x)\n\nmodel = MFCC_CNN().to(device)\n\n# Loss: still keep class weights (fine), even though sampler already helps\ncw = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=train_labels)\ncw = torch.tensor(cw, dtype=torch.float32).to(device)\ncriterion = nn.CrossEntropyLoss(weight=cw)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\n# LR scheduler improves stability\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"max\", factor=0.5, patience=2\n)\n\n# =============================\n# 4) Eval\n# =============================\ndef eval_macro_f1(model):\n    model.eval()\n    ys, ps = [], []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            logits = model(x)\n            pred = torch.argmax(logits, dim=1)\n            ys.extend(y.cpu().numpy().tolist())\n            ps.extend(pred.cpu().numpy().tolist())\n    return f1_score(ys, ps, average=\"macro\")\n\n# =============================\n# 5) Train with early stopping\n# =============================\nbest_f1 = -1.0\nbest_state = None\npatience = 6\nbad = 0\nEPOCHS = 30\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    total_loss = 0.0\n\n    for x, y in train_loader:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    val_f1 = eval_macro_f1(model)\n    scheduler.step(val_f1)\n\n    print(f\"Epoch {epoch:02d} | train_loss={total_loss/len(train_loader):.4f} | val_macroF1={val_f1:.4f}\")\n\n    if val_f1 > best_f1 + 1e-4:\n        best_f1 = val_f1\n        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        bad = 0\n    else:\n        bad += 1\n        if bad >= patience:\n            print(\"Early stopping.\")\n            break\n\n# Safety: best_state fallback\nif best_state is None:\n    print(\"⚠️ No improvement detected; saving last epoch weights.\")\nelse:\n    model.load_state_dict(best_state)\n\nprint(\"✅ Best MFCC val macro-F1:\", best_f1)\n\nmfcc_path = os.path.join(SAVE_DIR, \"mfcc_cnn_tamil_16k_stratspk.pt\")\ntorch.save(model.state_dict(), mfcc_path)\nprint(\"✅ Saved:\", mfcc_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T18:12:23.345504Z","iopub.execute_input":"2026-02-28T18:12:23.346137Z","iopub.status.idle":"2026-02-28T18:18:52.988560Z","shell.execute_reply.started":"2026-02-28T18:12:23.346105Z","shell.execute_reply":"2026-02-28T18:18:52.987795Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | train_loss=0.5848 | val_macroF1=0.1638\nEpoch 02 | train_loss=0.3986 | val_macroF1=0.8499\nEpoch 03 | train_loss=0.2208 | val_macroF1=0.7824\nEpoch 04 | train_loss=0.1880 | val_macroF1=0.7609\nEpoch 05 | train_loss=0.1382 | val_macroF1=0.8882\nEpoch 06 | train_loss=0.1347 | val_macroF1=0.8345\nEpoch 07 | train_loss=0.1097 | val_macroF1=0.8288\nEpoch 08 | train_loss=0.1092 | val_macroF1=0.9185\nEpoch 09 | train_loss=0.0988 | val_macroF1=0.8645\nEpoch 10 | train_loss=0.0964 | val_macroF1=0.9489\nEpoch 11 | train_loss=0.0842 | val_macroF1=0.9382\nEpoch 12 | train_loss=0.0796 | val_macroF1=0.8288\nEpoch 13 | train_loss=0.0782 | val_macroF1=0.8773\nEpoch 14 | train_loss=0.0980 | val_macroF1=0.9113\nEpoch 15 | train_loss=0.0702 | val_macroF1=0.9258\nEpoch 16 | train_loss=0.0895 | val_macroF1=0.9258\nEarly stopping.\n✅ Best MFCC val macro-F1: 0.9488555643251776\n✅ Saved: /kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink, display\n\n# ✅ your model path (as saved in training code)\nmodel_path = \"/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\"\n\nassert os.path.isfile(model_path), f\"❌ Model not found: {model_path}\"\n\nprint(\"✅ Model ready:\", model_path)\ndisplay(FileLink(model_path))   # click to download","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T18:20:09.849512Z","iopub.execute_input":"2026-02-28T18:20:09.849876Z","iopub.status.idle":"2026-02-28T18:20:09.856569Z","shell.execute_reply.started":"2026-02-28T18:20:09.849845Z","shell.execute_reply":"2026-02-28T18:20:09.856014Z"}},"outputs":[{"name":"stdout","text":"✅ Model ready: /kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt","text/html":"<a href='/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt' target='_blank'>/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt</a><br>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import classification_report\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =============================\n# Paths\n# =============================\nGT_CSV    = \"/kaggle/input/datasets/tahmimahoque/tamil-test/Tamil_GT.xlsx - tam.csv\"\nMODEL_PATH = \"/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\"\nSEARCH_ROOT = \"/kaggle/input\"   # search wavs anywhere in kaggle input\n\nassert os.path.isfile(GT_CSV), f\"❌ Missing GT csv: {GT_CSV}\"\nassert os.path.isfile(MODEL_PATH), f\"❌ Missing MFCC model: {MODEL_PATH}\"\n\ngt = pd.read_csv(GT_CSV)\nassert {\"filename\",\"label\"}.issubset(gt.columns), f\"❌ Need columns filename,label; got {gt.columns.tolist()}\"\n\n# labels: D->1, ND->0\ngt[\"label_num\"] = gt[\"label\"].map({\"D\": 1, \"ND\": 0})\nif gt[\"label_num\"].isna().any():\n    bad = gt[gt[\"label_num\"].isna()][[\"filename\",\"label\"]].head(10)\n    raise ValueError(f\"❌ Unknown labels found. Examples:\\n{bad}\")\n\ntargets = set(gt[\"filename\"].astype(str).str.lower().tolist())\n\n# =============================\n# 1) Locate audio files in /kaggle/input\n# =============================\nfound = {}\nfor root, dirs, files in os.walk(SEARCH_ROOT):\n    for fn in files:\n        low = fn.lower()\n        if low in targets:  # exact filename match\n            found[low] = os.path.join(root, fn)\n\nprint(f\"✅ Found audio: {len(found)}/{len(gt)}\")\nif len(found) < len(gt):\n    missing = [f for f in gt[\"filename\"].tolist() if str(f).lower() not in found]\n    print(\"⚠️ Missing examples (first 20):\", missing[:20])\n\n# If no audio found, stop early (no point running model)\nassert len(found) > 0, (\n    \"❌ No matching wav found in /kaggle/input. \"\n    \"You need to add the dataset that contains t1.wav, t2.wav, ... to this notebook.\"\n)\n\n# =============================\n# 2) MFCC extraction (same as your training)\n# =============================\ndef extract_mfcc(file_path, target_sr=16000, n_mfcc=40, max_len=320):\n    y, _ = librosa.load(file_path, sr=target_sr, mono=True)\n    y, _ = librosa.effects.trim(y, top_db=30)\n\n    if y is None or len(y) == 0:\n        y = np.zeros(int(target_sr * 0.5), dtype=np.float32)\n\n    mfcc = librosa.feature.mfcc(y=y, sr=target_sr, n_mfcc=n_mfcc)\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n\n    T = mfcc.shape[1]\n    if T < max_len:\n        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_len]\n\n    return mfcc.astype(np.float32)\n\n# =============================\n# 3) Model (same architecture)\n# =============================\nclass MFCC_CNN(nn.Module):\n    def __init__(self, n_classes=2):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d((2,2)),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1,1)),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.4),\n            nn.Linear(64, n_classes)\n        )\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.features(x)\n        return self.classifier(x)\n\nmodel = MFCC_CNN().to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device))\nmodel.eval()\n\n# =============================\n# 4) Predict + Classification report (no confusion matrix)\n# =============================\ny_true, y_pred = [], []\nmissing_audio = 0\n\nwith torch.no_grad():\n    for _, row in gt.iterrows():\n        key = str(row[\"filename\"]).lower()\n        if key not in found:\n            missing_audio += 1\n            continue\n\n        fp = found[key]\n        x = torch.from_numpy(extract_mfcc(fp)).unsqueeze(0).to(device)  # [1,40,320]\n        logits = model(x)\n        pred = int(torch.argmax(logits, dim=1).item())\n\n        y_true.append(int(row[\"label_num\"]))\n        y_pred.append(pred)\n\nprint(\"Missing audio skipped:\", missing_audio)\nprint(\"\\n✅ MFCC CLASSIFICATION REPORT (TEST)\")\nprint(classification_report(y_true, y_pred, digits=4, target_names=[\"ND(0)\", \"D(1)\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T19:11:36.151173Z","iopub.execute_input":"2026-02-28T19:11:36.151878Z","iopub.status.idle":"2026-02-28T19:11:45.384203Z","shell.execute_reply.started":"2026-02-28T19:11:36.151847Z","shell.execute_reply":"2026-02-28T19:11:45.381315Z"}},"outputs":[{"name":"stdout","text":"✅ Found audio: 160/160\nMissing audio skipped: 0\n\n✅ MFCC CLASSIFICATION REPORT (TEST)\n              precision    recall  f1-score   support\n\n       ND(0)     0.6504    1.0000    0.7882        80\n        D(1)     1.0000    0.4625    0.6325        80\n\n    accuracy                         0.7312       160\n   macro avg     0.8252    0.7312    0.7103       160\nweighted avg     0.8252    0.7312    0.7103       160\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Wev2Vec2","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nimport torch, librosa\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    Wav2Vec2FeatureExtractor,\n    Wav2Vec2ForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback\n)\n\n# =============================\n# 0) Seeds / deterministic\n# =============================\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\ntorch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ✅ Kaggle paths\nCSV_PATH = \"/kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\"\nassert os.path.isfile(CSV_PATH), f\"❌ Missing CSV: {CSV_PATH}. Run BLOCK 1→3 first.\"\n\ndf = pd.read_csv(CSV_PATH)\n\nW2V2_DIR = \"/kaggle/working/wav2vec2_tamil_16k_stratspk\"\nos.makedirs(W2V2_DIR, exist_ok=True)\n\n# =============================\n# 1) Feature extractor\n# =============================\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n    \"facebook/wav2vec2-xls-r-300m\",\n    sampling_rate=16000\n)\n\n# =============================\n# 2) Dataset + collate\n# =============================\nclass TamilW2V2Dataset(Dataset):\n    def __init__(self, df, split):\n        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        audio, _ = librosa.load(row[\"file_path\"], sr=16000, mono=True)\n        audio, _ = librosa.effects.trim(audio, top_db=30)\n        if audio is None or len(audio) == 0:\n            audio = np.zeros(int(16000 * 0.5), dtype=np.float32)  # 0.5s fallback\n        return {\"input_values\": audio, \"labels\": int(row[\"label\"])}\n\ntrain_ds = TamilW2V2Dataset(df, \"train\")\nval_ds   = TamilW2V2Dataset(df, \"val\")\n\ndef collate_fn(batch):\n    inputs = [b[\"input_values\"] for b in batch]\n    labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n    feats = feature_extractor(inputs, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n    feats[\"labels\"] = labels\n    return feats\n\n# =============================\n# 3) Class weights\n# =============================\ntrain_labels = df[df[\"split\"]==\"train\"][\"label\"].astype(int).values\ncw = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=train_labels)\ncw = torch.tensor(cw, dtype=torch.float32)  # keep CPU; move inside loss\n\n# =============================\n# 4) Weighted Trainer (version-proof)\n# =============================\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        w = cw.to(logits.device)\n        loss = torch.nn.functional.cross_entropy(logits, labels, weight=w)\n        return (loss, outputs) if return_outputs else loss\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    if isinstance(logits, (tuple, list)):\n        logits = logits[0]\n    preds = np.argmax(logits, axis=1)\n    return {\"f1\": f1_score(labels, preds, average=\"macro\")}\n\n# =============================\n# 5) Build model\n# =============================\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\n    \"facebook/wav2vec2-xls-r-300m\",\n    num_labels=2,\n    problem_type=\"single_label_classification\"\n).to(device)\n\n# =============================\n# STAGE 1: Freeze encoder (stable)\n# =============================\nfor p in model.wav2vec2.parameters():\n    p.requires_grad = False\nfor p in model.classifier.parameters():\n    p.requires_grad = True\n\nargs_stage1 = TrainingArguments(\n    output_dir=os.path.join(W2V2_DIR, \"stage1\"),\n\n    # HF version-safe: some versions use evaluation_strategy\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n\n    learning_rate=5e-5,\n    num_train_epochs=8,\n    warmup_steps=50,\n    weight_decay=0.01,\n\n    fp16=True,\n    max_grad_norm=1.0,\n    logging_steps=50,\n    save_total_limit=2,\n    report_to=\"none\",\n    seed=SEED,\n    remove_unused_columns=False\n)\n\ntrainer1 = WeightedTrainer(\n    model=model,\n    args=args_stage1,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\ntrainer1.train()\nprint(\"✅ Stage1 best F1:\", trainer1.state.best_metric)\n\n# =============================\n# STAGE 2: Unfreeze last 2 layers (fp16 OFF for stability)\n# =============================\nfor p in model.wav2vec2.parameters():\n    p.requires_grad = False\n\nfor layer in model.wav2vec2.encoder.layers[-2:]:\n    for p in layer.parameters():\n        p.requires_grad = True\n\nfor p in model.classifier.parameters():\n    p.requires_grad = True\n\nargs_stage2 = TrainingArguments(\n    output_dir=os.path.join(W2V2_DIR, \"stage2\"),\n\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n\n    learning_rate=1e-5,\n    num_train_epochs=8,\n    warmup_steps=30,\n    weight_decay=0.01,\n\n    fp16=False,                 # ✅ stability fix\n    max_grad_norm=1.0,\n    logging_steps=50,\n    save_total_limit=2,\n    report_to=\"none\",\n    seed=SEED,\n    remove_unused_columns=False\n)\n\ntrainer2 = WeightedTrainer(\n    model=model,\n    args=args_stage2,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\ntrainer2.train()\nprint(\"✅ Stage2 best F1:\", trainer2.state.best_metric)\n\n# =============================\n# Save final (Kaggle working)\n# =============================\nFINAL_DIR = os.path.join(W2V2_DIR, \"final\")\nos.makedirs(FINAL_DIR, exist_ok=True)\ntrainer2.model.save_pretrained(FINAL_DIR)\nfeature_extractor.save_pretrained(FINAL_DIR)\nprint(\"✅ Saved FINAL W2V2:\", FINAL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T18:29:54.021362Z","iopub.execute_input":"2026-02-28T18:29:54.021917Z","iopub.status.idle":"2026-02-28T19:11:15.041859Z","shell.execute_reply.started":"2026-02-28T18:29:54.021886Z","shell.execute_reply":"2026-02-28T19:11:15.041230Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/422 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb74f0abd134e5a89bc47202ce4dbb4"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-xls-r-300m\nKey                          | Status     | \n-----------------------------+------------+-\nquantizer.weight_proj.weight | UNEXPECTED | \nproject_q.weight             | UNEXPECTED | \nquantizer.codevectors        | UNEXPECTED | \nproject_hid.weight           | UNEXPECTED | \nproject_q.bias               | UNEXPECTED | \nproject_hid.bias             | UNEXPECTED | \nquantizer.weight_proj.bias   | UNEXPECTED | \nclassifier.bias              | MISSING    | \nprojector.weight             | MISSING    | \nclassifier.weight            | MISSING    | \nprojector.bias               | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='584' max='584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [584/584 29:54, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.377728</td>\n      <td>0.745079</td>\n      <td>0.163810</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.318523</td>\n      <td>0.726357</td>\n      <td>0.233334</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.200667</td>\n      <td>0.694417</td>\n      <td>0.395929</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.116570</td>\n      <td>0.622674</td>\n      <td>0.579575</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.035288</td>\n      <td>0.592111</td>\n      <td>0.632979</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.003225</td>\n      <td>0.570781</td>\n      <td>0.660662</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.920664</td>\n      <td>0.568193</td>\n      <td>0.660662</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.920280</td>\n      <td>0.576966</td>\n      <td>0.636882</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"716e7d8b5ad6497283bc8815f839f132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad9b0323ba5543168e155bdc145be57c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42fa396b22234c01bcad5462f477684c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e2f0852f444cd3bd1ea6452b03727b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b3509bb56fe4d369eb53110e74438f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a60a12daaa5e4eb0bd5f3e720bde39eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41815fc87ba84ace90d660fdc9e1084f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a491e9282d3457091c75ef04e64497d"}},"metadata":{}},{"name":"stdout","text":"✅ Stage1 best F1: 0.6606619187264349\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='219' max='584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [219/584 11:13 < 18:53, 0.32 it/s, Epoch 3/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.933142</td>\n      <td>0.652913</td>\n      <td>0.523529</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.867891</td>\n      <td>0.676651</td>\n      <td>0.474546</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.765987</td>\n      <td>0.773055</td>\n      <td>0.362667</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d22e9d5d7084d37b857e5b751e2577d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9540b680638c4c47a971661f8413121f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"656ab535e5dc46128652631ccdf16e2c"}},"metadata":{}},{"name":"stdout","text":"✅ Stage2 best F1: 0.5235294117647059\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2beeb1cd87cb4f38b61814a96dac6fb0"}},"metadata":{}},{"name":"stdout","text":"✅ Saved FINAL W2V2: /kaggle/working/wav2vec2_tamil_16k_stratspk/final\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nfrom sklearn.metrics import classification_report\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =============================\n# Paths\n# =============================\nGT_CSV     = \"/kaggle/input/datasets/tahmimahoque/tamil-test/Tamil_GT.xlsx - tam.csv\"\nFINAL_DIR  = \"/kaggle/working/wav2vec2_tamil_16k_stratspk/final\"\nSEARCH_ROOT = \"/kaggle/input\"   # search wavs anywhere in kaggle input\n\nassert os.path.isfile(GT_CSV), f\"❌ Missing GT csv: {GT_CSV}\"\nassert os.path.isdir(FINAL_DIR), f\"❌ Missing W2V2 model dir: {FINAL_DIR} (train+save first)\"\n\n# =============================\n# Load GT\n# =============================\ngt = pd.read_csv(GT_CSV)\nassert {\"filename\",\"label\"}.issubset(gt.columns), f\"❌ Need columns filename,label; got {gt.columns.tolist()}\"\n\n# labels: D->1, ND->0\ngt[\"label_num\"] = gt[\"label\"].map({\"D\": 1, \"ND\": 0})\nif gt[\"label_num\"].isna().any():\n    bad = gt[gt[\"label_num\"].isna()][[\"filename\",\"label\"]].head(10)\n    raise ValueError(f\"❌ Unknown labels found. Examples:\\n{bad}\")\n\ntargets = set(gt[\"filename\"].astype(str).str.lower().tolist())\n\n# =============================\n# 1) Locate audio files in /kaggle/input\n# =============================\nfound = {}\nfor root, dirs, files in os.walk(SEARCH_ROOT):\n    for fn in files:\n        low = fn.lower()\n        if low in targets:  # exact filename match\n            found[low] = os.path.join(root, fn)\n\nprint(f\"✅ Found audio: {len(found)}/{len(gt)}\")\nif len(found) < len(gt):\n    missing = [f for f in gt[\"filename\"].tolist() if str(f).lower() not in found]\n    print(\"⚠️ Missing examples (first 20):\", missing[:20])\n\nassert len(found) > 0, (\n    \"❌ No matching wav found in /kaggle/input. \"\n    \"You need to add the dataset that contains t1.wav, t2.wav, ... to this notebook.\"\n)\n\n# =============================\n# 2) Load Wav2Vec2 model + extractor\n# =============================\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(FINAL_DIR)\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(FINAL_DIR).to(device)\nmodel.eval()\n\n# =============================\n# 3) Predict helper (same trimming style)\n# =============================\n@torch.no_grad()\ndef predict_one(audio_path):\n    audio, _ = librosa.load(audio_path, sr=16000, mono=True)\n    audio, _ = librosa.effects.trim(audio, top_db=30)\n\n    if audio is None or len(audio) == 0:\n        audio = np.zeros(int(16000 * 0.5), dtype=np.float32)\n\n    feats = feature_extractor([audio], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n    feats = {k: v.to(device) for k, v in feats.items()}\n    logits = model(**feats).logits\n    return int(torch.argmax(logits, dim=1).item())\n\n# =============================\n# 4) Predict + Classification report (no confusion matrix)\n# =============================\ny_true, y_pred = [], []\nmissing_audio = 0\n\nfor _, row in gt.iterrows():\n    key = str(row[\"filename\"]).lower()\n    if key not in found:\n        missing_audio += 1\n        continue\n\n    fp = found[key]\n    pred = predict_one(fp)\n\n    y_true.append(int(row[\"label_num\"]))\n    y_pred.append(pred)\n\nprint(\"Missing audio skipped:\", missing_audio)\nprint(\"\\n✅ W2V2 CLASSIFICATION REPORT (TEST)\")\nprint(classification_report(y_true, y_pred, digits=4, target_names=[\"ND(0)\", \"D(1)\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T19:15:00.900739Z","iopub.execute_input":"2026-02-28T19:15:00.901122Z","iopub.status.idle":"2026-02-28T19:15:19.127847Z","shell.execute_reply.started":"2026-02-28T19:15:00.901091Z","shell.execute_reply":"2026-02-28T19:15:19.127083Z"}},"outputs":[{"name":"stdout","text":"✅ Found audio: 160/160\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/426 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b47f6bc1bb51430291841f236004be13"}},"metadata":{}},{"name":"stdout","text":"Missing audio skipped: 0\n\n✅ W2V2 CLASSIFICATION REPORT (TEST)\n              precision    recall  f1-score   support\n\n       ND(0)     1.0000    0.1500    0.2609        80\n        D(1)     0.5405    1.0000    0.7018        80\n\n    accuracy                         0.5750       160\n   macro avg     0.7703    0.5750    0.4813       160\nweighted avg     0.7703    0.5750    0.4813       160\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Ensemble Optimization with Threshold Adjustment\n\nFine-tunes MFCC + Wav2Vec2 ensemble weight and slightly increases the decision threshold to improve Non-Depressed predictions while maintaining balanced overall performance.","metadata":{}},{"cell_type":"code","source":"import os, re, glob, json\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    f1_score,\n    fbeta_score\n)\nfrom transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =============================\n# PATHS (Kaggle)\n# =============================\nGT_CSV      = \"/kaggle/input/datasets/tahmimahoque/tamil-test/Tamil_GT.xlsx - tam.csv\"\nSEARCH_ROOT = \"/kaggle/input\"\n\nCSV_VAL     = \"/kaggle/working/Tamil_16k/tamil_trainval_16k_stratspk.csv\"\nMFCC_PATH   = \"/kaggle/working/models_tamil/mfcc_cnn_tamil_16k_stratspk.pt\"\nSTAGE2_DIR  = \"/kaggle/working/wav2vec2_tamil_16k_stratspk/stage2\"\nBASE_W2V2   = \"facebook/wav2vec2-xls-r-300m\"\n\nassert os.path.isfile(GT_CSV), f\"❌ Missing GT CSV: {GT_CSV}\"\nassert os.path.isfile(CSV_VAL), f\"❌ Missing VAL CSV: {CSV_VAL}\"\nassert os.path.isfile(MFCC_PATH), f\"❌ Missing MFCC model: {MFCC_PATH}\"\nassert os.path.isdir(STAGE2_DIR), f\"❌ Missing W2V2 stage2 dir: {STAGE2_DIR}\"\n\n# =============================\n# 1) Best checkpoint finder\n# =============================\ndef find_best_checkpoint(stage2_dir: str) -> str:\n    state_path = os.path.join(stage2_dir, \"trainer_state.json\")\n    if os.path.isfile(state_path):\n        with open(state_path, \"r\") as f:\n            state = json.load(f)\n        best_ckpt = state.get(\"best_model_checkpoint\", None)\n        if best_ckpt and os.path.isdir(best_ckpt):\n            return best_ckpt\n\n    ckpts = glob.glob(os.path.join(stage2_dir, \"checkpoint-*\"))\n    if not ckpts:\n        raise FileNotFoundError(f\"❌ No checkpoints found under {stage2_dir}\")\n\n    def step_num(p):\n        m = re.search(r\"checkpoint-(\\d+)$\", p)\n        return int(m.group(1)) if m else -1\n\n    return sorted(ckpts, key=step_num)[-1]\n\nW2V2_CKPT = find_best_checkpoint(STAGE2_DIR)\nprint(\"✅ Using W2V2 checkpoint:\", W2V2_CKPT)\n\n# =============================\n# 2) MFCC extractor + MFCC model\n# =============================\ndef extract_mfcc(file_path, target_sr=16000, n_mfcc=40, max_len=320):\n    y, _ = librosa.load(file_path, sr=target_sr, mono=True)\n    y, _ = librosa.effects.trim(y, top_db=30)\n    if y is None or len(y) == 0:\n        y = np.zeros(int(target_sr * 0.5), dtype=np.float32)\n\n    mfcc = librosa.feature.mfcc(y=y, sr=target_sr, n_mfcc=n_mfcc)\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n\n    T = mfcc.shape[1]\n    if T < max_len:\n        mfcc = np.pad(mfcc, ((0,0),(0, max_len-T)), mode=\"constant\")\n    else:\n        mfcc = mfcc[:, :max_len]\n    return mfcc.astype(np.float32)\n\nclass MFCC_CNN(nn.Module):\n    def __init__(self, n_classes=2):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d((2,2)), nn.Dropout(0.25),\n\n            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d((2,2)), nn.Dropout(0.25),\n\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1,1)),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.4),\n            nn.Linear(64, n_classes)\n        )\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.features(x)\n        return self.classifier(x)\n\nmfcc_model = MFCC_CNN().to(device)\nmfcc_model.load_state_dict(torch.load(MFCC_PATH, map_location=device))\nmfcc_model.eval()\n\n# =============================\n# 3) W2V2 extractor + model\n# =============================\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(BASE_W2V2, sampling_rate=16000)\nw2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(W2V2_CKPT).to(device)\nw2v2_model.eval()\n\nsoftmax = torch.nn.Softmax(dim=1)\n\n@torch.no_grad()\ndef mfcc_prob(file_path):\n    x = torch.from_numpy(extract_mfcc(file_path)).unsqueeze(0).to(device)\n    logits = mfcc_model(x)\n    return softmax(logits).squeeze(0).cpu().numpy()  # [2]\n\n@torch.no_grad()\ndef w2v2_prob(file_path):\n    audio, _ = librosa.load(file_path, sr=16000, mono=True)\n    audio, _ = librosa.effects.trim(audio, top_db=30)\n    if audio is None or len(audio) == 0:\n        audio = np.zeros(int(16000 * 0.5), dtype=np.float32)\n\n    feats = feature_extractor([audio], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n    feats = {k: v.to(device) for k, v in feats.items()}\n    logits = w2v2_model(**feats).logits\n    return softmax(logits).squeeze(0).cpu().numpy()  # [2]\n\n# =============================\n# 4) Compute VAL probs once (fast tuning)\n# =============================\ndf_all = pd.read_csv(CSV_VAL)\nval_df = df_all[df_all[\"split\"]==\"val\"].reset_index(drop=True)\nval_y  = val_df[\"label\"].astype(int).values\nval_fp = val_df[\"file_path\"].tolist()\n\nprint(\"✅ VAL samples:\", len(val_fp))\n\nval_m, val_w = [], []\nfor fp in val_fp:\n    if not os.path.exists(fp):\n        raise FileNotFoundError(f\"❌ Missing val audio: {fp}\")\n    val_m.append(mfcc_prob(fp))\n    val_w.append(w2v2_prob(fp))\n\nval_m = np.stack(val_m)  # [N,2]\nval_w = np.stack(val_w)  # [N,2]\n\n# =============================\n# 5) Tune (weight + threshold) on VAL\n# =============================\nOBJECTIVE = \"f2\"  # keep your working setup\nbest = {\"score\": -1, \"w\": None, \"t\": None}\n\nw_grid = np.linspace(0.0, 1.0, 101)\nt_grid = np.linspace(0.10, 0.90, 81)\n\nfor w in w_grid:\n    p = w * val_m + (1 - w) * val_w\n    pD = p[:, 1]\n    for t in t_grid:\n        preds = (pD >= t).astype(int)\n\n        if OBJECTIVE == \"macro_f1\":\n            score = f1_score(val_y, preds, average=\"macro\")\n        else:\n            score = fbeta_score(val_y, preds, beta=2, pos_label=1)\n\n        if score > best[\"score\"]:\n            best.update({\"score\": float(score), \"w\": float(w), \"t\": float(t)})\n\nprint(f\"✅ Tuned on VAL | objective={OBJECTIVE}: w_mfcc={best['w']:.2f}, thr={best['t']:.2f}, score={best['score']:.4f}\")\n\n# =============================\n# ✅ OPTION 1\n# =============================\nTHR_BUMP = 0.03  \nthr_used = min(0.95, best[\"t\"] + THR_BUMP)\nprint(f\"✅ Threshold used for TEST: {thr_used:.2f} (tuned thr={best['t']:.2f} + bump={THR_BUMP:.2f})\")\n\n# =============================\n# 6) Load TEST GT + locate test audios\n# =============================\ngt = pd.read_csv(GT_CSV)\ngt[\"label_num\"] = gt[\"label\"].map({\"D\":1, \"ND\":0})\nif gt[\"label_num\"].isna().any():\n    bad = gt[gt[\"label_num\"].isna()][[\"filename\",\"label\"]].head(10)\n    raise ValueError(f\"❌ Unknown labels found. Examples:\\n{bad}\")\n\ntargets = set(gt[\"filename\"].astype(str).str.lower().tolist())\n\nfound = {}\nfor root, _, files in os.walk(SEARCH_ROOT):\n    for fn in files:\n        low = fn.lower()\n        if low in targets:\n            found[low] = os.path.join(root, fn)\n\nprint(f\"✅ Found TEST audio: {len(found)}/{len(gt)}\")\nassert len(found) > 0, \"❌ No matching test wav found in /kaggle/input\"\n\n# =============================\n# 7) Apply tuned (w, thr_used) to TEST\n# =============================\ny_true, y_pred = [], []\nmissing_audio = 0\n\nfor _, row in gt.iterrows():\n    key = str(row[\"filename\"]).lower()\n    if key not in found:\n        missing_audio += 1\n        continue\n\n    fp = found[key]\n    p1 = mfcc_prob(fp)\n    p2 = w2v2_prob(fp)\n    p  = best[\"w\"] * p1 + (1 - best[\"w\"]) * p2\n\n    pred = int(p[1] >= thr_used)   # ✅ bumped threshold\n    y_true.append(int(row[\"label_num\"]))\n    y_pred.append(pred)\n\nprint(\"Missing audio skipped:\", missing_audio)\nprint(f\"\\n✅ ENSEMBLE TEST REPORT | w_mfcc={best['w']:.2f}, thr_used={thr_used:.2f}\")\nprint(classification_report(y_true, y_pred, digits=4, target_names=[\"ND(0)\", \"D(1)\"]))\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[\"ND\",\"D\"], yticklabels=[\"ND\",\"D\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(f\"Confusion Matrix \")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T20:07:16.064813Z","iopub.execute_input":"2026-02-28T20:07:16.065703Z","iopub.status.idle":"2026-02-28T20:08:02.044257Z","shell.execute_reply.started":"2026-02-28T20:07:16.065669Z","shell.execute_reply":"2026-02-28T20:08:02.043627Z"}},"outputs":[{"name":"stdout","text":"✅ Using W2V2 checkpoint: /kaggle/working/wav2vec2_tamil_16k_stratspk/stage2/checkpoint-219\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/426 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9577425b3d4bbb83bd1753330881e1"}},"metadata":{}},{"name":"stdout","text":"✅ VAL samples: 216\n✅ Tuned on VAL | objective=f2: w_mfcc=0.00, thr=0.80, score=0.9938\n✅ Threshold used for TEST: 0.83 (tuned thr=0.80 + bump=0.03)\n✅ Found TEST audio: 160/160\nMissing audio skipped: 0\n\n✅ ENSEMBLE TEST REPORT | w_mfcc=0.00, thr_used=0.83\n              precision    recall  f1-score   support\n\n       ND(0)     0.9383    0.9500    0.9441        80\n        D(1)     0.9494    0.9375    0.9434        80\n\n    accuracy                         0.9437       160\n   macro avg     0.9438    0.9437    0.9437       160\nweighted avg     0.9438    0.9437    0.9437       160\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x400 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"# Prediction on test data submission","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ✅ DD-DL FINAL PREDICTION (Tamil RUN2 - Ensemble, Kaggle)\n# - Reads wavs directly from Kaggle input folder (no zip)\n# - ensemble_predict() must be defined + models loaded before this cell\n# ============================================================\n\nimport os, glob\nimport pandas as pd\n\nTEAM_NAME = \"TriVector\"\nRUN = \"run2\"\n\nTEST_DIR = \"/kaggle/input/datasets/tahmimahoque/depression-det/Depression_det/Depression_det/Test-set-tamil/Test-set-tamil\"\nSAVE_DIR = \"/kaggle/working/submissions\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# 1) Collect wav files\nwavs = glob.glob(os.path.join(TEST_DIR, \"**\", \"*.wav\"), recursive=True) + \\\n       glob.glob(os.path.join(TEST_DIR, \"**\", \"*.WAV\"), recursive=True)\n\nassert len(wavs) > 0, f\"❌ No wav found in: {TEST_DIR}\"\nwavs = sorted(wavs)\nprint(\"✅ Total test wavs:\", len(wavs))\n\n# 2) Predict (ensemble_predict MUST already be defined above)\ndef to_text_label(pred_int):\n    return \"Depressed\" if int(pred_int) == 1 else \"Non-depressed\"\n\nrows = []\nfor fp in wavs:\n    pred, _ = ensemble_predict(fp)   # ✅ ensemble\n    file_id = os.path.splitext(os.path.basename(fp))[0]\n    rows.append({\"file_name\": file_id, \"labels\": to_text_label(pred)})\n\n# 3) Save CSV\ncsv_name = f\"{TEAM_NAME}_Tamil_{RUN}.csv\"\ncsv_path = os.path.join(SAVE_DIR, csv_name)\npd.DataFrame(rows).to_csv(csv_path, index=False)\n\nprint(\"✅ Saved Tamil RUN2 CSV:\", csv_path)\nprint(pd.read_csv(csv_path).head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T19:26:02.431711Z","iopub.execute_input":"2026-02-28T19:26:02.432372Z","iopub.status.idle":"2026-02-28T19:26:18.713521Z","shell.execute_reply.started":"2026-02-28T19:26:02.432342Z","shell.execute_reply":"2026-02-28T19:26:18.712588Z"}},"outputs":[{"name":"stdout","text":"✅ Total test wavs: 160\n✅ Saved Tamil RUN2 CSV: /kaggle/working/submissions/TriVector_Tamil_run2.csv\n  file_name         labels\n0        t1      Depressed\n1       t10  Non-depressed\n2      t100  Non-depressed\n3      t101  Non-depressed\n4      t102  Non-depressed\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}